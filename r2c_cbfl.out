Dataset multi Source real Target clipart Labeled num perclass 3 Network resnet34
126 classes in this dataset
Unlabelled Target Dataset Size:  18325
Labelled Target Dataset Size:  378
Misc. Labelled Target Dataset Size:  378
Bank keys - Target:  dict_keys(['feat_vec', 'labels', 'names', 'domain_identifier']) Source:  dict_keys(['feat_vec', 'labels', 'names', 'domain_identifier'])
Num  - Target:  18325 Source:  70358
Unlabeled Target Data Size: 381
S real T clipart Train Ep: 0 lr0.01 	 Loss Classification: 4.890052 Loss T 0.471103 Method MME

S real T clipart Train Ep: 100 lr0.009925650290240803 	 Loss Classification: 1.283135 Loss T 0.283160 Method MME

S real T clipart Train Ep: 200 lr0.009852577760521605 	 Loss Classification: 1.010528 Loss T 0.225192 Method MME

S real T clipart Train Ep: 300 lr0.009780748269686728 	 Loss Classification: 0.869402 Loss T 0.228012 Method MME

S real T clipart Train Ep: 400 lr0.009710128909124701 	 Loss Classification: 0.706364 Loss T 0.183431 Method MME

S real T clipart Train Ep: 500 lr0.00964068794694323 	 Loss Classification: 1.469412 Loss T 0.185112 Method MME


Labeled Target set: Average loss: 2.5030, Accuracy: 480/1080 F1 (44.4444%)


Test set: Average loss: 2.2922, Accuracy: 8974/18312 F1 (49.0061%)


Val set: Average loss: 2.3264, Accuracy: 170/360 F1 (47.2222%)

best acc test 49.006116  acc val 47.222222 acc labeled target 44.444444
saving model...
S real T clipart Train Ep: 600 lr0.00957239477517603 	 Loss Classification: 0.561415 Loss T 0.170170 Method MME

S real T clipart Train Ep: 700 lr0.009505219859830012 	 Loss Classification: 0.380528 Loss T 0.141688 Method MME

S real T clipart Train Ep: 800 lr0.009439134693595126 	 Loss Classification: 0.851286 Loss T 0.176668 Method MME

S real T clipart Train Ep: 900 lr0.009374111751051751 	 Loss Classification: 0.580462 Loss T 0.124207 Method MME

S real T clipart Train Ep: 1000 lr0.009310124446222227 	 Loss Classification: 0.623616 Loss T 0.118863 Method MME


Labeled Target set: Average loss: 2.3272, Accuracy: 542/1080 F1 (50.1852%)


Test set: Average loss: 2.1730, Accuracy: 9601/18312 F1 (52.4301%)


Val set: Average loss: 2.2719, Accuracy: 191/360 F1 (53.0556%)

best acc test 52.430100  acc val 53.055556 acc labeled target 50.185185
saving model...
S real T clipart Train Ep: 1100 lr0.00924714709232377 	 Loss Classification: 0.769284 Loss T 0.133322 Method MME

S real T clipart Train Ep: 1200 lr0.009185154863590003 	 Loss Classification: 0.405095 Loss T 0.117616 Method MME

S real T clipart Train Ep: 1300 lr0.00912412375903735 	 Loss Classification: 0.293154 Loss T 0.103858 Method MME

S real T clipart Train Ep: 1400 lr0.009064030568061049 	 Loss Classification: 0.436825 Loss T 0.126031 Method MME

S real T clipart Train Ep: 1500 lr0.009004852837753237 	 Loss Classification: 0.531897 Loss T 0.118180 Method MME


Labeled Target set: Average loss: 2.1374, Accuracy: 567/1080 F1 (52.5000%)


Test set: Average loss: 1.8829, Accuracy: 10691/18312 F1 (58.3825%)


Val set: Average loss: 1.9273, Accuracy: 211/360 F1 (58.6111%)

best acc test 58.382481  acc val 58.611111 acc labeled target 52.500000
saving model...
S real T clipart Train Ep: 1600 lr0.008946568841842816 	 Loss Classification: 0.499096 Loss T 0.104103 Method MME

S real T clipart Train Ep: 1700 lr0.008889157551163433 	 Loss Classification: 0.837162 Loss T 0.123159 Method MME

S real T clipart Train Ep: 1800 lr0.008832598605562044 	 Loss Classification: 0.691673 Loss T 0.119071 Method MME

S real T clipart Train Ep: 1900 lr0.008776872287166303 	 Loss Classification: 0.545851 Loss T 0.113298 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [0.5555556  0.6666667  0.5        0.33333334 0.         0.5
 0.33333334 0.5555556  0.6666667  0.11111111 0.5555556  0.33333334
 0.5        0.5555556  0.6666667  0.33333334 0.8888889  0.7777778
 0.8888889  0.44444445 0.         0.5555556  0.5        0.6666667
 0.8888889  0.33333334 0.5555556  0.33333334 0.         0.8888889
 0.33333334 0.22222222 0.44444445 1.         0.8333333  1.
 0.5555556  0.6666667  0.33333334 0.33333334 0.44444445 0.8333333
 0.33333334 0.44444445 0.         0.6666667  0.33333334 0.6666667
 0.22222222 0.6666667  0.7777778  0.6666667  0.7777778  0.6666667
 0.6666667  0.6666667  0.7777778  0.6666667  0.         0.6666667
 0.6666667  0.6666667  0.6666667  0.5555556  1.         0.6666667
 0.33333334 1.         0.7777778  0.11111111 0.22222222 0.6666667
 1.         0.         0.         0.5555556  0.         0.7777778
 0.11111111 0.7777778  0.         1.         0.11111111 0.
 1.         0.8888889  0.7777778  0.5555556  0.         0.33333334
 0.5555556  0.5555556  0.         0.8333333  0.         0.8888889
 0.22222222 0.44444445 0.6666667  0.44444445 0.8888889  0.6666667
 0.22222222 0.44444445 0.33333334 0.33333334 0.33333334 0.33333334
 0.         0.6666667  0.33333334 0.8888889  0.7777778  0.8888889
 0.7777778  0.6666667  0.6666667  0.7777778  0.         0.11111111
 0.5        0.6666667  1.         1.         0.33333334 0.7777778 ]
Top k classes which perform poorly are:  [73, 88, 92, 94, 83, 44, 74, 28, 80, 20, 118, 76, 108, 58, 4, 119, 69, 78, 9, 82, 31, 96, 48, 102, 70, 104, 38, 46, 42, 6, 124, 3, 39, 106, 11, 110, 15, 66, 107, 25, 105, 30, 27, 89, 103, 32, 97, 19, 40, 99, 43, 22, 5, 2, 120, 12, 63, 87, 75, 0, 91, 26, 21, 13, 10, 7, 90, 36, 98, 101, 109, 115, 116, 121, 62, 71, 49, 53, 47, 54, 55, 45, 57, 59, 60, 37, 61, 65, 14, 8, 1, 23, 51, 50, 112, 114, 17, 117, 125, 68, 56, 86, 77, 79, 52, 93, 41, 34, 95, 16, 18, 113, 100, 85, 24, 29, 111, 64, 81, 33, 67, 35, 72, 122, 123, 84]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 19  61  23  48  21  15  11  54  65   8  87  91  14  96 102  70 108 103
  67  57  50  97  36 124  56  42  23  43  36  13  64   7  44 228 125 167
  84 126  67  24 141  39  60  29 127  84 266  55 142 159 135  54 286  91
  20 138 331 107  39  73  98  30  81 135 156  39  37  29  20   4  38 147
  62  29  12  11  39  58  12 143   3  45  12  17  73  96  33  89   3  27
  17  51  17  56  48 163  88  76  65  33  78  64 107 165  77  17 213 198
  21 106  72 231 205 165 130 111  43 155   8  47 108 147  83 180  38 244]
CBFL per class weights: tensor([1.5372, 0.5830, 1.2948, 0.6982, 1.4044, 1.9095, 2.5532, 0.6380, 0.5571,
        3.4589, 0.4584, 0.4459, 2.0359, 0.4317, 0.4167, 0.5290, 0.4035, 0.4144,
        0.5453, 0.6128, 0.6765, 0.4291, 0.8802, 0.3751, 0.6209, 0.7760, 1.2948,
        0.7615, 0.8802, 2.1818, 0.5633, 3.9335, 0.7477, 0.2973, 0.3736, 0.3286,
        0.4687, 0.3721, 0.5453, 1.2468, 0.3527, 0.8241, 0.5901, 1.0569, 0.3706,
        0.4687, 0.2870, 0.6293, 0.3516, 0.3350, 0.3599, 0.6380, 0.2832, 0.4459,
        1.4675, 0.3562, 0.2772, 0.4056, 0.8241, 0.5140, 0.4265, 1.0266, 0.4798,
        0.3599, 0.3376, 0.8241, 0.8605, 1.0569, 1.4675, 6.7816, 0.8418, 0.3462,
        0.5762, 1.0569, 2.3520, 2.5532, 0.8241, 0.6049, 2.3520, 0.3505, 8.9970,
        0.7345, 2.3520, 1.7014, 0.5140, 0.4317, 0.9467, 0.4520, 8.9970, 1.1244,
        1.7014, 0.6663, 1.7014, 0.6209, 0.6982, 0.3317, 0.4552, 0.5003, 0.5571,
        0.9467, 0.4918, 0.5633, 0.4056, 0.3301, 0.4960, 1.7014, 0.3028, 0.3095,
        1.4044, 0.4077, 0.5189, 0.2963, 0.3062, 0.3301, 0.3664, 0.3975, 0.7615,
        0.3385, 3.4589, 0.7098, 0.4035, 0.3462, 0.4723, 0.3196, 0.8418, 0.2924],
       device='cuda:0')
S real T clipart Train Ep: 2000 lr0.008721959494934213 	 Loss Classification: 0.649910 Loss T 0.102134 Method MME


Labeled Target set: Average loss: 2.2094, Accuracy: 587/1080 F1 (54.3519%)


Test set: Average loss: 2.0464, Accuracy: 10617/18312 F1 (57.9784%)


Val set: Average loss: 2.1188, Accuracy: 203/360 F1 (56.3889%)

best acc test 58.382481  acc val 56.388889 acc labeled target 54.351852
saving model...
S real T clipart Train Ep: 2100 lr0.008667841720414475 	 Loss Classification: 0.342230 Loss T 0.085962 Method MME

S real T clipart Train Ep: 2200 lr0.008614501024650454 	 Loss Classification: 2.749452 Loss T 0.000521 Method MME

S real T clipart Train Ep: 2300 lr0.008561920016164943 	 Loss Classification: 4.038361 Loss T 0.005952 Method MME

S real T clipart Train Ep: 2400 lr0.008510081829966844 	 Loss Classification: 1.644222 Loss T 0.094149 Method MME

S real T clipart Train Ep: 2500 lr0.008458970107524513 	 Loss Classification: 2.150513 Loss T 0.015356 Method MME


Labeled Target set: Average loss: 5.4873, Accuracy: 66/1080 F1 (6.1111%)


Test set: Average loss: 5.2631, Accuracy: 1052/18312 F1 (5.7449%)


Val set: Average loss: 5.2475, Accuracy: 24/360 F1 (6.6667%)

best acc test 58.382481  acc val 6.666667 acc labeled target 6.111111
saving model...
S real T clipart Train Ep: 2600 lr0.008408568977653933 	 Loss Classification: 3.400423 Loss T 0.009278 Method MME

S real T clipart Train Ep: 2700 lr0.00835886303827305 	 Loss Classification: 2.643403 Loss T 0.012884 Method MME

S real T clipart Train Ep: 2800 lr0.008309837338976545 	 Loss Classification: 2.446283 Loss T 0.020176 Method MME

S real T clipart Train Ep: 2900 lr0.008261477364388068 	 Loss Classification: 2.725097 Loss T 0.000173 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [0.22222222 0.         0.         0.44444445 0.         0.
 0.         0.33333334 0.11111111 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.11111111 0.
 0.         0.33333334 0.         0.         0.         0.22222222
 0.33333334 0.11111111 0.44444445 0.         0.         0.
 0.         0.         0.         0.33333334 0.         0.
 0.         0.         0.         0.         0.         0.33333334
 0.         0.         0.         0.         0.11111111 0.11111111
 0.         0.         0.         0.11111111 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.22222222 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.11111111 0.11111111 0.33333334 0.         1.         0.33333334
 0.44444445 0.         0.         0.33333334 0.         0.11111111
 0.         0.         0.22222222 0.         0.         0.22222222
 0.         0.         0.         0.         0.         0.
 0.22222222 0.         0.11111111 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.11111111 0.22222222 0.         0.         0.        ]
Top k classes which perform poorly are:  [62, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 68, 67, 66, 65, 64, 63, 124, 61, 60, 59, 58, 82, 83, 87, 91, 123, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 56, 109, 106, 105, 104, 103, 102, 100, 99, 97, 96, 94, 92, 107, 55, 125, 5, 14, 28, 27, 6, 26, 9, 54, 10, 24, 11, 23, 21, 20, 19, 12, 18, 17, 16, 15, 4, 2, 13, 34, 51, 50, 49, 48, 1, 46, 45, 44, 33, 42, 41, 43, 38, 40, 35, 36, 37, 52, 57, 31, 8, 110, 84, 22, 95, 85, 121, 53, 122, 0, 101, 69, 108, 29, 98, 89, 7, 86, 93, 30, 39, 47, 25, 3, 32, 90, 88]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [    4     3     5     8     4     5     4     7     9     4     8     8
     6     9     9    10     9     6     6     6    10    11     6    10
     4     8     3     4     6     4     7     8     4    14    11     7
     8    12     8     5     8     4     8     8     8     5     7     6
     6     9     6     7    23     6     3     6    14    11     7     7
     9     4     7     9    13     7     5     7     5    20    11     8
     4     5     4     3     6     5     5     9   697     8     4     6
     6    13     5     6 16748     7     3     8     6     7     4     9
     5     6     5     6     9     5     7    15    11     7    12    10
     9     5     5    13     7     9     7     6     7    15     5     4
     8     7     8    12     5    11]
CBFL per class weights: tensor([1.5804, 2.0967, 1.2706, 0.8061, 1.5804, 1.2706, 1.5804, 0.9167, 0.7201,
        1.5804, 0.8061, 0.8061, 1.0642, 0.7201, 0.7201, 0.6513, 0.7201, 1.0642,
        1.0642, 1.0642, 0.6513, 0.5950, 1.0642, 0.6513, 1.5804, 0.8061, 2.0967,
        1.5804, 1.0642, 1.5804, 0.9167, 0.8061, 1.5804, 0.4745, 0.5950, 0.9167,
        0.8061, 0.5481, 0.8061, 1.2706, 0.8061, 1.5804, 0.8061, 0.8061, 0.8061,
        1.2706, 0.9167, 1.0642, 1.0642, 0.7201, 1.0642, 0.9167, 0.3017, 1.0642,
        2.0967, 1.0642, 0.4745, 0.5950, 0.9167, 0.9167, 0.7201, 1.5804, 0.9167,
        0.7201, 0.5084, 0.9167, 1.2706, 0.9167, 1.2706, 0.3420, 0.5950, 0.8061,
        1.5804, 1.2706, 1.5804, 2.0967, 1.0642, 1.2706, 1.2706, 0.7201, 0.0623,
        0.8061, 1.5804, 1.0642, 1.0642, 0.5084, 1.2706, 1.0642, 0.0623, 0.9167,
        2.0967, 0.8061, 1.0642, 0.9167, 1.5804, 0.7201, 1.2706, 1.0642, 1.2706,
        1.0642, 0.7201, 1.2706, 0.9167, 0.4450, 0.5950, 0.9167, 0.5481, 0.6513,
        0.7201, 1.2706, 1.2706, 0.5084, 0.9167, 0.7201, 0.9167, 1.0642, 0.9167,
        0.4450, 1.2706, 1.5804, 0.8061, 0.9167, 0.8061, 0.5481, 1.2706, 0.5950],
       device='cuda:0')
S real T clipart Train Ep: 3000 lr0.008213769018249545 	 Loss Classification: 1.609484 Loss T 0.161775 Method MME


Labeled Target set: Average loss: 4.9404, Accuracy: 203/1080 F1 (18.7963%)


Test set: Average loss: 4.5642, Accuracy: 3950/18312 F1 (21.5706%)


Val set: Average loss: 4.5753, Accuracy: 87/360 F1 (24.1667%)

best acc test 58.382481  acc val 24.166667 acc labeled target 18.796296
saving model...
S real T clipart Train Ep: 3100 lr0.008166698608209509 	 Loss Classification: 0.986320 Loss T 0.120423 Method MME

S real T clipart Train Ep: 3200 lr0.008120252831274708 	 Loss Classification: 0.754658 Loss T 0.133392 Method MME

S real T clipart Train Ep: 3300 lr0.008074418759891278 	 Loss Classification: 0.496889 Loss T 0.040008 Method MME

S real T clipart Train Ep: 3400 lr0.008029183828623731 	 Loss Classification: 0.911468 Loss T 0.037296 Method MME

S real T clipart Train Ep: 3500 lr0.007984535821401871 	 Loss Classification: 0.320128 Loss T 0.029789 Method MME


Labeled Target set: Average loss: 5.8374, Accuracy: 28/1080 F1 (2.5926%)


Test set: Average loss: 5.5990, Accuracy: 315/18312 F1 (1.7202%)


Val set: Average loss: 5.4842, Accuracy: 10/360 F1 (2.7778%)

best acc test 58.382481  acc val 2.777778 acc labeled target 2.592593
saving model...
S real T clipart Train Ep: 3600 lr0.007940462859307384 	 Loss Classification: 0.755632 Loss T 0.153165 Method MME

S real T clipart Train Ep: 3700 lr0.007896953388873518 	 Loss Classification: 1.050520 Loss T 0.124462 Method MME

S real T clipart Train Ep: 3800 lr0.007853996170872714 	 Loss Classification: 0.519985 Loss T 0.133783 Method MME

S real T clipart Train Ep: 3900 lr0.00781158026956848 	 Loss Classification: 0.577075 Loss T 0.104795 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [0.33333334 0.16666667 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.33333334 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.22222222
 0.44444445 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.33333334 0.         0.
 0.         0.         0.         0.         0.         0.22222222
 1.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.16666667 0.         0.         0.         0.        ]
Top k classes which perform poorly are:  [62, 91, 88, 87, 86, 85, 84, 83, 82, 80, 79, 78, 77, 92, 76, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 75, 93, 94, 95, 123, 122, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103, 102, 101, 100, 99, 98, 97, 96, 124, 61, 125, 30, 31, 3, 29, 28, 27, 26, 25, 24, 23, 4, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 32, 6, 2, 46, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 33, 5, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 45, 1, 121, 59, 89, 81, 22, 0, 60, 90]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [  18   61   23   27   17   10    9   28   11    7   25   22   77   46
   48   23   39   59   52   33    9   33   24   29   49   24  123   66
   15   17   31    6   38   17   29   95   29   33   26   57   68   17
   15   12   50   81  122   19   81   51   97   35  128   59   80   60
   28   64   23   37   54   35   60   21   41   25   36   16   14    8
   14   70   61   20   15  137   23   34    9   75  154   14  114    6
   57   27   19   60 3815   19 9512   16   10   29   39   70   41   51
   58    9   18   38   47   33   26    8   61   43   10   37   21   95
   92   26   74   81   15   56    6   27   61   83   44   63   48  108]
CBFL per class weights: tensor([1.3183, 0.4760, 1.0570, 0.9179, 1.3890, 2.2815, 2.5225, 0.8894, 2.0844,
        3.2113, 0.9819, 1.0997, 0.4049, 0.5893, 0.5700, 1.0570, 0.6728, 0.4877,
        0.5360, 0.7729, 2.5225, 0.7729, 1.0179, 0.8629, 0.5610, 1.0179, 0.3075,
        0.4499, 1.5589, 1.3890, 0.8149, 3.7279, 0.6872, 1.3890, 0.8629, 0.3547,
        0.8629, 0.7729, 0.9487, 0.5002, 0.4406, 1.3890, 1.5589, 1.9201, 0.5523,
        0.3917, 0.3088, 1.2550, 0.3917, 0.5440, 0.3503, 0.7356, 0.3014, 0.4877,
        0.3949, 0.4817, 0.8894, 0.4599, 1.0570, 0.7025, 0.5209, 0.7356, 0.4817,
        1.1465, 0.6460, 0.9819, 0.7186, 1.4686, 1.6621, 2.8238, 1.6621, 0.4319,
        0.4760, 1.1980, 1.5589, 0.2918, 1.0570, 0.7537, 2.5225, 0.4121, 0.2771,
        1.6621, 0.3199, 3.7279, 0.5002, 0.9179, 1.2550, 0.4817, 0.2182, 1.2550,
        0.2182, 1.4686, 2.2815, 0.8629, 0.6728, 0.4319, 0.6460, 0.5440, 0.4939,
        2.5225, 1.3183, 0.6872, 0.5795, 0.7729, 0.9487, 2.8238, 0.4760, 0.6217,
        2.2815, 0.7025, 1.1465, 0.3547, 0.3616, 0.9487, 0.4158, 0.3917, 1.5589,
        0.5069, 3.7279, 0.9179, 0.4760, 0.3856, 0.6104, 0.4651, 0.5700, 0.3294],
       device='cuda:0')
S real T clipart Train Ep: 4000 lr0.007769695042409123 	 Loss Classification: 1.450332 Loss T 0.139808 Method MME


Labeled Target set: Average loss: 3.3142, Accuracy: 350/1080 F1 (32.4074%)


Test set: Average loss: 3.1101, Accuracy: 6771/18312 F1 (36.9758%)


Val set: Average loss: 3.1579, Accuracy: 137/360 F1 (38.0556%)

best acc test 58.382481  acc val 38.055556 acc labeled target 32.407407
saving model...
S real T clipart Train Ep: 4100 lr0.007728330130142108 	 Loss Classification: 0.975495 Loss T 0.094830 Method MME

S real T clipart Train Ep: 4200 lr0.007687475447329114 	 Loss Classification: 0.806840 Loss T 0.063618 Method MME

S real T clipart Train Ep: 4300 lr0.0076471211732427845 	 Loss Classification: 0.559856 Loss T 0.065795 Method MME

S real T clipart Train Ep: 4400 lr0.007607257743127307 	 Loss Classification: 0.560677 Loss T 0.142362 Method MME

S real T clipart Train Ep: 4500 lr0.007567875839805858 	 Loss Classification: 1.402980 Loss T 0.168780 Method MME


Labeled Target set: Average loss: 2.7069, Accuracy: 487/1080 F1 (45.0926%)


Test set: Average loss: 2.4708, Accuracy: 9002/18312 F1 (49.1590%)


Val set: Average loss: 2.4956, Accuracy: 172/360 F1 (47.7778%)

best acc test 58.382481  acc val 47.777778 acc labeled target 45.092593
saving model...
S real T clipart Train Ep: 4600 lr0.007528966385618854 	 Loss Classification: 0.619852 Loss T 0.051802 Method MME

S real T clipart Train Ep: 4700 lr0.007490520534677821 	 Loss Classification: 0.356041 Loss T 0.011244 Method MME

S real T clipart Train Ep: 4800 lr0.007452529665420465 	 Loss Classification: 0.380421 Loss T 0.129102 Method MME

S real T clipart Train Ep: 4900 lr0.007414985373453289 	 Loss Classification: 1.066017 Loss T 0.054670 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [0.8888889  0.44444445 0.5555556  0.44444445 0.11111111 0.33333334
 0.5        0.44444445 0.         0.11111111 0.6666667  0.8888889
 0.5555556  0.         0.33333334 0.         0.8888889  0.7777778
 0.33333334 0.8888889  0.         0.5555556  1.         0.22222222
 0.5        0.44444445 0.5555556  0.6666667  0.         0.6666667
 0.44444445 0.22222222 0.44444445 0.         0.33333334 0.7777778
 0.33333334 0.22222222 0.33333334 0.22222222 0.33333334 0.11111111
 0.7777778  1.         0.         0.7777778  0.33333334 0.44444445
 0.33333334 0.6666667  0.8888889  0.11111111 1.         0.6666667
 0.16666667 1.         0.         0.5        0.22222222 0.6666667
 0.7777778  0.44444445 0.44444445 0.22222222 0.5        0.44444445
 0.22222222 0.44444445 1.         0.         0.5555556  0.33333334
 0.33333334 0.5555556  0.8333333  0.         0.44444445 1.
 0.11111111 0.5555556  0.         1.         0.         0.
 0.6666667  1.         0.6666667  1.         0.         0.
 0.         0.33333334 0.11111111 0.6666667  0.44444445 0.6666667
 0.33333334 0.33333334 0.44444445 0.5555556  0.8888889  0.5555556
 0.         0.11111111 0.         0.22222222 0.11111111 0.6666667
 0.         0.6666667  0.33333334 0.6666667  0.8888889  0.44444445
 0.5555556  0.6666667  0.33333334 0.         0.5        0.6666667
 0.6666667  0.6666667  0.5555556  1.         0.         0.22222222]
Top k classes which perform poorly are:  [88, 56, 69, 75, 44, 80, 82, 33, 83, 89, 28, 90, 20, 102, 15, 124, 108, 13, 117, 104, 8, 9, 106, 41, 78, 103, 51, 92, 4, 54, 66, 58, 39, 37, 125, 31, 63, 105, 23, 110, 18, 97, 96, 116, 71, 72, 48, 38, 46, 91, 5, 40, 14, 36, 34, 76, 94, 67, 98, 65, 113, 62, 47, 1, 30, 25, 3, 61, 32, 7, 118, 24, 6, 57, 64, 114, 26, 79, 101, 12, 73, 70, 21, 99, 122, 2, 95, 107, 10, 27, 93, 109, 86, 84, 115, 49, 119, 120, 121, 53, 59, 29, 111, 60, 17, 45, 35, 42, 74, 0, 100, 112, 16, 19, 11, 50, 123, 55, 68, 52, 22, 81, 85, 87, 43, 77]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [   6   20    7    9   11    7    7   12   14    6   18   18   19   16
   16   11   22   18   20   16 5809   18   10   14   17   18   19   16
    7    3   22  288   13   18   21   21    9   15   17   14   10    7
   15   51   16   12   22   11   26   13   27    8   34   18   11   13
   44   24   13   10   21   10   12   14   17   15   11    7   15    7
   16   33   25   11    9   19   14   13   56   19   19    9   22 3689
   13   20   10   19  417   10 1116   10   12   13   14   27    9   11
   16   17   19   17   13   17   26   63   22   10    9   12   12   31
   24   24   20   24   13   23 5332   11   12   23   19   31   13   30]
CBFL per class weights: tensor([2.2627, 0.7272, 1.9492, 1.5311, 1.2652, 1.9492, 1.9492, 1.1655, 1.0088,
        2.2627, 0.8002, 0.8002, 0.7617, 0.8914, 0.8914, 1.2652, 0.6675, 0.8002,
        0.7272, 0.8914, 0.1324, 0.8002, 1.3848, 1.0088, 0.8431, 0.8002, 0.7617,
        0.8914, 1.9492, 4.4583, 0.6675, 0.1402, 1.0811, 0.8002, 0.6959, 0.6959,
        1.5311, 0.9462, 0.8431, 1.0088, 1.3848, 1.9492, 0.9462, 0.3302, 0.8914,
        1.1655, 0.6675, 1.2652, 0.5758, 1.0811, 0.5572, 1.7140, 0.4575, 0.8002,
        1.2652, 1.0811, 0.3705, 0.6178, 1.0811, 1.3848, 0.6959, 1.3848, 1.1655,
        1.0088, 0.8431, 0.9462, 1.2652, 1.9492, 0.9462, 1.9492, 0.8914, 0.4691,
        0.5960, 1.2652, 1.5311, 0.7617, 1.0088, 1.0811, 0.3077, 0.7617, 0.7617,
        1.5311, 0.6675, 0.1324, 1.0811, 0.7272, 1.3848, 0.7617, 0.1344, 1.3848,
        0.1324, 1.3848, 1.1655, 1.0811, 1.0088, 0.5572, 1.5311, 1.2652, 0.8914,
        0.8431, 0.7617, 0.8431, 1.0811, 0.8431, 0.5758, 0.2823, 0.6675, 1.3848,
        1.5311, 1.1655, 1.1655, 0.4946, 0.6178, 0.6178, 0.7272, 0.6178, 1.0811,
        0.6416, 0.1324, 1.2652, 1.1655, 0.6416, 0.7617, 0.4946, 1.0811, 0.5087],
       device='cuda:0')
S real T clipart Train Ep: 5000 lr0.007377879464668811 	 Loss Classification: 0.738946 Loss T 0.013077 Method MME


Labeled Target set: Average loss: 8.4414, Accuracy: 9/1080 F1 (0.8333%)


Test set: Average loss: 8.5159, Accuracy: 84/18312 F1 (0.4587%)


Val set: Average loss: 8.1324, Accuracy: 3/360 F1 (0.8333%)

best acc test 58.382481  acc val 0.833333 acc labeled target 0.833333
saving model...
S real T clipart Train Ep: 5100 lr0.007341203948625087 	 Loss Classification: 0.355591 Loss T 0.029404 Method MME

S real T clipart Train Ep: 5200 lr0.007304951032175895 	 Loss Classification: 0.654977 Loss T 0.020952 Method MME

S real T clipart Train Ep: 5300 lr0.007269113113340497 	 Loss Classification: 0.789559 Loss T 0.097780 Method MME

S real T clipart Train Ep: 5400 lr0.007233682775402483 	 Loss Classification: 0.419525 Loss T 0.121865 Method MME

S real T clipart Train Ep: 5500 lr0.007198652781227704 	 Loss Classification: 0.780941 Loss T 0.072424 Method MME


Labeled Target set: Average loss: 4.1624, Accuracy: 220/1080 F1 (20.3704%)


Test set: Average loss: 3.8915, Accuracy: 3971/18312 F1 (21.6852%)


Val set: Average loss: 3.8108, Accuracy: 84/360 F1 (23.3333%)

best acc test 58.382481  acc val 23.333333 acc labeled target 20.370370
saving model...
S real T clipart Train Ep: 5600 lr0.007164016067791809 	 Loss Classification: 0.166235 Loss T 0.047368 Method MME

S real T clipart Train Ep: 5700 lr0.0071297657409083726 	 Loss Classification: 1.515623 Loss T 0.008207 Method MME

S real T clipart Train Ep: 5800 lr0.0070958950701490225 	 Loss Classification: 0.289977 Loss T 0.009504 Method MME

S real T clipart Train Ep: 5900 lr0.007062397483947426 	 Loss Classification: 0.355958 Loss T 0.005534 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [0.8888889  0.         0.33333334 0.33333334 0.11111111 0.33333334
 0.         0.         0.         0.         0.         0.
 0.         0.         0.11111111 0.33333334 0.33333334 0.5555556
 0.         0.11111111 0.         0.11111111 0.33333334 0.
 0.         0.11111111 0.33333334 0.16666667 0.         1.
 0.         0.         0.33333334 0.11111111 0.         0.
 0.6666667  0.11111111 0.33333334 0.33333334 0.44444445 0.
 0.22222222 0.         0.         0.22222222 0.         0.5
 0.5        0.5555556  0.6666667  0.         1.         0.5555556
 0.7777778  0.33333334 0.         0.6666667  0.         0.6666667
 0.6666667  0.         0.         0.         0.33333334 0.
 0.         0.33333334 0.44444445 0.         0.         0.
 0.33333334 0.         0.         0.         0.         0.6666667
 0.         0.33333334 0.         0.33333334 0.         0.
 0.44444445 0.33333334 0.33333334 0.8888889  0.         0.33333334
 0.         0.         0.         0.33333334 0.         0.44444445
 0.22222222 0.33333334 0.         0.         0.44444445 0.
 0.         0.         0.         0.11111111 0.         0.5555556
 0.         0.         0.33333334 0.6666667  0.         0.
 0.22222222 0.5555556  0.         0.         0.         0.
 0.         0.5        0.         0.8888889  0.         0.        ]
Top k classes which perform poorly are:  [62, 82, 80, 78, 76, 75, 74, 73, 71, 83, 70, 66, 65, 63, 124, 61, 58, 56, 51, 69, 88, 90, 91, 122, 120, 119, 118, 117, 116, 113, 112, 109, 108, 106, 104, 103, 102, 101, 99, 98, 94, 92, 46, 44, 125, 34, 31, 18, 35, 13, 12, 11, 28, 9, 8, 10, 20, 43, 1, 7, 23, 24, 6, 41, 30, 105, 37, 4, 21, 19, 14, 33, 25, 27, 45, 114, 42, 96, 97, 16, 15, 93, 89, 110, 5, 3, 2, 22, 85, 86, 55, 38, 64, 67, 39, 72, 32, 26, 79, 81, 95, 100, 68, 84, 40, 48, 47, 121, 49, 53, 115, 107, 17, 57, 59, 60, 50, 36, 77, 111, 54, 87, 123, 0, 52, 29]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [    7     3     4     6     5     3     6     5     4     3     3     3
     4     3    10     4     8     4     9     9   225     4     6     7
     4     4     3     5   783 16380     8    11     5     4     6     4
     7     3     4     7    13     3     4     5     7     8     6     5
     4     5     7     4    13     6     5     6     3     5     4    12
     5     6     3     4     4     4     3     9     5     3     4     5
     4     3     3     4     4     3     4     6     3     5     5   170
     5     7     4     6    16     5    58     4     4     3     4    14
     4     7     4     5     5     4     3     6     5    10     5     4
     3     5     4     6     5     5     6     8     4     8   427     5
     5     8     5    14     4    12]
CBFL per class weights: tensor([0.7098, 1.6235, 1.2238, 0.8240, 0.9839, 1.6235, 0.8240, 0.9839, 1.2238,
        1.6235, 1.6235, 1.6235, 1.2238, 1.6235, 0.5043, 1.2238, 0.6242, 1.2238,
        0.5576, 0.5576, 0.0538, 1.2238, 0.8240, 0.7098, 1.2238, 1.2238, 1.6235,
        0.9839, 0.0482, 0.0482, 0.6242, 0.4607, 0.9839, 1.2238, 0.8240, 1.2238,
        0.7098, 1.6235, 1.2238, 0.7098, 0.3937, 1.6235, 1.2238, 0.9839, 0.7098,
        0.6242, 0.8240, 0.9839, 1.2238, 0.9839, 0.7098, 1.2238, 0.3937, 0.8240,
        0.9839, 0.8240, 1.6235, 0.9839, 1.2238, 0.4244, 0.9839, 0.8240, 1.6235,
        1.2238, 1.2238, 1.2238, 1.6235, 0.5576, 0.9839, 1.6235, 1.2238, 0.9839,
        1.2238, 1.6235, 1.6235, 1.2238, 1.2238, 1.6235, 1.2238, 0.8240, 1.6235,
        0.9839, 0.9839, 0.0589, 0.9839, 0.7098, 1.2238, 0.8240, 0.3246, 0.9839,
        0.1092, 1.2238, 1.2238, 1.6235, 1.2238, 0.3674, 1.2238, 0.7098, 1.2238,
        0.9839, 0.9839, 1.2238, 1.6235, 0.8240, 0.9839, 0.5043, 0.9839, 1.2238,
        1.6235, 0.9839, 1.2238, 0.8240, 0.9839, 0.9839, 0.8240, 0.6242, 1.2238,
        0.6242, 0.0489, 0.9839, 0.9839, 0.6242, 0.9839, 0.3674, 1.2238, 0.4244],
       device='cuda:0')
S real T clipart Train Ep: 6000 lr0.007029266564879363 	 Loss Classification: 0.692217 Loss T 0.008443 Method MME


Labeled Target set: Average loss: 6.0998, Accuracy: 36/1080 F1 (3.3333%)


Test set: Average loss: 5.9131, Accuracy: 705/18312 F1 (3.8499%)


Val set: Average loss: 5.7938, Accuracy: 18/360 F1 (5.0000%)

best acc test 58.382481  acc val 5.000000 acc labeled target 3.333333
saving model...
S real T clipart Train Ep: 6100 lr0.006996496045111504 	 Loss Classification: 0.123583 Loss T 0.083015 Method MME

S real T clipart Train Ep: 6200 lr0.00696407980201184 	 Loss Classification: 0.543830 Loss T 0.107954 Method MME

S real T clipart Train Ep: 6300 lr0.006932011853915101 	 Loss Classification: 1.016767 Loss T 0.108134 Method MME

S real T clipart Train Ep: 6400 lr0.006900286356036734 	 Loss Classification: 0.328527 Loss T 0.101589 Method MME

S real T clipart Train Ep: 6500 lr0.006868897596529406 	 Loss Classification: 0.462671 Loss T 0.096149 Method MME


Labeled Target set: Average loss: 2.3969, Accuracy: 576/1080 F1 (53.3333%)


Test set: Average loss: 2.2027, Accuracy: 10123/18312 F1 (55.2807%)


Val set: Average loss: 2.3970, Accuracy: 193/360 F1 (53.6111%)

best acc test 58.382481  acc val 53.611111 acc labeled target 53.333333
saving model...
S real T clipart Train Ep: 6600 lr0.006837839992676177 	 Loss Classification: 0.521580 Loss T 0.107987 Method MME

S real T clipart Train Ep: 6700 lr0.006807108087214876 	 Loss Classification: 0.306347 Loss T 0.100554 Method MME

S real T clipart Train Ep: 6800 lr0.006776696544788352 	 Loss Classification: 0.274112 Loss T 0.085550 Method MME

S real T clipart Train Ep: 6900 lr0.006746600148515609 	 Loss Classification: 0.370484 Loss T 0.088184 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [0.6666667  1.         0.5555556  0.44444445 0.11111111 0.5555556
 0.33333334 0.5555556  0.6666667  0.         0.6666667  1.
 0.11111111 1.         0.5555556  0.22222222 0.8888889  1.
 0.22222222 0.6666667  0.         0.8333333  0.8888889  0.5555556
 0.33333334 0.7777778  0.7777778  0.6666667  0.         0.
 0.5555556  0.         0.33333334 0.33333334 0.         0.6666667
 0.5555556  1.         0.6666667  0.         0.6666667  0.5555556
 0.44444445 0.44444445 0.         0.         0.44444445 0.6666667
 0.33333334 0.6666667  0.6666667  0.5        0.8888889  0.5
 1.         0.7777778  1.         0.6666667  0.         0.6666667
 0.8888889  0.6666667  0.8888889  0.33333334 1.         0.7777778
 0.33333334 0.6666667  0.6666667  0.11111111 0.5555556  0.33333334
 1.         0.7777778  0.33333334 1.         0.7777778  1.
 0.11111111 0.7777778  0.         0.6666667  0.         0.
 0.22222222 0.6666667  0.6666667  1.         0.         0.33333334
 0.         0.44444445 0.         1.         0.44444445 0.5
 1.         0.33333334 1.         0.         1.         0.7777778
 0.22222222 0.11111111 0.         0.33333334 0.33333334 0.5555556
 0.33333334 0.5555556  0.33333334 1.         0.6666667  0.8888889
 1.         0.6666667  0.6666667  0.5555556  0.         0.8333333
 1.         0.6666667  0.6666667  1.         0.         0.6666667 ]
Top k classes which perform poorly are:  [124, 92, 104, 88, 58, 20, 31, 34, 83, 82, 39, 9, 90, 29, 45, 118, 28, 80, 99, 44, 69, 4, 103, 78, 12, 84, 18, 15, 102, 74, 48, 97, 33, 106, 105, 89, 24, 108, 110, 63, 66, 71, 32, 6, 94, 46, 43, 42, 91, 3, 95, 51, 53, 70, 36, 30, 107, 23, 109, 2, 5, 7, 14, 41, 117, 122, 81, 116, 115, 112, 85, 86, 121, 0, 125, 57, 40, 38, 47, 35, 68, 67, 49, 8, 61, 50, 59, 19, 27, 10, 101, 73, 26, 25, 65, 79, 76, 55, 119, 21, 22, 62, 113, 52, 60, 16, 114, 13, 120, 1, 123, 11, 75, 111, 37, 98, 96, 93, 54, 56, 87, 64, 72, 77, 17, 100]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [  14   93   32   39   13   29   14   52   46   15  157  394   41  396
   63   42   70   99   45   38   99   90   35   50   60   58   88   37
  362 8260   42   10   36  176   77  147   57  221   75   17  102   37
   39   20   49   27  169   35  224  126  107   38   93   70   62   93
  445  100   52   26   85   24  110   85  116   33   37   12   17   16
   45   76   66  458   15    9   50  193   10   74    4   34   15   79
   57   50   36   61   10   28   26   27   28   85   43  120  115   39
  133    9   57   82   87  102   58   17  211  178   36  102   87  181
  155   90   97   96   56  106  173   43   98  103   56  120   54  125]
CBFL per class weights: tensor([2.3843, 0.5153, 1.1379, 0.9651, 2.5551, 1.2378, 2.3843, 0.7689, 0.8454,
        2.2363, 0.3943, 0.3190, 0.9267, 0.3189, 0.6671, 0.9088, 0.6195, 0.4965,
        0.8602, 0.9858, 0.4965, 0.5257, 1.0553, 0.7923, 0.6911, 0.7085, 0.5331,
        1.0077, 0.3214, 0.3129, 0.9088, 3.2729, 1.0308, 0.3773, 0.5809, 0.4055,
        0.7176, 0.3510, 0.5911, 1.9926, 0.4880, 1.0077, 0.9651, 1.7186, 0.8047,
        1.3168, 0.3830, 1.0553, 0.3498, 0.4358, 0.4750, 0.9858, 0.5153, 0.6195,
        0.6749, 0.5153, 0.3166, 0.4936, 0.7689, 1.3609, 0.5448, 1.4602, 0.4678,
        0.5448, 0.4546, 1.1087, 1.0077, 2.7545, 1.9926, 2.1068, 0.8602, 0.5859,
        0.6454, 0.3161, 2.2363, 3.6186, 0.7923, 0.3655, 3.2729, 0.5965, 7.9421,
        1.0812, 2.2363, 0.5711, 0.7176, 0.7923, 1.0308, 0.6828, 3.2729, 1.2759,
        1.3609, 1.3168, 1.2759, 0.5448, 0.8919, 0.4467, 0.4567, 0.9651, 0.4245,
        3.6186, 0.7176, 0.5575, 0.5369, 0.4880, 0.7085, 1.9926, 0.3556, 0.3758,
        1.0308, 0.4880, 0.5369, 0.3735, 0.3964, 0.5257, 0.5025, 0.5056, 0.7271,
        0.4775, 0.3797, 0.8919, 0.4995, 0.4853, 0.7271, 0.4467, 0.7472, 0.4375],
       device='cuda:0')
S real T clipart Train Ep: 7000 lr0.006716813796678979 	 Loss Classification: 1.128662 Loss T 0.102325 Method MME


Labeled Target set: Average loss: 2.4622, Accuracy: 530/1080 F1 (49.0741%)


Test set: Average loss: 2.3105, Accuracy: 9594/18312 F1 (52.3919%)


Val set: Average loss: 2.4549, Accuracy: 185/360 F1 (51.3889%)

best acc test 58.382481  acc val 51.388889 acc labeled target 49.074074
saving model...
S real T clipart Train Ep: 7100 lr0.006687332499522798 	 Loss Classification: 0.228835 Loss T 0.109747 Method MME

S real T clipart Train Ep: 7200 lr0.006658151376159165 	 Loss Classification: 0.285319 Loss T 0.111367 Method MME

S real T clipart Train Ep: 7300 lr0.00662926565157663 	 Loss Classification: 0.491503 Loss T 0.092698 Method MME

S real T clipart Train Ep: 7400 lr0.006600670653747793 	 Loss Classification: 0.446083 Loss T 0.109569 Method MME

S real T clipart Train Ep: 7500 lr0.0065723618108320175 	 Loss Classification: 0.215105 Loss T 0.120028 Method MME


Labeled Target set: Average loss: 0.1917, Accuracy: 1031/1080 F1 (95.4630%)


Test set: Average loss: 2.0244, Accuracy: 10300/18312 F1 (56.2473%)


Val set: Average loss: 1.9558, Accuracy: 211/360 F1 (58.6111%)

best acc test 56.247270  acc val 58.611111 acc labeled target 95.462963
saving model...
S real T clipart Train Ep: 7600 lr0.006544334648469591 	 Loss Classification: 0.273837 Loss T 0.100149 Method MME

S real T clipart Train Ep: 7700 lr0.006516584787163857 	 Loss Classification: 0.425297 Loss T 0.090990 Method MME

S real T clipart Train Ep: 7800 lr0.006489107939747966 	 Loss Classification: 0.850235 Loss T 0.105262 Method MME

S real T clipart Train Ep: 7900 lr0.0064618999089330565 	 Loss Classification: 0.326276 Loss T 0.105906 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        0.8333333 1.        1.        1.        0.8888889
 0.8888889 0.8888889 0.8888889 0.8333333 1.        1.        1.
 0.8888889 1.        0.8888889 1.        1.        1.        0.8888889
 1.        1.        1.        0.7777778 1.        1.        1.
 0.8888889 0.8333333 0.8888889 1.        1.        1.        1.
 1.        1.        1.        1.        0.8888889 0.8888889 1.
 1.        0.8888889 1.        1.        1.        0.6666667 1.
 0.8888889 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        0.8888889 1.        1.        1.        1.
 1.        0.8888889 1.        0.6666667 1.        1.        0.8888889
 1.        1.        0.6666667 1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        0.8888889 0.8888889 0.8888889
 0.7777778 1.        1.        1.        1.        0.6666667 1.
 1.        1.        0.8333333 1.        0.8888889 1.        1.
 0.8888889 0.8888889 1.        1.        1.        1.        0.5555556
 1.        1.        1.        1.        1.        0.6666667 1.       ]
Top k classes which perform poorly are:  [118, 73, 47, 103, 124, 63, 79, 98, 24, 29, 10, 2, 107, 71, 95, 96, 43, 97, 40, 39, 30, 8, 28, 109, 9, 49, 76, 112, 20, 113, 7, 6, 14, 65, 16, 78, 90, 80, 77, 81, 91, 87, 86, 75, 85, 84, 83, 82, 89, 88, 0, 93, 123, 122, 121, 120, 119, 117, 116, 115, 114, 92, 111, 108, 106, 105, 104, 102, 74, 100, 99, 94, 110, 101, 62, 70, 35, 34, 33, 32, 31, 27, 26, 25, 23, 22, 36, 21, 18, 17, 15, 13, 12, 11, 5, 4, 3, 1, 19, 72, 37, 41, 69, 68, 67, 66, 64, 61, 60, 59, 58, 57, 38, 56, 54, 53, 52, 51, 50, 48, 46, 45, 44, 42, 55, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [  45   89   41   64   68   38   79   66   69   78  156  308   54  278
   93   66   97  101   78   79   70  113   38   67   67   66   77   55
  222 5003   65   99   56  176   99  157   75  119   79  146  110   45
   71   77   84  116  183   58  218  135  119   86  212  103   65   98
  316  105  128   88   85   81  109   92  125   50   69   76   43   59
   54   97   80  300   58  175   64  124  152   95 1025   53   75   61
   87   78   38   73  191   42   36   49   58   76   50  120  110   80
  114  260   74   91  101  123   79   58  170  167   36  110  117  172
  139  108  100  109   70  148  105   60  105  113   80  132   70  189]
CBFL per class weights: tensor([1.5337, 0.9438, 1.6522, 1.1762, 1.1270, 1.7577, 1.0183, 1.1508, 1.1156,
        1.0268, 0.7050, 0.5844, 1.3322, 0.5943, 0.9188, 1.1508, 0.8960, 0.8751,
        1.0268, 1.0183, 1.1046, 0.8220, 1.7577, 1.1387, 1.1387, 1.1508, 1.0356,
        1.3140, 0.6251, 0.5580, 1.1633, 0.8853, 1.2964, 0.6727, 0.8853, 0.7031,
        1.0540, 0.7999, 1.0183, 0.7252, 0.8341, 1.5337, 1.0938, 1.0356, 0.9787,
        0.8106, 0.6634, 1.2632, 0.6282, 0.7515, 0.7999, 0.9643, 0.6332, 0.8653,
        1.1633, 0.8906, 0.5823, 0.8559, 0.7710, 0.9505, 0.9714, 1.0018, 0.8383,
        0.9248, 0.7801, 1.4126, 1.1156, 1.0447, 1.5901, 1.2474, 1.3322, 0.8960,
        1.0100, 0.5868, 1.2632, 0.6741, 1.1762, 0.7832, 0.7127, 0.9071, 0.5580,
        1.3512, 1.0540, 1.2175, 0.9573, 1.0268, 1.7577, 1.0733, 0.6539, 1.6204,
        1.8380, 1.4348, 1.2632, 1.0447, 1.4126, 0.7964, 0.8341, 1.0100, 0.8181,
        0.6021, 1.0635, 0.9310, 0.8751, 0.7864, 1.0183, 1.2632, 0.6814, 0.6860,
        1.8380, 0.8341, 0.8070, 0.6784, 0.7413, 0.8426, 0.8801, 0.8383, 1.1046,
        0.7209, 0.8559, 1.2322, 0.8559, 0.8220, 1.0100, 0.7595, 1.1046, 0.6562],
       device='cuda:0')
S real T clipart Train Ep: 8000 lr0.006434956584934828 	 Loss Classification: 0.767043 Loss T 0.089113 Method MME


Labeled Target set: Average loss: 0.3509, Accuracy: 969/1080 F1 (89.7222%)


Test set: Average loss: 2.6463, Accuracy: 9040/18312 F1 (49.3665%)


Val set: Average loss: 2.5280, Accuracy: 190/360 F1 (52.7778%)

best acc test 56.247270  acc val 52.777778 acc labeled target 89.722222
saving model...
S real T clipart Train Ep: 8100 lr0.006408273943175546 	 Loss Classification: 0.796831 Loss T 0.094155 Method MME

S real T clipart Train Ep: 8200 lr0.006381848042058713 	 Loss Classification: 0.440502 Loss T 0.079564 Method MME

S real T clipart Train Ep: 8300 lr0.0063556750208137005 	 Loss Classification: 0.399488 Loss T 0.100180 Method MME

S real T clipart Train Ep: 8400 lr0.006329751097407787 	 Loss Classification: 0.642561 Loss T 0.084357 Method MME

S real T clipart Train Ep: 8500 lr0.0063040725665231365 	 Loss Classification: 1.265639 Loss T 0.103149 Method MME


Labeled Target set: Average loss: 0.0944, Accuracy: 1056/1080 F1 (97.7778%)


Test set: Average loss: 1.6265, Accuracy: 11951/18312 F1 (65.2632%)


Val set: Average loss: 1.5859, Accuracy: 236/360 F1 (65.5556%)

best acc test 65.263215  acc val 65.555556 acc labeled target 97.777778
saving model...
S real T clipart Train Ep: 8600 lr0.006278635797596355 	 Loss Classification: 0.464969 Loss T 0.089966 Method MME

S real T clipart Train Ep: 8700 lr0.006253437232918371 	 Loss Classification: 0.416158 Loss T 0.093716 Method MME

S real T clipart Train Ep: 8800 lr0.0062284733857924735 	 Loss Classification: 0.684550 Loss T 0.096270 Method MME

S real T clipart Train Ep: 8900 lr0.006203740838748417 	 Loss Classification: 0.170815 Loss T 0.083123 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        0.8888889 1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        0.8888889 1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        0.8888889 1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        0.6666667 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        0.8888889 1.        1.        1.        1.
 0.6666667 1.        1.        0.8888889 1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.8888889 1.        1.        1.        1.        1.        1.
 1.        0.8888889 1.        1.        1.        1.        1.
 1.        0.8888889 1.        1.        1.        0.6666667 1.
 1.        0.8888889 1.        1.        1.        0.8888889 1.
 1.        0.8888889 1.        1.        1.        1.        1.
 0.8888889 1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [63, 47, 103, 66, 84, 45, 29, 99, 106, 92, 110, 113, 18, 58, 69, 4, 119, 6, 73, 88, 87, 86, 85, 68, 70, 83, 82, 81, 80, 71, 79, 78, 72, 77, 76, 75, 74, 89, 0, 91, 123, 122, 121, 120, 118, 117, 116, 115, 114, 112, 111, 109, 108, 107, 105, 104, 102, 101, 100, 98, 97, 96, 67, 94, 93, 90, 95, 62, 64, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 5, 3, 2, 1, 30, 31, 32, 33, 124, 61, 60, 59, 57, 56, 55, 54, 53, 52, 51, 50, 65, 49, 46, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 48, 125]
