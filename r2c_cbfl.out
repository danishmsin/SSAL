Dataset multi Source real Target clipart Labeled num perclass 3 Network resnet34
126 classes in this dataset
Unlabelled Target Dataset Size:  18325
Labelled Target Dataset Size:  378
Misc. Labelled Target Dataset Size:  378
Bank keys - Target:  dict_keys(['feat_vec', 'labels', 'names', 'domain_identifier']) Source:  dict_keys(['feat_vec', 'labels', 'names', 'domain_identifier'])
Num  - Target:  18325 Source:  70358
Unlabeled Target Data Size: 381
S real T clipart Train Ep: 0 lr0.01 	 Loss Classification: 4.890052 Loss T 0.471103 Method MME

S real T clipart Train Ep: 100 lr0.009925650290240803 	 Loss Classification: 1.283135 Loss T 0.283160 Method MME

S real T clipart Train Ep: 200 lr0.009852577760521605 	 Loss Classification: 1.010528 Loss T 0.225192 Method MME

S real T clipart Train Ep: 300 lr0.009780748269686728 	 Loss Classification: 0.869402 Loss T 0.228012 Method MME

S real T clipart Train Ep: 400 lr0.009710128909124701 	 Loss Classification: 0.706364 Loss T 0.183431 Method MME

S real T clipart Train Ep: 500 lr0.00964068794694323 	 Loss Classification: 1.469412 Loss T 0.185112 Method MME


Labeled Target set: Average loss: 2.5030, Accuracy: 480/1080 F1 (44.4444%)


Test set: Average loss: 2.2922, Accuracy: 8974/18312 F1 (49.0061%)


Val set: Average loss: 2.3264, Accuracy: 170/360 F1 (47.2222%)

best acc test 49.006116  acc val 47.222222 acc labeled target 44.444444
saving model...
S real T clipart Train Ep: 600 lr0.00957239477517603 	 Loss Classification: 0.561415 Loss T 0.170170 Method MME

S real T clipart Train Ep: 700 lr0.009505219859830012 	 Loss Classification: 0.380528 Loss T 0.141688 Method MME

S real T clipart Train Ep: 800 lr0.009439134693595126 	 Loss Classification: 0.851286 Loss T 0.176668 Method MME

S real T clipart Train Ep: 900 lr0.009374111751051751 	 Loss Classification: 0.580462 Loss T 0.124207 Method MME

S real T clipart Train Ep: 1000 lr0.009310124446222227 	 Loss Classification: 0.623616 Loss T 0.118863 Method MME


Labeled Target set: Average loss: 2.3272, Accuracy: 542/1080 F1 (50.1852%)


Test set: Average loss: 2.1730, Accuracy: 9601/18312 F1 (52.4301%)


Val set: Average loss: 2.2719, Accuracy: 191/360 F1 (53.0556%)

best acc test 52.430100  acc val 53.055556 acc labeled target 50.185185
saving model...
S real T clipart Train Ep: 1100 lr0.00924714709232377 	 Loss Classification: 0.769284 Loss T 0.133322 Method MME

S real T clipart Train Ep: 1200 lr0.009185154863590003 	 Loss Classification: 0.405095 Loss T 0.117616 Method MME

S real T clipart Train Ep: 1300 lr0.00912412375903735 	 Loss Classification: 0.293154 Loss T 0.103858 Method MME

S real T clipart Train Ep: 1400 lr0.009064030568061049 	 Loss Classification: 0.436825 Loss T 0.126031 Method MME

S real T clipart Train Ep: 1500 lr0.009004852837753237 	 Loss Classification: 0.531897 Loss T 0.118180 Method MME


Labeled Target set: Average loss: 2.1374, Accuracy: 567/1080 F1 (52.5000%)


Test set: Average loss: 1.8829, Accuracy: 10691/18312 F1 (58.3825%)


Val set: Average loss: 1.9273, Accuracy: 211/360 F1 (58.6111%)

best acc test 58.382481  acc val 58.611111 acc labeled target 52.500000
saving model...
S real T clipart Train Ep: 1600 lr0.008946568841842816 	 Loss Classification: 0.499096 Loss T 0.104103 Method MME

S real T clipart Train Ep: 1700 lr0.008889157551163433 	 Loss Classification: 0.837162 Loss T 0.123159 Method MME

S real T clipart Train Ep: 1800 lr0.008832598605562044 	 Loss Classification: 0.691673 Loss T 0.119071 Method MME

S real T clipart Train Ep: 1900 lr0.008776872287166303 	 Loss Classification: 0.545851 Loss T 0.113298 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [0.5555556  0.6666667  0.5        0.33333334 0.         0.5
 0.33333334 0.5555556  0.6666667  0.11111111 0.5555556  0.33333334
 0.5        0.5555556  0.6666667  0.33333334 0.8888889  0.7777778
 0.8888889  0.44444445 0.         0.5555556  0.5        0.6666667
 0.8888889  0.33333334 0.5555556  0.33333334 0.         0.8888889
 0.33333334 0.22222222 0.44444445 1.         0.8333333  1.
 0.5555556  0.6666667  0.33333334 0.33333334 0.44444445 0.8333333
 0.33333334 0.44444445 0.         0.6666667  0.33333334 0.6666667
 0.22222222 0.6666667  0.7777778  0.6666667  0.7777778  0.6666667
 0.6666667  0.6666667  0.7777778  0.6666667  0.         0.6666667
 0.6666667  0.6666667  0.6666667  0.5555556  1.         0.6666667
 0.33333334 1.         0.7777778  0.11111111 0.22222222 0.6666667
 1.         0.         0.         0.5555556  0.         0.7777778
 0.11111111 0.7777778  0.         1.         0.11111111 0.
 1.         0.8888889  0.7777778  0.5555556  0.         0.33333334
 0.5555556  0.5555556  0.         0.8333333  0.         0.8888889
 0.22222222 0.44444445 0.6666667  0.44444445 0.8888889  0.6666667
 0.22222222 0.44444445 0.33333334 0.33333334 0.33333334 0.33333334
 0.         0.6666667  0.33333334 0.8888889  0.7777778  0.8888889
 0.7777778  0.6666667  0.6666667  0.7777778  0.         0.11111111
 0.5        0.6666667  1.         1.         0.33333334 0.7777778 ]
Top k classes which perform poorly are:  [73, 88, 92, 94, 83, 44, 74, 28, 80, 20, 118, 76, 108, 58, 4, 119, 69, 78, 9, 82, 31, 96, 48, 102, 70, 104, 38, 46, 42, 6, 124, 3, 39, 106, 11, 110, 15, 66, 107, 25, 105, 30, 27, 89, 103, 32, 97, 19, 40, 99, 43, 22, 5, 2, 120, 12, 63, 87, 75, 0, 91, 26, 21, 13, 10, 7, 90, 36, 98, 101, 109, 115, 116, 121, 62, 71, 49, 53, 47, 54, 55, 45, 57, 59, 60, 37, 61, 65, 14, 8, 1, 23, 51, 50, 112, 114, 17, 117, 125, 68, 56, 86, 77, 79, 52, 93, 41, 34, 95, 16, 18, 113, 100, 85, 24, 29, 111, 64, 81, 33, 67, 35, 72, 122, 123, 84]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 19  61  23  48  21  15  11  54  65   8  87  91  14  96 102  70 108 103
  67  57  50  97  36 124  56  42  23  43  36  13  64   7  44 228 125 167
  84 126  67  24 141  39  60  29 127  84 266  55 142 159 135  54 286  91
  20 138 331 107  39  73  98  30  81 135 156  39  37  29  20   4  38 147
  62  29  12  11  39  58  12 143   3  45  12  17  73  96  33  89   3  27
  17  51  17  56  48 163  88  76  65  33  78  64 107 165  77  17 213 198
  21 106  72 231 205 165 130 111  43 155   8  47 108 147  83 180  38 244]
CBFL per class weights: tensor([1.5372, 0.5830, 1.2948, 0.6982, 1.4044, 1.9095, 2.5532, 0.6380, 0.5571,
        3.4589, 0.4584, 0.4459, 2.0359, 0.4317, 0.4167, 0.5290, 0.4035, 0.4144,
        0.5453, 0.6128, 0.6765, 0.4291, 0.8802, 0.3751, 0.6209, 0.7760, 1.2948,
        0.7615, 0.8802, 2.1818, 0.5633, 3.9335, 0.7477, 0.2973, 0.3736, 0.3286,
        0.4687, 0.3721, 0.5453, 1.2468, 0.3527, 0.8241, 0.5901, 1.0569, 0.3706,
        0.4687, 0.2870, 0.6293, 0.3516, 0.3350, 0.3599, 0.6380, 0.2832, 0.4459,
        1.4675, 0.3562, 0.2772, 0.4056, 0.8241, 0.5140, 0.4265, 1.0266, 0.4798,
        0.3599, 0.3376, 0.8241, 0.8605, 1.0569, 1.4675, 6.7816, 0.8418, 0.3462,
        0.5762, 1.0569, 2.3520, 2.5532, 0.8241, 0.6049, 2.3520, 0.3505, 8.9970,
        0.7345, 2.3520, 1.7014, 0.5140, 0.4317, 0.9467, 0.4520, 8.9970, 1.1244,
        1.7014, 0.6663, 1.7014, 0.6209, 0.6982, 0.3317, 0.4552, 0.5003, 0.5571,
        0.9467, 0.4918, 0.5633, 0.4056, 0.3301, 0.4960, 1.7014, 0.3028, 0.3095,
        1.4044, 0.4077, 0.5189, 0.2963, 0.3062, 0.3301, 0.3664, 0.3975, 0.7615,
        0.3385, 3.4589, 0.7098, 0.4035, 0.3462, 0.4723, 0.3196, 0.8418, 0.2924],
       device='cuda:0')
S real T clipart Train Ep: 2000 lr0.008721959494934213 	 Loss Classification: 0.649910 Loss T 0.102134 Method MME


Labeled Target set: Average loss: 2.2094, Accuracy: 587/1080 F1 (54.3519%)


Test set: Average loss: 2.0464, Accuracy: 10617/18312 F1 (57.9784%)


Val set: Average loss: 2.1188, Accuracy: 203/360 F1 (56.3889%)

best acc test 58.382481  acc val 56.388889 acc labeled target 54.351852
saving model...
S real T clipart Train Ep: 2100 lr0.008667841720414475 	 Loss Classification: 0.342230 Loss T 0.085962 Method MME

S real T clipart Train Ep: 2200 lr0.008614501024650454 	 Loss Classification: 2.749452 Loss T 0.000521 Method MME

S real T clipart Train Ep: 2300 lr0.008561920016164943 	 Loss Classification: 4.038361 Loss T 0.005952 Method MME

S real T clipart Train Ep: 2400 lr0.008510081829966844 	 Loss Classification: 1.644222 Loss T 0.094149 Method MME

S real T clipart Train Ep: 2500 lr0.008458970107524513 	 Loss Classification: 2.150513 Loss T 0.015356 Method MME


Labeled Target set: Average loss: 5.4873, Accuracy: 66/1080 F1 (6.1111%)


Test set: Average loss: 5.2631, Accuracy: 1052/18312 F1 (5.7449%)


Val set: Average loss: 5.2475, Accuracy: 24/360 F1 (6.6667%)

best acc test 58.382481  acc val 6.666667 acc labeled target 6.111111
saving model...
S real T clipart Train Ep: 2600 lr0.008408568977653933 	 Loss Classification: 3.400423 Loss T 0.009278 Method MME

S real T clipart Train Ep: 2700 lr0.00835886303827305 	 Loss Classification: 2.643403 Loss T 0.012884 Method MME

S real T clipart Train Ep: 2800 lr0.008309837338976545 	 Loss Classification: 2.446283 Loss T 0.020176 Method MME

S real T clipart Train Ep: 2900 lr0.008261477364388068 	 Loss Classification: 2.725097 Loss T 0.000173 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [0.22222222 0.         0.         0.44444445 0.         0.
 0.         0.33333334 0.11111111 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.11111111 0.
 0.         0.33333334 0.         0.         0.         0.22222222
 0.33333334 0.11111111 0.44444445 0.         0.         0.
 0.         0.         0.         0.33333334 0.         0.
 0.         0.         0.         0.         0.         0.33333334
 0.         0.         0.         0.         0.11111111 0.11111111
 0.         0.         0.         0.11111111 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.22222222 0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.11111111 0.11111111 0.33333334 0.         1.         0.33333334
 0.44444445 0.         0.         0.33333334 0.         0.11111111
 0.         0.         0.22222222 0.         0.         0.22222222
 0.         0.         0.         0.         0.         0.
 0.22222222 0.         0.11111111 0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.11111111 0.22222222 0.         0.         0.        ]
Top k classes which perform poorly are:  [62, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 68, 67, 66, 65, 64, 63, 124, 61, 60, 59, 58, 82, 83, 87, 91, 123, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 56, 109, 106, 105, 104, 103, 102, 100, 99, 97, 96, 94, 92, 107, 55, 125, 5, 14, 28, 27, 6, 26, 9, 54, 10, 24, 11, 23, 21, 20, 19, 12, 18, 17, 16, 15, 4, 2, 13, 34, 51, 50, 49, 48, 1, 46, 45, 44, 33, 42, 41, 43, 38, 40, 35, 36, 37, 52, 57, 31, 8, 110, 84, 22, 95, 85, 121, 53, 122, 0, 101, 69, 108, 29, 98, 89, 7, 86, 93, 30, 39, 47, 25, 3, 32, 90, 88]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [    4     3     5     8     4     5     4     7     9     4     8     8
     6     9     9    10     9     6     6     6    10    11     6    10
     4     8     3     4     6     4     7     8     4    14    11     7
     8    12     8     5     8     4     8     8     8     5     7     6
     6     9     6     7    23     6     3     6    14    11     7     7
     9     4     7     9    13     7     5     7     5    20    11     8
     4     5     4     3     6     5     5     9   697     8     4     6
     6    13     5     6 16748     7     3     8     6     7     4     9
     5     6     5     6     9     5     7    15    11     7    12    10
     9     5     5    13     7     9     7     6     7    15     5     4
     8     7     8    12     5    11]
CBFL per class weights: tensor([1.5804, 2.0967, 1.2706, 0.8061, 1.5804, 1.2706, 1.5804, 0.9167, 0.7201,
        1.5804, 0.8061, 0.8061, 1.0642, 0.7201, 0.7201, 0.6513, 0.7201, 1.0642,
        1.0642, 1.0642, 0.6513, 0.5950, 1.0642, 0.6513, 1.5804, 0.8061, 2.0967,
        1.5804, 1.0642, 1.5804, 0.9167, 0.8061, 1.5804, 0.4745, 0.5950, 0.9167,
        0.8061, 0.5481, 0.8061, 1.2706, 0.8061, 1.5804, 0.8061, 0.8061, 0.8061,
        1.2706, 0.9167, 1.0642, 1.0642, 0.7201, 1.0642, 0.9167, 0.3017, 1.0642,
        2.0967, 1.0642, 0.4745, 0.5950, 0.9167, 0.9167, 0.7201, 1.5804, 0.9167,
        0.7201, 0.5084, 0.9167, 1.2706, 0.9167, 1.2706, 0.3420, 0.5950, 0.8061,
        1.5804, 1.2706, 1.5804, 2.0967, 1.0642, 1.2706, 1.2706, 0.7201, 0.0623,
        0.8061, 1.5804, 1.0642, 1.0642, 0.5084, 1.2706, 1.0642, 0.0623, 0.9167,
        2.0967, 0.8061, 1.0642, 0.9167, 1.5804, 0.7201, 1.2706, 1.0642, 1.2706,
        1.0642, 0.7201, 1.2706, 0.9167, 0.4450, 0.5950, 0.9167, 0.5481, 0.6513,
        0.7201, 1.2706, 1.2706, 0.5084, 0.9167, 0.7201, 0.9167, 1.0642, 0.9167,
        0.4450, 1.2706, 1.5804, 0.8061, 0.9167, 0.8061, 0.5481, 1.2706, 0.5950],
       device='cuda:0')
S real T clipart Train Ep: 3000 lr0.008213769018249545 	 Loss Classification: 1.609484 Loss T 0.161775 Method MME


Labeled Target set: Average loss: 4.9404, Accuracy: 203/1080 F1 (18.7963%)


Test set: Average loss: 4.5642, Accuracy: 3950/18312 F1 (21.5706%)


Val set: Average loss: 4.5753, Accuracy: 87/360 F1 (24.1667%)

best acc test 58.382481  acc val 24.166667 acc labeled target 18.796296
saving model...
S real T clipart Train Ep: 3100 lr0.008166698608209509 	 Loss Classification: 0.986320 Loss T 0.120423 Method MME

S real T clipart Train Ep: 3200 lr0.008120252831274708 	 Loss Classification: 0.754658 Loss T 0.133392 Method MME

S real T clipart Train Ep: 3300 lr0.008074418759891278 	 Loss Classification: 0.496889 Loss T 0.040008 Method MME

S real T clipart Train Ep: 3400 lr0.008029183828623731 	 Loss Classification: 0.911468 Loss T 0.037296 Method MME

S real T clipart Train Ep: 3500 lr0.007984535821401871 	 Loss Classification: 0.320128 Loss T 0.029789 Method MME


Labeled Target set: Average loss: 5.8374, Accuracy: 28/1080 F1 (2.5926%)


Test set: Average loss: 5.5990, Accuracy: 315/18312 F1 (1.7202%)


Val set: Average loss: 5.4842, Accuracy: 10/360 F1 (2.7778%)

best acc test 58.382481  acc val 2.777778 acc labeled target 2.592593
saving model...
S real T clipart Train Ep: 3600 lr0.007940462859307384 	 Loss Classification: 0.755632 Loss T 0.153165 Method MME

S real T clipart Train Ep: 3700 lr0.007896953388873518 	 Loss Classification: 1.050520 Loss T 0.124462 Method MME

S real T clipart Train Ep: 3800 lr0.007853996170872714 	 Loss Classification: 0.519985 Loss T 0.133783 Method MME

S real T clipart Train Ep: 3900 lr0.00781158026956848 	 Loss Classification: 0.577075 Loss T 0.104795 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [0.33333334 0.16666667 0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.33333334 0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.22222222
 0.44444445 0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.33333334 0.         0.
 0.         0.         0.         0.         0.         0.22222222
 1.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.         0.         0.         0.         0.
 0.         0.16666667 0.         0.         0.         0.        ]
Top k classes which perform poorly are:  [62, 91, 88, 87, 86, 85, 84, 83, 82, 80, 79, 78, 77, 92, 76, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 75, 93, 94, 95, 123, 122, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 103, 102, 101, 100, 99, 98, 97, 96, 124, 61, 125, 30, 31, 3, 29, 28, 27, 26, 25, 24, 23, 4, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 32, 6, 2, 46, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 33, 5, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 45, 1, 121, 59, 89, 81, 22, 0, 60, 90]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [  18   61   23   27   17   10    9   28   11    7   25   22   77   46
   48   23   39   59   52   33    9   33   24   29   49   24  123   66
   15   17   31    6   38   17   29   95   29   33   26   57   68   17
   15   12   50   81  122   19   81   51   97   35  128   59   80   60
   28   64   23   37   54   35   60   21   41   25   36   16   14    8
   14   70   61   20   15  137   23   34    9   75  154   14  114    6
   57   27   19   60 3815   19 9512   16   10   29   39   70   41   51
   58    9   18   38   47   33   26    8   61   43   10   37   21   95
   92   26   74   81   15   56    6   27   61   83   44   63   48  108]
CBFL per class weights: tensor([1.3183, 0.4760, 1.0570, 0.9179, 1.3890, 2.2815, 2.5225, 0.8894, 2.0844,
        3.2113, 0.9819, 1.0997, 0.4049, 0.5893, 0.5700, 1.0570, 0.6728, 0.4877,
        0.5360, 0.7729, 2.5225, 0.7729, 1.0179, 0.8629, 0.5610, 1.0179, 0.3075,
        0.4499, 1.5589, 1.3890, 0.8149, 3.7279, 0.6872, 1.3890, 0.8629, 0.3547,
        0.8629, 0.7729, 0.9487, 0.5002, 0.4406, 1.3890, 1.5589, 1.9201, 0.5523,
        0.3917, 0.3088, 1.2550, 0.3917, 0.5440, 0.3503, 0.7356, 0.3014, 0.4877,
        0.3949, 0.4817, 0.8894, 0.4599, 1.0570, 0.7025, 0.5209, 0.7356, 0.4817,
        1.1465, 0.6460, 0.9819, 0.7186, 1.4686, 1.6621, 2.8238, 1.6621, 0.4319,
        0.4760, 1.1980, 1.5589, 0.2918, 1.0570, 0.7537, 2.5225, 0.4121, 0.2771,
        1.6621, 0.3199, 3.7279, 0.5002, 0.9179, 1.2550, 0.4817, 0.2182, 1.2550,
        0.2182, 1.4686, 2.2815, 0.8629, 0.6728, 0.4319, 0.6460, 0.5440, 0.4939,
        2.5225, 1.3183, 0.6872, 0.5795, 0.7729, 0.9487, 2.8238, 0.4760, 0.6217,
        2.2815, 0.7025, 1.1465, 0.3547, 0.3616, 0.9487, 0.4158, 0.3917, 1.5589,
        0.5069, 3.7279, 0.9179, 0.4760, 0.3856, 0.6104, 0.4651, 0.5700, 0.3294],
       device='cuda:0')
S real T clipart Train Ep: 4000 lr0.007769695042409123 	 Loss Classification: 1.450332 Loss T 0.139808 Method MME


Labeled Target set: Average loss: 3.3142, Accuracy: 350/1080 F1 (32.4074%)


Test set: Average loss: 3.1101, Accuracy: 6771/18312 F1 (36.9758%)


Val set: Average loss: 3.1579, Accuracy: 137/360 F1 (38.0556%)

best acc test 58.382481  acc val 38.055556 acc labeled target 32.407407
saving model...
S real T clipart Train Ep: 4100 lr0.007728330130142108 	 Loss Classification: 0.975495 Loss T 0.094830 Method MME

S real T clipart Train Ep: 4200 lr0.007687475447329114 	 Loss Classification: 0.806840 Loss T 0.063618 Method MME

S real T clipart Train Ep: 4300 lr0.0076471211732427845 	 Loss Classification: 0.559856 Loss T 0.065795 Method MME

S real T clipart Train Ep: 4400 lr0.007607257743127307 	 Loss Classification: 0.560677 Loss T 0.142362 Method MME

S real T clipart Train Ep: 4500 lr0.007567875839805858 	 Loss Classification: 1.402980 Loss T 0.168780 Method MME


Labeled Target set: Average loss: 2.7069, Accuracy: 487/1080 F1 (45.0926%)


Test set: Average loss: 2.4708, Accuracy: 9002/18312 F1 (49.1590%)


Val set: Average loss: 2.4956, Accuracy: 172/360 F1 (47.7778%)

best acc test 58.382481  acc val 47.777778 acc labeled target 45.092593
saving model...
S real T clipart Train Ep: 4600 lr0.007528966385618854 	 Loss Classification: 0.619852 Loss T 0.051802 Method MME

S real T clipart Train Ep: 4700 lr0.007490520534677821 	 Loss Classification: 0.356041 Loss T 0.011244 Method MME

S real T clipart Train Ep: 4800 lr0.007452529665420465 	 Loss Classification: 0.380421 Loss T 0.129102 Method MME

S real T clipart Train Ep: 4900 lr0.007414985373453289 	 Loss Classification: 1.066017 Loss T 0.054670 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [0.8888889  0.44444445 0.5555556  0.44444445 0.11111111 0.33333334
 0.5        0.44444445 0.         0.11111111 0.6666667  0.8888889
 0.5555556  0.         0.33333334 0.         0.8888889  0.7777778
 0.33333334 0.8888889  0.         0.5555556  1.         0.22222222
 0.5        0.44444445 0.5555556  0.6666667  0.         0.6666667
 0.44444445 0.22222222 0.44444445 0.         0.33333334 0.7777778
 0.33333334 0.22222222 0.33333334 0.22222222 0.33333334 0.11111111
 0.7777778  1.         0.         0.7777778  0.33333334 0.44444445
 0.33333334 0.6666667  0.8888889  0.11111111 1.         0.6666667
 0.16666667 1.         0.         0.5        0.22222222 0.6666667
 0.7777778  0.44444445 0.44444445 0.22222222 0.5        0.44444445
 0.22222222 0.44444445 1.         0.         0.5555556  0.33333334
 0.33333334 0.5555556  0.8333333  0.         0.44444445 1.
 0.11111111 0.5555556  0.         1.         0.         0.
 0.6666667  1.         0.6666667  1.         0.         0.
 0.         0.33333334 0.11111111 0.6666667  0.44444445 0.6666667
 0.33333334 0.33333334 0.44444445 0.5555556  0.8888889  0.5555556
 0.         0.11111111 0.         0.22222222 0.11111111 0.6666667
 0.         0.6666667  0.33333334 0.6666667  0.8888889  0.44444445
 0.5555556  0.6666667  0.33333334 0.         0.5        0.6666667
 0.6666667  0.6666667  0.5555556  1.         0.         0.22222222]
Top k classes which perform poorly are:  [88, 56, 69, 75, 44, 80, 82, 33, 83, 89, 28, 90, 20, 102, 15, 124, 108, 13, 117, 104, 8, 9, 106, 41, 78, 103, 51, 92, 4, 54, 66, 58, 39, 37, 125, 31, 63, 105, 23, 110, 18, 97, 96, 116, 71, 72, 48, 38, 46, 91, 5, 40, 14, 36, 34, 76, 94, 67, 98, 65, 113, 62, 47, 1, 30, 25, 3, 61, 32, 7, 118, 24, 6, 57, 64, 114, 26, 79, 101, 12, 73, 70, 21, 99, 122, 2, 95, 107, 10, 27, 93, 109, 86, 84, 115, 49, 119, 120, 121, 53, 59, 29, 111, 60, 17, 45, 35, 42, 74, 0, 100, 112, 16, 19, 11, 50, 123, 55, 68, 52, 22, 81, 85, 87, 43, 77]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [   6   20    7    9   11    7    7   12   14    6   18   18   19   16
   16   11   22   18   20   16 5809   18   10   14   17   18   19   16
    7    3   22  288   13   18   21   21    9   15   17   14   10    7
   15   51   16   12   22   11   26   13   27    8   34   18   11   13
   44   24   13   10   21   10   12   14   17   15   11    7   15    7
   16   33   25   11    9   19   14   13   56   19   19    9   22 3689
   13   20   10   19  417   10 1116   10   12   13   14   27    9   11
   16   17   19   17   13   17   26   63   22   10    9   12   12   31
   24   24   20   24   13   23 5332   11   12   23   19   31   13   30]
CBFL per class weights: tensor([2.2627, 0.7272, 1.9492, 1.5311, 1.2652, 1.9492, 1.9492, 1.1655, 1.0088,
        2.2627, 0.8002, 0.8002, 0.7617, 0.8914, 0.8914, 1.2652, 0.6675, 0.8002,
        0.7272, 0.8914, 0.1324, 0.8002, 1.3848, 1.0088, 0.8431, 0.8002, 0.7617,
        0.8914, 1.9492, 4.4583, 0.6675, 0.1402, 1.0811, 0.8002, 0.6959, 0.6959,
        1.5311, 0.9462, 0.8431, 1.0088, 1.3848, 1.9492, 0.9462, 0.3302, 0.8914,
        1.1655, 0.6675, 1.2652, 0.5758, 1.0811, 0.5572, 1.7140, 0.4575, 0.8002,
        1.2652, 1.0811, 0.3705, 0.6178, 1.0811, 1.3848, 0.6959, 1.3848, 1.1655,
        1.0088, 0.8431, 0.9462, 1.2652, 1.9492, 0.9462, 1.9492, 0.8914, 0.4691,
        0.5960, 1.2652, 1.5311, 0.7617, 1.0088, 1.0811, 0.3077, 0.7617, 0.7617,
        1.5311, 0.6675, 0.1324, 1.0811, 0.7272, 1.3848, 0.7617, 0.1344, 1.3848,
        0.1324, 1.3848, 1.1655, 1.0811, 1.0088, 0.5572, 1.5311, 1.2652, 0.8914,
        0.8431, 0.7617, 0.8431, 1.0811, 0.8431, 0.5758, 0.2823, 0.6675, 1.3848,
        1.5311, 1.1655, 1.1655, 0.4946, 0.6178, 0.6178, 0.7272, 0.6178, 1.0811,
        0.6416, 0.1324, 1.2652, 1.1655, 0.6416, 0.7617, 0.4946, 1.0811, 0.5087],
       device='cuda:0')
S real T clipart Train Ep: 5000 lr0.007377879464668811 	 Loss Classification: 0.738946 Loss T 0.013077 Method MME


Labeled Target set: Average loss: 8.4414, Accuracy: 9/1080 F1 (0.8333%)


Test set: Average loss: 8.5159, Accuracy: 84/18312 F1 (0.4587%)


Val set: Average loss: 8.1324, Accuracy: 3/360 F1 (0.8333%)

best acc test 58.382481  acc val 0.833333 acc labeled target 0.833333
saving model...
S real T clipart Train Ep: 5100 lr0.007341203948625087 	 Loss Classification: 0.355591 Loss T 0.029404 Method MME

S real T clipart Train Ep: 5200 lr0.007304951032175895 	 Loss Classification: 0.654977 Loss T 0.020952 Method MME

S real T clipart Train Ep: 5300 lr0.007269113113340497 	 Loss Classification: 0.789559 Loss T 0.097780 Method MME

S real T clipart Train Ep: 5400 lr0.007233682775402483 	 Loss Classification: 0.419525 Loss T 0.121865 Method MME

S real T clipart Train Ep: 5500 lr0.007198652781227704 	 Loss Classification: 0.780941 Loss T 0.072424 Method MME


Labeled Target set: Average loss: 4.1624, Accuracy: 220/1080 F1 (20.3704%)


Test set: Average loss: 3.8915, Accuracy: 3971/18312 F1 (21.6852%)


Val set: Average loss: 3.8108, Accuracy: 84/360 F1 (23.3333%)

best acc test 58.382481  acc val 23.333333 acc labeled target 20.370370
saving model...
S real T clipart Train Ep: 5600 lr0.007164016067791809 	 Loss Classification: 0.166235 Loss T 0.047368 Method MME

S real T clipart Train Ep: 5700 lr0.0071297657409083726 	 Loss Classification: 1.515623 Loss T 0.008207 Method MME

S real T clipart Train Ep: 5800 lr0.0070958950701490225 	 Loss Classification: 0.289977 Loss T 0.009504 Method MME

S real T clipart Train Ep: 5900 lr0.007062397483947426 	 Loss Classification: 0.355958 Loss T 0.005534 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [0.8888889  0.         0.33333334 0.33333334 0.11111111 0.33333334
 0.         0.         0.         0.         0.         0.
 0.         0.         0.11111111 0.33333334 0.33333334 0.5555556
 0.         0.11111111 0.         0.11111111 0.33333334 0.
 0.         0.11111111 0.33333334 0.16666667 0.         1.
 0.         0.         0.33333334 0.11111111 0.         0.
 0.6666667  0.11111111 0.33333334 0.33333334 0.44444445 0.
 0.22222222 0.         0.         0.22222222 0.         0.5
 0.5        0.5555556  0.6666667  0.         1.         0.5555556
 0.7777778  0.33333334 0.         0.6666667  0.         0.6666667
 0.6666667  0.         0.         0.         0.33333334 0.
 0.         0.33333334 0.44444445 0.         0.         0.
 0.33333334 0.         0.         0.         0.         0.6666667
 0.         0.33333334 0.         0.33333334 0.         0.
 0.44444445 0.33333334 0.33333334 0.8888889  0.         0.33333334
 0.         0.         0.         0.33333334 0.         0.44444445
 0.22222222 0.33333334 0.         0.         0.44444445 0.
 0.         0.         0.         0.11111111 0.         0.5555556
 0.         0.         0.33333334 0.6666667  0.         0.
 0.22222222 0.5555556  0.         0.         0.         0.
 0.         0.5        0.         0.8888889  0.         0.        ]
Top k classes which perform poorly are:  [62, 82, 80, 78, 76, 75, 74, 73, 71, 83, 70, 66, 65, 63, 124, 61, 58, 56, 51, 69, 88, 90, 91, 122, 120, 119, 118, 117, 116, 113, 112, 109, 108, 106, 104, 103, 102, 101, 99, 98, 94, 92, 46, 44, 125, 34, 31, 18, 35, 13, 12, 11, 28, 9, 8, 10, 20, 43, 1, 7, 23, 24, 6, 41, 30, 105, 37, 4, 21, 19, 14, 33, 25, 27, 45, 114, 42, 96, 97, 16, 15, 93, 89, 110, 5, 3, 2, 22, 85, 86, 55, 38, 64, 67, 39, 72, 32, 26, 79, 81, 95, 100, 68, 84, 40, 48, 47, 121, 49, 53, 115, 107, 17, 57, 59, 60, 50, 36, 77, 111, 54, 87, 123, 0, 52, 29]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [    7     3     4     6     5     3     6     5     4     3     3     3
     4     3    10     4     8     4     9     9   225     4     6     7
     4     4     3     5   783 16380     8    11     5     4     6     4
     7     3     4     7    13     3     4     5     7     8     6     5
     4     5     7     4    13     6     5     6     3     5     4    12
     5     6     3     4     4     4     3     9     5     3     4     5
     4     3     3     4     4     3     4     6     3     5     5   170
     5     7     4     6    16     5    58     4     4     3     4    14
     4     7     4     5     5     4     3     6     5    10     5     4
     3     5     4     6     5     5     6     8     4     8   427     5
     5     8     5    14     4    12]
CBFL per class weights: tensor([0.7098, 1.6235, 1.2238, 0.8240, 0.9839, 1.6235, 0.8240, 0.9839, 1.2238,
        1.6235, 1.6235, 1.6235, 1.2238, 1.6235, 0.5043, 1.2238, 0.6242, 1.2238,
        0.5576, 0.5576, 0.0538, 1.2238, 0.8240, 0.7098, 1.2238, 1.2238, 1.6235,
        0.9839, 0.0482, 0.0482, 0.6242, 0.4607, 0.9839, 1.2238, 0.8240, 1.2238,
        0.7098, 1.6235, 1.2238, 0.7098, 0.3937, 1.6235, 1.2238, 0.9839, 0.7098,
        0.6242, 0.8240, 0.9839, 1.2238, 0.9839, 0.7098, 1.2238, 0.3937, 0.8240,
        0.9839, 0.8240, 1.6235, 0.9839, 1.2238, 0.4244, 0.9839, 0.8240, 1.6235,
        1.2238, 1.2238, 1.2238, 1.6235, 0.5576, 0.9839, 1.6235, 1.2238, 0.9839,
        1.2238, 1.6235, 1.6235, 1.2238, 1.2238, 1.6235, 1.2238, 0.8240, 1.6235,
        0.9839, 0.9839, 0.0589, 0.9839, 0.7098, 1.2238, 0.8240, 0.3246, 0.9839,
        0.1092, 1.2238, 1.2238, 1.6235, 1.2238, 0.3674, 1.2238, 0.7098, 1.2238,
        0.9839, 0.9839, 1.2238, 1.6235, 0.8240, 0.9839, 0.5043, 0.9839, 1.2238,
        1.6235, 0.9839, 1.2238, 0.8240, 0.9839, 0.9839, 0.8240, 0.6242, 1.2238,
        0.6242, 0.0489, 0.9839, 0.9839, 0.6242, 0.9839, 0.3674, 1.2238, 0.4244],
       device='cuda:0')
S real T clipart Train Ep: 6000 lr0.007029266564879363 	 Loss Classification: 0.692217 Loss T 0.008443 Method MME


Labeled Target set: Average loss: 6.0998, Accuracy: 36/1080 F1 (3.3333%)


Test set: Average loss: 5.9131, Accuracy: 705/18312 F1 (3.8499%)


Val set: Average loss: 5.7938, Accuracy: 18/360 F1 (5.0000%)

best acc test 58.382481  acc val 5.000000 acc labeled target 3.333333
saving model...
S real T clipart Train Ep: 6100 lr0.006996496045111504 	 Loss Classification: 0.123583 Loss T 0.083015 Method MME

S real T clipart Train Ep: 6200 lr0.00696407980201184 	 Loss Classification: 0.543830 Loss T 0.107954 Method MME

S real T clipart Train Ep: 6300 lr0.006932011853915101 	 Loss Classification: 1.016767 Loss T 0.108134 Method MME

S real T clipart Train Ep: 6400 lr0.006900286356036734 	 Loss Classification: 0.328527 Loss T 0.101589 Method MME

S real T clipart Train Ep: 6500 lr0.006868897596529406 	 Loss Classification: 0.462671 Loss T 0.096149 Method MME


Labeled Target set: Average loss: 2.3969, Accuracy: 576/1080 F1 (53.3333%)


Test set: Average loss: 2.2027, Accuracy: 10123/18312 F1 (55.2807%)


Val set: Average loss: 2.3970, Accuracy: 193/360 F1 (53.6111%)

best acc test 58.382481  acc val 53.611111 acc labeled target 53.333333
saving model...
S real T clipart Train Ep: 6600 lr0.006837839992676177 	 Loss Classification: 0.521580 Loss T 0.107987 Method MME

S real T clipart Train Ep: 6700 lr0.006807108087214876 	 Loss Classification: 0.306347 Loss T 0.100554 Method MME

S real T clipart Train Ep: 6800 lr0.006776696544788352 	 Loss Classification: 0.274112 Loss T 0.085550 Method MME

S real T clipart Train Ep: 6900 lr0.006746600148515609 	 Loss Classification: 0.370484 Loss T 0.088184 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [0.6666667  1.         0.5555556  0.44444445 0.11111111 0.5555556
 0.33333334 0.5555556  0.6666667  0.         0.6666667  1.
 0.11111111 1.         0.5555556  0.22222222 0.8888889  1.
 0.22222222 0.6666667  0.         0.8333333  0.8888889  0.5555556
 0.33333334 0.7777778  0.7777778  0.6666667  0.         0.
 0.5555556  0.         0.33333334 0.33333334 0.         0.6666667
 0.5555556  1.         0.6666667  0.         0.6666667  0.5555556
 0.44444445 0.44444445 0.         0.         0.44444445 0.6666667
 0.33333334 0.6666667  0.6666667  0.5        0.8888889  0.5
 1.         0.7777778  1.         0.6666667  0.         0.6666667
 0.8888889  0.6666667  0.8888889  0.33333334 1.         0.7777778
 0.33333334 0.6666667  0.6666667  0.11111111 0.5555556  0.33333334
 1.         0.7777778  0.33333334 1.         0.7777778  1.
 0.11111111 0.7777778  0.         0.6666667  0.         0.
 0.22222222 0.6666667  0.6666667  1.         0.         0.33333334
 0.         0.44444445 0.         1.         0.44444445 0.5
 1.         0.33333334 1.         0.         1.         0.7777778
 0.22222222 0.11111111 0.         0.33333334 0.33333334 0.5555556
 0.33333334 0.5555556  0.33333334 1.         0.6666667  0.8888889
 1.         0.6666667  0.6666667  0.5555556  0.         0.8333333
 1.         0.6666667  0.6666667  1.         0.         0.6666667 ]
Top k classes which perform poorly are:  [124, 92, 104, 88, 58, 20, 31, 34, 83, 82, 39, 9, 90, 29, 45, 118, 28, 80, 99, 44, 69, 4, 103, 78, 12, 84, 18, 15, 102, 74, 48, 97, 33, 106, 105, 89, 24, 108, 110, 63, 66, 71, 32, 6, 94, 46, 43, 42, 91, 3, 95, 51, 53, 70, 36, 30, 107, 23, 109, 2, 5, 7, 14, 41, 117, 122, 81, 116, 115, 112, 85, 86, 121, 0, 125, 57, 40, 38, 47, 35, 68, 67, 49, 8, 61, 50, 59, 19, 27, 10, 101, 73, 26, 25, 65, 79, 76, 55, 119, 21, 22, 62, 113, 52, 60, 16, 114, 13, 120, 1, 123, 11, 75, 111, 37, 98, 96, 93, 54, 56, 87, 64, 72, 77, 17, 100]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [  14   93   32   39   13   29   14   52   46   15  157  394   41  396
   63   42   70   99   45   38   99   90   35   50   60   58   88   37
  362 8260   42   10   36  176   77  147   57  221   75   17  102   37
   39   20   49   27  169   35  224  126  107   38   93   70   62   93
  445  100   52   26   85   24  110   85  116   33   37   12   17   16
   45   76   66  458   15    9   50  193   10   74    4   34   15   79
   57   50   36   61   10   28   26   27   28   85   43  120  115   39
  133    9   57   82   87  102   58   17  211  178   36  102   87  181
  155   90   97   96   56  106  173   43   98  103   56  120   54  125]
CBFL per class weights: tensor([2.3843, 0.5153, 1.1379, 0.9651, 2.5551, 1.2378, 2.3843, 0.7689, 0.8454,
        2.2363, 0.3943, 0.3190, 0.9267, 0.3189, 0.6671, 0.9088, 0.6195, 0.4965,
        0.8602, 0.9858, 0.4965, 0.5257, 1.0553, 0.7923, 0.6911, 0.7085, 0.5331,
        1.0077, 0.3214, 0.3129, 0.9088, 3.2729, 1.0308, 0.3773, 0.5809, 0.4055,
        0.7176, 0.3510, 0.5911, 1.9926, 0.4880, 1.0077, 0.9651, 1.7186, 0.8047,
        1.3168, 0.3830, 1.0553, 0.3498, 0.4358, 0.4750, 0.9858, 0.5153, 0.6195,
        0.6749, 0.5153, 0.3166, 0.4936, 0.7689, 1.3609, 0.5448, 1.4602, 0.4678,
        0.5448, 0.4546, 1.1087, 1.0077, 2.7545, 1.9926, 2.1068, 0.8602, 0.5859,
        0.6454, 0.3161, 2.2363, 3.6186, 0.7923, 0.3655, 3.2729, 0.5965, 7.9421,
        1.0812, 2.2363, 0.5711, 0.7176, 0.7923, 1.0308, 0.6828, 3.2729, 1.2759,
        1.3609, 1.3168, 1.2759, 0.5448, 0.8919, 0.4467, 0.4567, 0.9651, 0.4245,
        3.6186, 0.7176, 0.5575, 0.5369, 0.4880, 0.7085, 1.9926, 0.3556, 0.3758,
        1.0308, 0.4880, 0.5369, 0.3735, 0.3964, 0.5257, 0.5025, 0.5056, 0.7271,
        0.4775, 0.3797, 0.8919, 0.4995, 0.4853, 0.7271, 0.4467, 0.7472, 0.4375],
       device='cuda:0')
S real T clipart Train Ep: 7000 lr0.006716813796678979 	 Loss Classification: 1.128662 Loss T 0.102325 Method MME


Labeled Target set: Average loss: 2.4622, Accuracy: 530/1080 F1 (49.0741%)


Test set: Average loss: 2.3105, Accuracy: 9594/18312 F1 (52.3919%)


Val set: Average loss: 2.4549, Accuracy: 185/360 F1 (51.3889%)

best acc test 58.382481  acc val 51.388889 acc labeled target 49.074074
saving model...
S real T clipart Train Ep: 7100 lr0.006687332499522798 	 Loss Classification: 0.228835 Loss T 0.109747 Method MME

S real T clipart Train Ep: 7200 lr0.006658151376159165 	 Loss Classification: 0.285319 Loss T 0.111367 Method MME

S real T clipart Train Ep: 7300 lr0.00662926565157663 	 Loss Classification: 0.491503 Loss T 0.092698 Method MME

S real T clipart Train Ep: 7400 lr0.006600670653747793 	 Loss Classification: 0.446083 Loss T 0.109569 Method MME

S real T clipart Train Ep: 7500 lr0.0065723618108320175 	 Loss Classification: 0.215105 Loss T 0.120028 Method MME


Labeled Target set: Average loss: 0.1917, Accuracy: 1031/1080 F1 (95.4630%)


Test set: Average loss: 2.0244, Accuracy: 10300/18312 F1 (56.2473%)


Val set: Average loss: 1.9558, Accuracy: 211/360 F1 (58.6111%)

best acc test 56.247270  acc val 58.611111 acc labeled target 95.462963
saving model...
S real T clipart Train Ep: 7600 lr0.006544334648469591 	 Loss Classification: 0.273837 Loss T 0.100149 Method MME

S real T clipart Train Ep: 7700 lr0.006516584787163857 	 Loss Classification: 0.425297 Loss T 0.090990 Method MME

S real T clipart Train Ep: 7800 lr0.006489107939747966 	 Loss Classification: 0.850235 Loss T 0.105262 Method MME

S real T clipart Train Ep: 7900 lr0.0064618999089330565 	 Loss Classification: 0.326276 Loss T 0.105906 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        0.8333333 1.        1.        1.        0.8888889
 0.8888889 0.8888889 0.8888889 0.8333333 1.        1.        1.
 0.8888889 1.        0.8888889 1.        1.        1.        0.8888889
 1.        1.        1.        0.7777778 1.        1.        1.
 0.8888889 0.8333333 0.8888889 1.        1.        1.        1.
 1.        1.        1.        1.        0.8888889 0.8888889 1.
 1.        0.8888889 1.        1.        1.        0.6666667 1.
 0.8888889 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        0.8888889 1.        1.        1.        1.
 1.        0.8888889 1.        0.6666667 1.        1.        0.8888889
 1.        1.        0.6666667 1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        0.8888889 0.8888889 0.8888889
 0.7777778 1.        1.        1.        1.        0.6666667 1.
 1.        1.        0.8333333 1.        0.8888889 1.        1.
 0.8888889 0.8888889 1.        1.        1.        1.        0.5555556
 1.        1.        1.        1.        1.        0.6666667 1.       ]
Top k classes which perform poorly are:  [118, 73, 47, 103, 124, 63, 79, 98, 24, 29, 10, 2, 107, 71, 95, 96, 43, 97, 40, 39, 30, 8, 28, 109, 9, 49, 76, 112, 20, 113, 7, 6, 14, 65, 16, 78, 90, 80, 77, 81, 91, 87, 86, 75, 85, 84, 83, 82, 89, 88, 0, 93, 123, 122, 121, 120, 119, 117, 116, 115, 114, 92, 111, 108, 106, 105, 104, 102, 74, 100, 99, 94, 110, 101, 62, 70, 35, 34, 33, 32, 31, 27, 26, 25, 23, 22, 36, 21, 18, 17, 15, 13, 12, 11, 5, 4, 3, 1, 19, 72, 37, 41, 69, 68, 67, 66, 64, 61, 60, 59, 58, 57, 38, 56, 54, 53, 52, 51, 50, 48, 46, 45, 44, 42, 55, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [  45   89   41   64   68   38   79   66   69   78  156  308   54  278
   93   66   97  101   78   79   70  113   38   67   67   66   77   55
  222 5003   65   99   56  176   99  157   75  119   79  146  110   45
   71   77   84  116  183   58  218  135  119   86  212  103   65   98
  316  105  128   88   85   81  109   92  125   50   69   76   43   59
   54   97   80  300   58  175   64  124  152   95 1025   53   75   61
   87   78   38   73  191   42   36   49   58   76   50  120  110   80
  114  260   74   91  101  123   79   58  170  167   36  110  117  172
  139  108  100  109   70  148  105   60  105  113   80  132   70  189]
CBFL per class weights: tensor([1.5337, 0.9438, 1.6522, 1.1762, 1.1270, 1.7577, 1.0183, 1.1508, 1.1156,
        1.0268, 0.7050, 0.5844, 1.3322, 0.5943, 0.9188, 1.1508, 0.8960, 0.8751,
        1.0268, 1.0183, 1.1046, 0.8220, 1.7577, 1.1387, 1.1387, 1.1508, 1.0356,
        1.3140, 0.6251, 0.5580, 1.1633, 0.8853, 1.2964, 0.6727, 0.8853, 0.7031,
        1.0540, 0.7999, 1.0183, 0.7252, 0.8341, 1.5337, 1.0938, 1.0356, 0.9787,
        0.8106, 0.6634, 1.2632, 0.6282, 0.7515, 0.7999, 0.9643, 0.6332, 0.8653,
        1.1633, 0.8906, 0.5823, 0.8559, 0.7710, 0.9505, 0.9714, 1.0018, 0.8383,
        0.9248, 0.7801, 1.4126, 1.1156, 1.0447, 1.5901, 1.2474, 1.3322, 0.8960,
        1.0100, 0.5868, 1.2632, 0.6741, 1.1762, 0.7832, 0.7127, 0.9071, 0.5580,
        1.3512, 1.0540, 1.2175, 0.9573, 1.0268, 1.7577, 1.0733, 0.6539, 1.6204,
        1.8380, 1.4348, 1.2632, 1.0447, 1.4126, 0.7964, 0.8341, 1.0100, 0.8181,
        0.6021, 1.0635, 0.9310, 0.8751, 0.7864, 1.0183, 1.2632, 0.6814, 0.6860,
        1.8380, 0.8341, 0.8070, 0.6784, 0.7413, 0.8426, 0.8801, 0.8383, 1.1046,
        0.7209, 0.8559, 1.2322, 0.8559, 0.8220, 1.0100, 0.7595, 1.1046, 0.6562],
       device='cuda:0')
S real T clipart Train Ep: 8000 lr0.006434956584934828 	 Loss Classification: 0.767043 Loss T 0.089113 Method MME


Labeled Target set: Average loss: 0.3509, Accuracy: 969/1080 F1 (89.7222%)


Test set: Average loss: 2.6463, Accuracy: 9040/18312 F1 (49.3665%)


Val set: Average loss: 2.5280, Accuracy: 190/360 F1 (52.7778%)

best acc test 56.247270  acc val 52.777778 acc labeled target 89.722222
saving model...
S real T clipart Train Ep: 8100 lr0.006408273943175546 	 Loss Classification: 0.796831 Loss T 0.094155 Method MME

S real T clipart Train Ep: 8200 lr0.006381848042058713 	 Loss Classification: 0.440502 Loss T 0.079564 Method MME

S real T clipart Train Ep: 8300 lr0.0063556750208137005 	 Loss Classification: 0.399488 Loss T 0.100180 Method MME

S real T clipart Train Ep: 8400 lr0.006329751097407787 	 Loss Classification: 0.642561 Loss T 0.084357 Method MME

S real T clipart Train Ep: 8500 lr0.0063040725665231365 	 Loss Classification: 1.265639 Loss T 0.103149 Method MME


Labeled Target set: Average loss: 0.0944, Accuracy: 1056/1080 F1 (97.7778%)


Test set: Average loss: 1.6265, Accuracy: 11951/18312 F1 (65.2632%)


Val set: Average loss: 1.5859, Accuracy: 236/360 F1 (65.5556%)

best acc test 65.263215  acc val 65.555556 acc labeled target 97.777778
saving model...
S real T clipart Train Ep: 8600 lr0.006278635797596355 	 Loss Classification: 0.464969 Loss T 0.089966 Method MME

S real T clipart Train Ep: 8700 lr0.006253437232918371 	 Loss Classification: 0.416158 Loss T 0.093716 Method MME

S real T clipart Train Ep: 8800 lr0.0062284733857924735 	 Loss Classification: 0.684550 Loss T 0.096270 Method MME

S real T clipart Train Ep: 8900 lr0.006203740838748417 	 Loss Classification: 0.170815 Loss T 0.083123 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        0.8888889 1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        0.8888889 1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        0.8888889 1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        0.6666667 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        0.8888889 1.        1.        1.        1.
 0.6666667 1.        1.        0.8888889 1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.8888889 1.        1.        1.        1.        1.        1.
 1.        0.8888889 1.        1.        1.        1.        1.
 1.        0.8888889 1.        1.        1.        0.6666667 1.
 1.        0.8888889 1.        1.        1.        0.8888889 1.
 1.        0.8888889 1.        1.        1.        1.        1.
 0.8888889 1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [63, 47, 103, 66, 84, 45, 29, 99, 106, 92, 110, 113, 18, 58, 69, 4, 119, 6, 73, 88, 87, 86, 85, 68, 70, 83, 82, 81, 80, 71, 79, 78, 72, 77, 76, 75, 74, 89, 0, 91, 123, 122, 121, 120, 118, 117, 116, 115, 114, 112, 111, 109, 108, 107, 105, 104, 102, 101, 100, 98, 97, 96, 67, 94, 93, 90, 95, 62, 64, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 5, 3, 2, 1, 30, 31, 32, 33, 124, 61, 60, 59, 57, 56, 55, 54, 53, 52, 51, 50, 65, 49, 46, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 48, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [  47   89   69   81   65   60   66   83   73   66  165  287   89  237
  140   80  100  117  112   96   76  126   62   98   72   73   66   84
  161 3455   76   76   68  192  180  165  112  142   81  141  140   69
   89   82  109  124  202   92  222  161  124  105  237  137   87  118
  290  112  147   86   90   98  116  111  142   71  119   67   54   63
   64  146   86  231   59  135  119  126  135  115  749   86   69   64
  109  113   62   93  160   69   73   85   83   79  261  138  107  110
  141  201   87  113  131  142   89   63  184  182   56  149  162  176
  161  151  119  110   76  168   83   71  115  124  107  152  162  210]
CBFL per class weights: tensor([1.6801, 1.0699, 1.2646, 1.1356, 1.3186, 1.3967, 1.3045, 1.1180, 1.2167,
        1.3045, 0.7813, 0.6699, 1.0699, 0.6969, 0.8376, 1.1448, 0.9977, 0.9147,
        0.9363, 1.0219, 1.1842, 0.8808, 1.3639, 1.0095, 1.2281, 1.2167, 1.3045,
        1.1094, 0.7889, 0.6325, 1.1842, 1.1842, 1.2775, 0.7399, 0.7564, 0.7813,
        0.9363, 0.8322, 1.1356, 0.8349, 0.8376, 1.2646, 1.0699, 1.1267, 0.9502,
        0.8878, 0.7281, 1.0484, 0.7086, 0.7889, 0.8878, 0.9702, 0.6969, 0.8460,
        1.0851, 0.9107, 0.6688, 0.9363, 0.8195, 1.0930, 1.0625, 1.0095, 0.9189,
        0.9408, 0.8322, 1.2399, 0.9067, 1.2908, 1.5101, 1.3483, 1.3333, 0.8220,
        1.0930, 0.7013, 1.4140, 0.8518, 0.9067, 0.8808, 0.8518, 0.9231, 0.6328,
        1.0930, 1.2646, 1.3333, 0.9502, 0.9318, 1.3639, 1.0415, 0.7909, 1.2646,
        1.2167, 1.1011, 1.1180, 1.1543, 0.6820, 0.8432, 0.9600, 0.9455, 0.8349,
        0.7292, 1.0851, 0.9318, 0.8641, 0.8322, 1.0699, 1.3483, 0.7506, 0.7535,
        1.4696, 0.8148, 0.7870, 0.7625, 0.7889, 0.8101, 0.9067, 0.9455, 1.1842,
        0.7759, 1.1180, 1.2399, 0.9231, 0.8878, 0.9600, 0.8078, 0.7870, 0.7197],
       device='cuda:0')
S real T clipart Train Ep: 9000 lr0.006179236241810624 	 Loss Classification: 0.266376 Loss T 0.094199 Method MME


Labeled Target set: Average loss: 0.1404, Accuracy: 1044/1080 F1 (96.6667%)


Test set: Average loss: 1.7705, Accuracy: 11696/18312 F1 (63.8707%)


Val set: Average loss: 1.8758, Accuracy: 222/360 F1 (61.6667%)

best acc test 65.263215  acc val 61.666667 acc labeled target 96.666667
saving model...
S real T clipart Train Ep: 9100 lr0.006154956310818535 	 Loss Classification: 0.172966 Loss T 0.081647 Method MME

S real T clipart Train Ep: 9200 lr0.0061308978257973165 	 Loss Classification: 0.103773 Loss T 0.095897 Method MME

S real T clipart Train Ep: 9300 lr0.0061070576293771285 	 Loss Classification: 0.979461 Loss T 0.087324 Method MME

S real T clipart Train Ep: 9400 lr0.006083432625259276 	 Loss Classification: 0.616037 Loss T 0.088032 Method MME

S real T clipart Train Ep: 9500 lr0.006060019776727621 	 Loss Classification: 0.298619 Loss T 0.091279 Method MME


Labeled Target set: Average loss: 0.0643, Accuracy: 1061/1080 F1 (98.2407%)


Test set: Average loss: 1.5580, Accuracy: 12227/18312 F1 (66.7704%)


Val set: Average loss: 1.5257, Accuracy: 252/360 F1 (70.0000%)

best acc test 66.770424  acc val 70.000000 acc labeled target 98.240741
saving model...
S real T clipart Train Ep: 9600 lr0.00603681610520369 	 Loss Classification: 0.195919 Loss T 0.074761 Method MME

S real T clipart Train Ep: 9700 lr0.0060138186888439825 	 Loss Classification: 0.312536 Loss T 0.065052 Method MME

S real T clipart Train Ep: 9800 lr0.005991024661178045 	 Loss Classification: 0.692107 Loss T 0.083130 Method MME

S real T clipart Train Ep: 9900 lr0.0059684312097859115 	 Loss Classification: 0.687780 Loss T 0.067354 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        0.8888889 1.
 1.        1.        1.        1.        1.        1.        1.
 0.8888889 1.        1.        1.        1.        1.        1.
 1.        1.        0.8888889 0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        0.6666667 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.8888889 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        0.8888889 1.
 1.        1.        1.        1.        1.        1.        0.8888889
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        1.        0.8888889
 0.8888889 1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [91, 47, 80, 108, 45, 12, 82, 10, 118, 119, 31, 90, 68, 30, 21, 88, 87, 86, 85, 84, 89, 83, 0, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 81, 92, 97, 94, 123, 122, 121, 120, 117, 116, 115, 114, 113, 112, 111, 110, 93, 109, 106, 105, 104, 103, 102, 101, 100, 99, 98, 67, 96, 95, 107, 66, 62, 64, 29, 28, 27, 26, 25, 24, 23, 22, 20, 19, 18, 17, 32, 16, 14, 13, 11, 9, 8, 7, 6, 5, 4, 3, 2, 1, 15, 65, 33, 35, 63, 124, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 34, 51, 49, 48, 46, 44, 43, 42, 41, 40, 39, 38, 37, 36, 50, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [  54   88   74   85   66  112  263   92   85   59  170  274   90  216
  155   91  105  122  122  103   75  132   82  124   75   85   67   84
  144 2627   83   66   82  197  206  166  120  140   92  133  150   83
  100   96  110  133  210  103  224  181  134  120  246  157  100  134
  267  121  164  101   89   93  124  133  148   79  139   57   64   72
   68  149   93  190   72  114  148  130  128  123  584   91   89   80
  127  130   73   97  144   88   82   98  105   80  173  149  113  135
  159  179  106  110  138  140   89   71  202  198   52  159  179  189
  177  173  129  111   79  175   79   79  115  132  124  169  184  215]
CBFL per class weights: tensor([1.5852, 1.1310, 1.2655, 1.1559, 1.3694, 0.9828, 0.7148, 1.1005, 1.1559,
        1.4843, 0.8108, 0.7091, 1.1154, 0.7494, 0.8411, 1.1078, 1.0185, 0.9397,
        0.9397, 1.0296, 1.2541, 0.9038, 1.1827, 0.9320, 1.2541, 1.1559, 1.3550,
        1.1646, 0.8682, 0.6639, 1.1735, 1.3694, 1.1827, 0.7703, 0.7598, 0.8182,
        0.9477, 0.8792, 1.1005, 0.9005, 0.8528, 1.1735, 1.0473, 1.0727, 0.9925,
        0.9005, 0.7555, 1.0296, 0.7421, 0.7925, 0.8973, 0.9477, 0.7251, 0.8366,
        1.0473, 0.8973, 0.7126, 0.9436, 0.8221, 1.0413, 1.1231, 1.0933, 0.9320,
        0.9005, 0.8578, 1.2117, 0.8821, 1.5225, 1.3995, 1.2892, 1.3410, 0.8553,
        1.0933, 0.7794, 1.2892, 0.9735, 0.8578, 0.9105, 0.9174, 0.9358, 0.6658,
        1.1078, 1.1231, 1.2018, 0.9209, 0.9105, 1.2772, 1.0661, 0.8682, 1.1310,
        1.1827, 1.0597, 1.0185, 1.2018, 0.8055, 0.8553, 0.9781, 0.8942, 0.8323,
        0.7956, 1.0131, 0.9925, 0.8851, 0.8792, 1.1231, 1.3016, 0.7643, 0.7691,
        1.6312, 0.8323, 0.7956, 0.7808, 0.7988, 0.8055, 0.9139, 0.9876, 1.2117,
        0.8021, 1.2117, 1.2117, 0.9690, 0.9038, 0.9320, 0.8126, 0.7879, 0.7504],
       device='cuda:0')
S real T clipart Train Ep: 10000 lr0.005946035575013605 	 Loss Classification: 0.146119 Loss T 0.065144 Method MME


Labeled Target set: Average loss: 0.1229, Accuracy: 1051/1080 F1 (97.3148%)


Test set: Average loss: 1.7097, Accuracy: 11875/18312 F1 (64.8482%)


Val set: Average loss: 1.6889, Accuracy: 242/360 F1 (67.2222%)

best acc test 66.770424  acc val 67.222222 acc labeled target 97.314815
saving model...
S real T clipart Train Ep: 10100 lr0.005923835048725393 	 Loss Classification: 0.571461 Loss T 0.077151 Method MME

S real T clipart Train Ep: 10200 lr0.005901826973091593 	 Loss Classification: 0.357970 Loss T 0.062614 Method MME

S real T clipart Train Ep: 10300 lr0.005880008739410735 	 Loss Classification: 0.359260 Loss T 0.065645 Method MME

S real T clipart Train Ep: 10400 lr0.005858377786964935 	 Loss Classification: 0.061099 Loss T 0.086193 Method MME

S real T clipart Train Ep: 10500 lr0.005836931601907399 	 Loss Classification: 0.214217 Loss T 0.059371 Method MME


Labeled Target set: Average loss: 0.0814, Accuracy: 1056/1080 F1 (97.7778%)


Test set: Average loss: 1.3939, Accuracy: 12892/18312 F1 (70.4019%)


Val set: Average loss: 1.3473, Accuracy: 251/360 F1 (69.7222%)

best acc test 66.770424  acc val 69.722222 acc labeled target 97.777778
saving model...
S real T clipart Train Ep: 10600 lr0.005815667716181004 	 Loss Classification: 0.413239 Loss T 0.071340 Method MME

S real T clipart Train Ep: 10700 lr0.005794583706466925 	 Loss Classification: 0.420788 Loss T 0.077135 Method MME

S real T clipart Train Ep: 10800 lr0.005773677193162352 	 Loss Classification: 0.129236 Loss T 0.076867 Method MME

S real T clipart Train Ep: 10900 lr0.005752945839386346 	 Loss Classification: 0.522191 Loss T 0.079710 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        0.8888889
 1.        0.8888889 1.        1.        1.        1.        1.
 1.        0.8888889 1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        0.8888889 1.        0.8888889 1.        1.
 1.        1.        0.8888889 1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        0.8888889 1.        1.        1.        1.
 0.5       1.        1.        1.        1.        1.        0.8888889
 1.        1.        1.        0.7777778 1.        1.        1.
 1.        1.        1.        1.        1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.6666667 0.8888889
 0.8888889 1.        1.        0.7777778 1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [63, 91, 103, 108, 73, 104, 37, 15, 44, 83, 105, 8, 58, 6, 39, 69, 86, 85, 84, 89, 90, 88, 87, 0, 81, 79, 78, 77, 76, 75, 74, 72, 71, 70, 82, 80, 95, 93, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 92, 111, 109, 107, 106, 102, 101, 100, 99, 98, 97, 96, 68, 94, 110, 67, 62, 65, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 30, 17, 14, 13, 12, 11, 10, 9, 7, 5, 4, 3, 2, 1, 16, 66, 31, 33, 64, 124, 61, 60, 59, 57, 56, 55, 54, 53, 52, 51, 32, 50, 48, 47, 46, 45, 43, 42, 41, 40, 38, 36, 35, 34, 49, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [  55   86   79   90   78  113  222   94   95   64  171  269  106  194
  174   96  107  129  119  109   75  138   86  134   78   84   74  145
  121 2097   84   55   87  200  220  166  122  147   98  137  152   84
  109   98  125  140  232  107  227  189  136  124  250  171  102  147
  255  123  178   98   99   92  129  148  158   81  142   60   64  113
   78  161   93  170   85  101  150  128  120  119  451   91   94   73
  136  130   81  102  132   89   90  109  112   85  153  152  121  139
  181  169  115  187  146  159   94   82  211  200   51  174  207  195
  180  177  135  115   87  180   88   85  118  137  138  173  206  237]
CBFL per class weights: tensor([1.6049, 1.1777, 1.2437, 1.1449, 1.2542, 1.0040, 0.7635, 1.1150, 1.1079,
        1.4365, 0.8304, 0.7304, 1.0398, 0.7946, 0.8250, 1.1011, 1.0344, 0.9380,
        0.9769, 1.0239, 1.2873, 0.9085, 1.1777, 0.9211, 1.2542, 1.1954, 1.2989,
        0.8884, 0.9686, 0.6815, 1.1954, 1.6049, 1.1692, 0.7869, 0.7654, 0.8399,
        0.9645, 0.8830, 1.0877, 0.9115, 0.8704, 1.1954, 1.0239, 1.0877, 0.9528,
        0.9025, 0.7548, 1.0344, 0.7590, 0.8014, 0.9147, 0.9566, 0.7416, 0.8304,
        1.0628, 0.8830, 0.7384, 0.9605, 0.8183, 1.0877, 1.0813, 1.1296, 0.9380,
        0.8804, 0.8565, 1.2236, 0.8967, 1.5049, 1.4365, 1.0040, 1.2542, 0.8500,
        1.1222, 0.8322, 1.1864, 1.0688, 0.8753, 0.9416, 0.9727, 0.9769, 0.6889,
        1.1371, 1.1150, 1.3109, 0.9147, 0.9345, 1.2236, 1.0628, 0.9277, 1.1528,
        1.1449, 1.0239, 1.0088, 1.1864, 0.8680, 0.8704, 0.9686, 0.9055, 0.8134,
        0.8341, 0.9946, 0.8043, 0.8857, 0.8543, 1.1150, 1.2140, 0.7744, 0.7869,
        1.6993, 0.8250, 0.7787, 0.7933, 0.8150, 0.8199, 0.9178, 0.9946, 1.1692,
        0.8150, 1.1609, 1.1864, 0.9812, 0.9115, 0.9085, 0.8268, 0.7799, 0.7509],
       device='cuda:0')
S real T clipart Train Ep: 11000 lr0.005732387350012933 	 Loss Classification: 0.559523 Loss T 0.087143 Method MME


Labeled Target set: Average loss: 0.1168, Accuracy: 1051/1080 F1 (97.3148%)


Test set: Average loss: 1.5814, Accuracy: 12270/18312 F1 (67.0052%)


Val set: Average loss: 1.5429, Accuracy: 248/360 F1 (68.8889%)

best acc test 66.770424  acc val 68.888889 acc labeled target 97.314815
saving model...
S real T clipart Train Ep: 11100 lr0.005711999470730565 	 Loss Classification: 0.435080 Loss T 0.080642 Method MME

S real T clipart Train Ep: 11200 lr0.005691779987127103 	 Loss Classification: 0.262336 Loss T 0.071924 Method MME

S real T clipart Train Ep: 11300 lr0.005671726723799515 	 Loss Classification: 1.045993 Loss T 0.076496 Method MME

S real T clipart Train Ep: 11400 lr0.005651837543487509 	 Loss Classification: 0.530707 Loss T 0.079385 Method MME

S real T clipart Train Ep: 11500 lr0.005632110346230358 	 Loss Classification: 0.386116 Loss T 0.072977 Method MME


Labeled Target set: Average loss: 0.0477, Accuracy: 1065/1080 F1 (98.6111%)


Test set: Average loss: 1.3778, Accuracy: 12950/18312 F1 (70.7187%)


Val set: Average loss: 1.2967, Accuracy: 258/360 F1 (71.6667%)

best acc test 70.718654  acc val 71.666667 acc labeled target 98.611111
saving model...
S real T clipart Train Ep: 11600 lr0.005612543068546177 	 Loss Classification: 0.312158 Loss T 0.078161 Method MME

S real T clipart Train Ep: 11700 lr0.005593133682632953 	 Loss Classification: 0.102012 Loss T 0.072741 Method MME

S real T clipart Train Ep: 11800 lr0.005573880195590681 	 Loss Classification: 0.220312 Loss T 0.060860 Method MME

S real T clipart Train Ep: 11900 lr0.0055547806486639095 	 Loss Classification: 0.135238 Loss T 0.070236 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.
 0.8888889 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        0.6666667 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.8888889 1.
 1.        1.        1.        1.        1.        0.8888889 1.
 1.        1.        1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.6666667 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [47, 103, 63, 28, 87, 45, 75, 20, 82, 89, 88, 86, 84, 91, 83, 90, 85, 0, 80, 79, 78, 77, 76, 74, 73, 72, 71, 70, 69, 68, 67, 66, 81, 92, 94, 95, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 102, 101, 100, 99, 98, 97, 96, 93, 65, 62, 124, 27, 26, 25, 24, 23, 22, 21, 19, 18, 17, 16, 15, 29, 14, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 13, 30, 31, 32, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 46, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 64, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [  64   88   80   89   82  115  175   98   96   74  171  267  102  195
  181   97  107  132  128  122   67  140   92  134   81  100   83  150
  117 1720   89   65   87  203  231  168  119  149  100  142  165   92
  118  101  144  142  240  112  229  206  138  132  255  177  101  149
  248  129  175  100  105   95  134  146  167   89  141   69   64  111
   77  164   98  165   88   92  149  130  115  126  386   94   90   75
  137  132   94  104  127  101   92  115  115   86  135  161  125  147
  193  161  116  232  150  156  102   90  223  222   61  175  228  205
  186  179  139  115   90  185   83   89  124  141  142  173  211  238]
CBFL per class weights: tensor([1.4643, 1.1833, 1.2574, 1.1750, 1.2374, 1.0138, 0.8392, 1.1087, 1.1223,
        1.3240, 0.8464, 0.7456, 1.0833, 0.8086, 0.8291, 1.1155, 1.0544, 0.9456,
        0.9598, 0.9831, 1.4176, 0.9199, 1.1514, 0.9388, 1.2473, 1.0957, 1.2278,
        0.8923, 1.0046, 0.6947, 1.1750, 1.4482, 1.1918, 0.7985, 0.7702, 0.8521,
        0.9958, 0.8948, 1.0957, 0.9140, 0.8581, 1.1514, 1.0002, 1.0895, 0.9083,
        0.9140, 0.7631, 1.0283, 0.7719, 0.7949, 0.9260, 0.9456, 0.7527, 0.8358,
        1.0895, 0.8948, 0.7573, 0.9562, 0.8392, 1.0957, 1.0656, 1.1293, 0.9388,
        0.9028, 0.8541, 1.1750, 0.9169, 1.3889, 1.4643, 1.0333, 1.2893, 0.8601,
        1.1087, 0.8581, 1.1833, 1.1514, 0.8948, 0.9526, 1.0138, 0.9673, 0.7093,
        1.1365, 1.1670, 1.3121, 0.9291, 0.9456, 1.1365, 1.0714, 0.9635, 1.0895,
        1.1514, 1.0138, 1.0138, 1.2005, 0.9356, 0.8665, 0.9712, 0.9001, 0.8113,
        0.8665, 1.0092, 0.7694, 0.8923, 0.8776, 1.0833, 1.1670, 0.7773, 0.7783,
        1.5157, 0.8392, 0.7728, 0.7961, 0.8213, 0.8324, 0.9229, 1.0138, 1.1670,
        0.8228, 1.2278, 1.1750, 0.9751, 0.9169, 0.9140, 0.8428, 0.7894, 0.7646],
       device='cuda:0')
S real T clipart Train Ep: 12000 lr0.005535833116504121 	 Loss Classification: 0.203768 Loss T 0.101751 Method MME


Labeled Target set: Average loss: 0.0977, Accuracy: 1059/1080 F1 (98.0556%)


Test set: Average loss: 1.4332, Accuracy: 12864/18312 F1 (70.2490%)


Val set: Average loss: 1.3890, Accuracy: 245/360 F1 (68.0556%)

best acc test 70.718654  acc val 68.055556 acc labeled target 98.055556
saving model...
S real T clipart Train Ep: 12100 lr0.00551703570645129 	 Loss Classification: 0.333839 Loss T 0.077158 Method MME

S real T clipart Train Ep: 12200 lr0.005498386557834078 	 Loss Classification: 0.444366 Loss T 0.070548 Method MME

S real T clipart Train Ep: 12300 lr0.00547988384128807 	 Loss Classification: 0.149996 Loss T 0.059562 Method MME

S real T clipart Train Ep: 12400 lr0.005461525758091539 	 Loss Classification: 0.458529 Loss T 0.062127 Method MME

S real T clipart Train Ep: 12500 lr0.005443310539518174 	 Loss Classification: 0.397408 Loss T 0.076826 Method MME


Labeled Target set: Average loss: 0.0527, Accuracy: 1065/1080 F1 (98.6111%)


Test set: Average loss: 1.4198, Accuracy: 12971/18312 F1 (70.8333%)


Val set: Average loss: 1.3771, Accuracy: 258/360 F1 (71.6667%)

best acc test 70.833333  acc val 71.666667 acc labeled target 98.611111
saving model...
S real T clipart Train Ep: 12600 lr0.005425236446206295 	 Loss Classification: 0.477167 Loss T 0.045128 Method MME

S real T clipart Train Ep: 12700 lr0.005407301767544059 	 Loss Classification: 0.114571 Loss T 0.059072 Method MME

S real T clipart Train Ep: 12800 lr0.005389504821070177 	 Loss Classification: 0.262554 Loss T 0.059776 Method MME

S real T clipart Train Ep: 12900 lr0.005371843951889677 	 Loss Classification: 0.168045 Loss T 0.076134 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        0.6666667 0.8888889
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.5       1.        1.        0.8333333 1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        0.6666667 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [63, 103, 47, 66, 45, 69, 76, 101, 48, 89, 88, 87, 86, 85, 84, 83, 82, 81, 68, 67, 79, 90, 78, 77, 75, 74, 73, 72, 71, 70, 80, 91, 0, 65, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 92, 110, 108, 107, 106, 105, 104, 102, 100, 99, 98, 97, 96, 95, 94, 109, 93, 62, 124, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 28, 64, 29, 31, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 46, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 30, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [  67   95   80   87   86  114   83  105   95   70  175  263   94  187
  194   98  108  133  129  122   71  140   93  137   82  101   84  140
  105 1399   92  436   83  206  250  169  116  149  106  151  164   99
  123  100  145  146  239  114  234  215  141  135  256  185  102  147
  227  132  178  106  107  102  133  140  172   93  146   65   70  100
   83  156  101  147   98   91  148  130  105  131  315  104  101   81
  151  139  100  106  118   96   84  112  129   85   88  165  141  157
  204  142  122  169  152  158  103  103  235  226   61  199  241  197
  187  180  136  118   96  185   77   95  123  142  145  173  216  245]
CBFL per class weights: tensor([1.4283, 1.1379, 1.2669, 1.2008, 1.2095, 1.0262, 1.2371, 1.0736, 1.1379,
        1.3855, 0.8456, 0.7535, 1.1451, 0.8260, 0.8160, 1.1171, 1.0569, 0.9493,
        0.9634, 0.9906, 1.3721, 0.9269, 1.1525, 0.9362, 1.2468, 1.0977, 1.2277,
        0.9269, 1.0736, 0.6999, 1.1601, 0.7088, 1.2371, 0.8009, 0.7616, 0.8566,
        1.0168, 0.9016, 1.0679, 0.8964, 0.8666, 1.1105, 0.9865, 1.1040, 0.9124,
        0.9096, 0.7696, 1.0262, 0.7735, 0.7911, 0.9239, 0.9426, 0.7577, 0.8291,
        1.0915, 0.9069, 0.7795, 0.9527, 0.8404, 1.0679, 1.0623, 1.0915, 0.9493,
        0.9269, 0.8510, 1.1525, 0.9096, 1.4592, 1.3855, 1.1040, 1.2371, 0.8843,
        1.0977, 0.9069, 1.1171, 1.1678, 0.9042, 0.9598, 1.0736, 0.9562, 0.7307,
        1.0795, 1.0977, 1.2567, 0.8964, 0.9299, 1.1040, 1.0679, 1.0077, 1.1308,
        1.2277, 1.0360, 0.9634, 1.2185, 1.1922, 0.8646, 0.9239, 0.8819, 0.8033,
        0.9209, 0.9906, 0.8566, 0.8939, 0.8797, 1.0854, 1.0854, 0.7727, 0.7804,
        1.5271, 0.8095, 0.7681, 0.8120, 0.8260, 0.8370, 0.9394, 1.0077, 1.1308,
        0.8291, 1.2991, 1.1379, 0.9865, 0.9209, 0.9124, 0.8491, 0.7900, 0.7651],
       device='cuda:0')
S real T clipart Train Ep: 13000 lr0.0053543175321042955 	 Loss Classification: 0.123740 Loss T 0.061035 Method MME


Labeled Target set: Average loss: 0.0868, Accuracy: 1062/1080 F1 (98.3333%)


Test set: Average loss: 1.4751, Accuracy: 12819/18312 F1 (70.0033%)


Val set: Average loss: 1.5080, Accuracy: 251/360 F1 (69.7222%)

best acc test 70.833333  acc val 69.722222 acc labeled target 98.333333
saving model...
S real T clipart Train Ep: 13100 lr0.005336923960257046 	 Loss Classification: 0.587765 Loss T 0.061644 Method MME

S real T clipart Train Ep: 13200 lr0.0053196616607905645 	 Loss Classification: 0.138712 Loss T 0.074803 Method MME

S real T clipart Train Ep: 13300 lr0.0053025290835188275 	 Loss Classification: 0.090234 Loss T 0.065265 Method MME

S real T clipart Train Ep: 13400 lr0.005285524703111859 	 Loss Classification: 0.446945 Loss T 0.062661 Method MME

S real T clipart Train Ep: 13500 lr0.005268647018593052 	 Loss Classification: 0.109723 Loss T 0.045353 Method MME


Labeled Target set: Average loss: 0.0633, Accuracy: 1062/1080 F1 (98.3333%)


Test set: Average loss: 1.3848, Accuracy: 13137/18312 F1 (71.7398%)


Val set: Average loss: 1.4637, Accuracy: 256/360 F1 (71.1111%)

best acc test 70.833333  acc val 71.111111 acc labeled target 98.333333
saving model...
S real T clipart Train Ep: 13600 lr0.005251894552848747 	 Loss Classification: 0.528764 Loss T 0.058950 Method MME

S real T clipart Train Ep: 13700 lr0.005235265852149712 	 Loss Classification: 0.345700 Loss T 0.060618 Method MME

S real T clipart Train Ep: 13800 lr0.005218759485684187 	 Loss Classification: 0.733638 Loss T 0.055674 Method MME

S real T clipart Train Ep: 13900 lr0.005202374045102174 	 Loss Classification: 0.195376 Loss T 0.048004 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.8888889 1.
 1.        1.        0.8888889 0.8888889 1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.8888889 1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.6666667 1.
 1.        0.8888889 1.        1.        0.8888889 1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.8888889 1.       ]
Top k classes which perform poorly are:  [63, 91, 103, 82, 124, 3, 31, 30, 34, 109, 106, 26, 83, 84, 0, 86, 88, 89, 90, 85, 87, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 81, 92, 96, 94, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 93, 111, 108, 107, 105, 104, 102, 101, 100, 99, 98, 97, 67, 95, 110, 66, 62, 64, 28, 27, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 29, 15, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 2, 1, 14, 65, 32, 35, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 33, 49, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 48, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [  68   98   80   92   87  246   74  109  105   72  177  258  102  179
  192  101  109  134  134  135   72  144   98  142   85  107   83  134
   97 1152  100  300   92  203  256  175  150  142  112  158  163   93
  139  103  146  150  245  110  236  211  141  140  258  188  101  148
  229  136  187  103  113  108  138  150  178   94  148   53   80  110
   87  158  105  148  101   82  151  133  102  134  268  102  108   80
  149  143   94  103  116   96   92  109  141   89   90  167  142  158
  207  140  123  166  156  160  107  107  237  234   64  202  246  202
  190  181  139  121   97  188   75   93  128  143  146  175  225  250]
CBFL per class weights: tensor([1.4276, 1.1281, 1.2794, 1.1715, 1.2126, 0.7720, 1.3472, 1.0619, 1.0842,
        1.3724, 0.8504, 0.7640, 1.1022, 0.8470, 0.8269, 1.1085, 1.0619, 0.9553,
        0.9553, 0.9519, 1.3724, 0.9242, 1.1281, 0.9300, 1.2305, 1.0728, 1.2493,
        0.9553, 1.1350, 0.7068, 1.1149, 0.7433, 1.1715, 0.8124, 0.7652, 0.8539,
        0.9079, 0.9300, 1.0463, 0.8883, 0.8773, 1.1639, 0.9391, 1.0961, 0.9186,
        0.9079, 0.7727, 1.0566, 0.7796, 0.8032, 0.9330, 0.9360, 0.7640, 0.8327,
        1.1085, 0.9131, 0.7854, 0.9486, 0.8342, 1.0961, 1.0413, 1.0673, 0.9422,
        0.9079, 0.8487, 1.1564, 0.9131, 1.7116, 1.2794, 1.0566, 1.2126, 0.8883,
        1.0842, 0.9131, 1.1085, 1.2591, 0.9053, 0.9587, 1.1022, 0.9553, 0.7581,
        1.1022, 1.0673, 1.2794, 0.9105, 0.9271, 1.1564, 1.0961, 1.0268, 1.1420,
        1.1715, 1.0619, 0.9330, 1.1956, 1.1874, 0.8690, 0.9300, 0.8883, 0.8077,
        0.9360, 0.9962, 0.8711, 0.8930, 0.8838, 1.0728, 1.0728, 0.7788, 0.7812,
        1.4899, 0.8137, 0.7720, 0.8137, 0.8297, 0.8436, 0.9391, 1.0046, 1.1350,
        0.8327, 1.3351, 1.1639, 0.9766, 0.9271, 0.9186, 0.8539, 0.7890, 0.7692],
       device='cuda:0')
S real T clipart Train Ep: 14000 lr0.005186108144070652 	 Loss Classification: 0.394687 Loss T 0.057122 Method MME


Labeled Target set: Average loss: 0.1062, Accuracy: 1058/1080 F1 (97.9630%)


Test set: Average loss: 1.4994, Accuracy: 12905/18312 F1 (70.4729%)


Val set: Average loss: 1.5582, Accuracy: 250/360 F1 (69.4444%)

best acc test 70.833333  acc val 69.444444 acc labeled target 97.962963
saving model...
S real T clipart Train Ep: 14100 lr0.005169960417839403 	 Loss Classification: 0.185814 Loss T 0.060035 Method MME

S real T clipart Train Ep: 14200 lr0.005153929522817161 	 Loss Classification: 0.232107 Loss T 0.064848 Method MME

S real T clipart Train Ep: 14300 lr0.005138014136157799 	 Loss Classification: 0.267944 Loss T 0.054105 Method MME

S real T clipart Train Ep: 14400 lr0.005122212955356274 	 Loss Classification: 0.215236 Loss T 0.059745 Method MME

S real T clipart Train Ep: 14500 lr0.005106524697854057 	 Loss Classification: 0.337572 Loss T 0.052602 Method MME


Labeled Target set: Average loss: 0.0493, Accuracy: 1064/1080 F1 (98.5185%)


Test set: Average loss: 1.3669, Accuracy: 13376/18312 F1 (73.0450%)


Val set: Average loss: 1.3471, Accuracy: 257/360 F1 (71.3889%)

best acc test 70.833333  acc val 71.388889 acc labeled target 98.518519
saving model...
S real T clipart Train Ep: 14600 lr0.005090948100653781 	 Loss Classification: 0.109023 Loss T 0.076912 Method MME

S real T clipart Train Ep: 14700 lr0.0050754819199428855 	 Loss Classification: 0.035333 Loss T 0.056152 Method MME

S real T clipart Train Ep: 14800 lr0.005060124930725974 	 Loss Classification: 0.096258 Loss T 0.039116 Method MME

S real T clipart Train Ep: 14900 lr0.00504487592646567 	 Loss Classification: 0.346240 Loss T 0.049714 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        0.8888889 1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.5       1.
 1.        1.        1.        1.        1.        1.        1.
 0.8888889 1.        1.        1.        1.        1.        1.
 0.8888889 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.7777778 0.8888889 1.        0.8888889 0.8888889 0.8888889 1.
 1.        1.        1.        1.        1.        0.6666667 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        0.8888889 1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [47, 103, 91, 94, 56, 92, 30, 95, 96, 63, 116, 114, 86, 85, 84, 83, 82, 81, 80, 79, 122, 78, 77, 76, 75, 74, 73, 72, 71, 123, 70, 69, 68, 67, 66, 87, 88, 89, 90, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 102, 101, 100, 99, 98, 97, 117, 118, 119, 65, 93, 120, 121, 115, 0, 62, 124, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 28, 64, 29, 32, 61, 60, 59, 58, 57, 55, 54, 53, 52, 51, 50, 49, 48, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 31, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 72  98  89  91  83 257  70 117 107  67 180 259 104 172 199 101 110 133
 133 149  67 147 102 147  88 108  82 136 103 934 102 253  89 203 272 177
 161 150 110 160 162  89 140 108 144 156 252 110 236 218 142 144 258 190
 106 152 224 138 196 103 118 120 139 145 178  95 153  59  83 108  91 164
 109 149 112  81 153 142 105 132 223 101 105  91 153 137  98 105 114 104
  95 113 140  95  98 168 139 158 211 138 118 172 167 175 109 107 243 236
  66 203 253 202 190 182 141 119  94 189  73  93 129 144 148 177 239 262]
CBFL per class weights: tensor([1.3834, 1.1372, 1.2052, 1.1888, 1.2593, 0.7707, 1.4104, 1.0304, 1.0814,
        1.4540, 0.8520, 0.7694, 1.0988, 0.8662, 0.8240, 1.1174, 1.0650, 0.9663,
        0.9663, 0.9178, 1.4540, 0.9232, 1.1111, 0.9232, 1.2136, 1.0758, 1.2691,
        0.9562, 1.1049, 0.7125, 1.1111, 0.7733, 1.2052, 0.8189, 0.7620, 0.8572,
        0.8887, 0.9151, 1.0650, 0.8909, 0.8865, 1.2052, 0.9435, 1.0758, 0.9316,
        0.9001, 0.7740, 1.0650, 0.7858, 0.8022, 0.9375, 0.9316, 0.7701, 0.8364,
        1.0871, 0.9100, 0.7963, 0.9498, 0.8279, 1.1049, 1.0258, 1.0169, 0.9466,
        0.9287, 0.8554, 1.1583, 0.9075, 1.5928, 1.2593, 1.0758, 1.1888, 0.8822,
        1.0704, 0.9178, 1.0546, 1.2792, 0.9075, 0.9375, 1.0929, 0.9698, 0.7972,
        1.1174, 1.0929, 1.1888, 0.9075, 0.9530, 1.1372, 1.0929, 1.0447, 1.0988,
        1.1583, 1.0496, 0.9435, 1.1583, 1.1372, 0.8740, 0.9466, 0.8954, 0.8096,
        0.9498, 1.0258, 0.8662, 0.8760, 0.8607, 1.0704, 1.0814, 0.7803, 0.7858,
        1.4694, 0.8189, 0.7733, 0.8202, 0.8364, 0.8487, 0.9405, 1.0213, 1.1657,
        0.8378, 1.3705, 1.1732, 0.9807, 0.9316, 0.9204, 0.8572, 0.7834, 0.7676],
       device='cuda:0')
S real T clipart Train Ep: 15000 lr0.005029733718731741 	 Loss Classification: 0.307928 Loss T 0.071781 Method MME


Labeled Target set: Average loss: 0.0774, Accuracy: 1063/1080 F1 (98.4259%)


Test set: Average loss: 1.4420, Accuracy: 13079/18312 F1 (71.4231%)


Val set: Average loss: 1.5418, Accuracy: 254/360 F1 (70.5556%)

best acc test 70.833333  acc val 70.555556 acc labeled target 98.425926
saving model...
S real T clipart Train Ep: 15100 lr0.005014697136858264 	 Loss Classification: 0.121416 Loss T 0.057499 Method MME

S real T clipart Train Ep: 15200 lr0.004999765027608607 	 Loss Classification: 0.610819 Loss T 0.062145 Method MME

S real T clipart Train Ep: 15300 lr0.004984936254848047 	 Loss Classification: 0.185384 Loss T 0.063445 Method MME

S real T clipart Train Ep: 15400 lr0.004970209699223787 	 Loss Classification: 0.236637 Loss T 0.042694 Method MME

S real T clipart Train Ep: 15500 lr0.0049555842578521995 	 Loss Classification: 0.104966 Loss T 0.049526 Method MME


Labeled Target set: Average loss: 0.0657, Accuracy: 1064/1080 F1 (98.5185%)


Test set: Average loss: 1.4118, Accuracy: 13371/18312 F1 (73.0177%)


Val set: Average loss: 1.4542, Accuracy: 261/360 F1 (72.5000%)

best acc test 73.017693  acc val 72.500000 acc labeled target 98.518519
saving model...
S real T clipart Train Ep: 15600 lr0.004941058844013093 	 Loss Classification: 0.269272 Loss T 0.055092 Method MME

S real T clipart Train Ep: 15700 lr0.004926632386850831 	 Loss Classification: 0.155084 Loss T 0.062598 Method MME

S real T clipart Train Ep: 15800 lr0.004912303831082109 	 Loss Classification: 0.416789 Loss T 0.049499 Method MME

S real T clipart Train Ep: 15900 lr0.004898072136710217 	 Loss Classification: 0.195446 Loss T 0.056333 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        0.8888889 1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.8888889 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        0.7777778 1.        1.        1.
 0.8888889 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        0.8888889 1.        1.
 1.        1.        1.        1.        1.        0.6666667 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        0.8888889 1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [63, 91, 103, 66, 70, 19, 95, 4, 120, 118, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 122, 69, 68, 67, 123, 65, 89, 90, 121, 92, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 119, 106, 104, 102, 101, 100, 99, 98, 97, 96, 64, 94, 93, 105, 0, 62, 61, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 3, 2, 1, 30, 124, 31, 33, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 32, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 71  99  91  92  77 154  64 121 110  78 187 268  97 176 200 102 109 137
 131 149  68 148 101 149  92 102  81 132  96 775  96 174  90 206 277 179
 151 152 114 159 165  84 139 111 145 152 257 115 241 223 144 149 258 198
 102 151 226 139 202 110 121 131 144 147 179  93 153  64  84 108  96 164
 111 146 114  83 152 141 103 132 200 109 103  93 156 136  99 110 113 108
 105 112 142  94 285 173 139 155 216 135 120 187 167 180 107 110 241 238
  69 205 261 198 189 178 145 121  96 197  75  97 127 145 144 178 248 265]
CBFL per class weights: tensor([1.4046, 1.1368, 1.1955, 1.1876, 1.3298, 0.9101, 1.5103, 1.0183, 1.0710,
        1.3186, 0.8456, 0.7685, 1.1505, 0.8638, 0.8273, 1.1173, 1.0764, 0.9583,
        0.9789, 0.9229, 1.4471, 0.9256, 1.1237, 0.9229, 1.1876, 1.1173, 1.2865,
        0.9753, 1.1576, 0.7168, 1.1576, 0.8674, 1.2036, 0.8199, 0.7637, 0.8585,
        0.9177, 0.9151, 1.0506, 0.8982, 0.8851, 1.2568, 0.9519, 1.0658, 0.9340,
        0.9151, 0.7750, 1.0457, 0.7863, 0.8017, 0.9369, 0.9229, 0.7744, 0.8299,
        1.1173, 0.9177, 0.7989, 0.9519, 0.8248, 1.0710, 1.0183, 0.9789, 0.9369,
        0.9284, 0.8585, 1.1798, 0.9126, 1.5103, 1.2568, 1.0819, 1.1576, 0.8872,
        1.0658, 0.9312, 1.0506, 1.2664, 0.9151, 0.9458, 1.1111, 0.9753, 0.8273,
        1.0764, 1.1111, 1.1798, 0.9052, 0.9616, 1.1368, 1.0710, 1.0555, 1.0819,
        1.0991, 1.0606, 0.9427, 1.1722, 0.7598, 0.8693, 0.9519, 0.9076, 0.8088,
        0.9650, 1.0227, 0.8456, 0.8809, 0.8569, 1.0875, 1.0710, 0.7863, 0.7886,
        1.4325, 0.8211, 0.7726, 0.8299, 0.8426, 0.8603, 0.9340, 1.0183, 1.1576,
        0.8313, 1.3534, 1.1505, 0.9938, 0.9340, 0.9369, 0.8603, 0.7811, 0.7702],
       device='cuda:0')
S real T clipart Train Ep: 16000 lr0.004883936278745637 	 Loss Classification: 0.372559 Loss T 0.033966 Method MME


Labeled Target set: Average loss: 0.1019, Accuracy: 1054/1080 F1 (97.5926%)


Test set: Average loss: 1.4990, Accuracy: 13064/18312 F1 (71.3412%)


Val set: Average loss: 1.5291, Accuracy: 261/360 F1 (72.5000%)

best acc test 71.341197  acc val 72.500000 acc labeled target 97.592593
saving model...
S real T clipart Train Ep: 16100 lr0.004869895246932789 	 Loss Classification: 0.124462 Loss T 0.052742 Method MME

S real T clipart Train Ep: 16200 lr0.004855948045482784 	 Loss Classification: 0.166936 Loss T 0.050787 Method MME

S real T clipart Train Ep: 16300 lr0.004842093692812012 	 Loss Classification: 0.161426 Loss T 0.044853 Method MME

S real T clipart Train Ep: 16400 lr0.004828331221286437 	 Loss Classification: 0.139729 Loss T 0.063920 Method MME

S real T clipart Train Ep: 16500 lr0.004814659676971443 	 Loss Classification: 0.516189 Loss T 0.052866 Method MME


Labeled Target set: Average loss: 0.0478, Accuracy: 1066/1080 F1 (98.7037%)


Test set: Average loss: 1.4415, Accuracy: 13280/18312 F1 (72.5208%)


Val set: Average loss: 1.3816, Accuracy: 262/360 F1 (72.7778%)

best acc test 72.520751  acc val 72.777778 acc labeled target 98.703704
saving model...
S real T clipart Train Ep: 16600 lr0.004801078119387078 	 Loss Classification: 0.398538 Loss T 0.067264 Method MME

S real T clipart Train Ep: 16700 lr0.004787585621268585 	 Loss Classification: 0.112784 Loss T 0.054559 Method MME

S real T clipart Train Ep: 16800 lr0.0047741812683320655 	 Loss Classification: 0.679475 Loss T 0.046046 Method MME

S real T clipart Train Ep: 16900 lr0.004760864159045157 	 Loss Classification: 0.170347 Loss T 0.037022 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        0.6666667 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        0.8888889 1.        1.
 0.8888889 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8333333 1.        1.        1.
 1.        1.        1.        1.        1.        0.6666667 1.
 1.        0.8888889 1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [63, 47, 103, 94, 106, 70, 45, 67, 86, 88, 84, 83, 82, 81, 80, 89, 87, 85, 79, 78, 77, 76, 75, 74, 73, 72, 71, 69, 68, 66, 65, 90, 91, 0, 64, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 105, 104, 102, 101, 100, 99, 98, 97, 96, 95, 92, 93, 62, 61, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 28, 124, 29, 31, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 46, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 30, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 74  99  99  94  76 137  67 128 108  80 191 257 102 170 200 102 113 136
 135 154  71 148 108 144  93  99  84 138  89 641 101 146  95 207 276 179
 157 153 116 162 170  89 147 112 146 156 258 116 242 224 144 148 264 196
 105 150 224 136 206 108 124 134 143 151 184  94 151  65  90 108  96 160
 113 150 114  73 156 143 105 132 180 112 104  92 160 136 103 104 112 113
 113 114 145  97 302 176 139 154 216 133 122 191 171 181 108 115 247 243
  70 207 264 203 190 175 143 121 100 201  76 103 126 144 146 180 250 265]
CBFL per class weights: tensor([1.3733, 1.1432, 1.1432, 1.1788, 1.3490, 0.9637, 1.4704, 0.9955, 1.0880,
        1.3041, 0.8443, 0.7794, 1.1236, 0.8799, 0.8320, 1.1236, 1.0615, 0.9670,
        0.9704, 0.9152, 1.4125, 0.9308, 1.0880, 0.9421, 1.1864, 1.1432, 1.2638,
        0.9605, 1.2188, 0.7217, 1.1300, 0.9364, 1.1714, 0.8233, 0.7685, 0.8634,
        0.9079, 0.9177, 1.0467, 0.8965, 0.8799, 1.2188, 0.9336, 1.0665, 0.9364,
        0.9103, 0.7788, 1.0467, 0.7899, 0.8053, 0.9421, 0.9308, 0.7751, 0.8373,
        1.1052, 0.9255, 0.8053, 0.9670, 0.8245, 1.0880, 1.0114, 0.9738, 0.9450,
        0.9228, 0.8551, 1.1788, 0.9228, 1.5021, 1.2104, 1.0880, 1.1641, 0.9010,
        1.0615, 0.9255, 1.0565, 1.3860, 0.9103, 0.9450, 1.1052, 0.9808, 0.8617,
        1.0665, 1.1112, 1.1942, 0.9010, 0.9670, 1.1174, 1.1112, 1.0665, 1.0615,
        1.0615, 1.0565, 0.9392, 1.1570, 0.7569, 0.8686, 0.9573, 0.9152, 0.8133,
        0.9772, 1.0197, 0.8443, 0.8779, 0.8600, 1.0880, 1.0515, 0.7862, 0.7891,
        1.4263, 0.8233, 0.7751, 0.8282, 0.8458, 0.8704, 0.9450, 1.0240, 1.1365,
        0.8307, 1.3490, 1.1174, 1.0033, 0.9421, 0.9364, 0.8617, 0.7841, 0.7745],
       device='cuda:0')
S real T clipart Train Ep: 17000 lr0.0047476334044026 	 Loss Classification: 0.325962 Loss T 0.050115 Method MME


Labeled Target set: Average loss: 0.1029, Accuracy: 1051/1080 F1 (97.3148%)


Test set: Average loss: 1.4583, Accuracy: 13193/18312 F1 (72.0457%)


Val set: Average loss: 1.3855, Accuracy: 266/360 F1 (73.8889%)

best acc test 72.045653  acc val 73.888889 acc labeled target 97.314815
saving model...
S real T clipart Train Ep: 17100 lr0.004734488127706559 	 Loss Classification: 0.147785 Loss T 0.066072 Method MME

S real T clipart Train Ep: 17200 lr0.004721427464351597 	 Loss Classification: 0.071742 Loss T 0.054182 Method MME

S real T clipart Train Ep: 17300 lr0.004708450561614184 	 Loss Classification: 0.120212 Loss T 0.048174 Method MME

S real T clipart Train Ep: 17400 lr0.004695556578446619 	 Loss Classification: 0.225448 Loss T 0.053216 Method MME

S real T clipart Train Ep: 17500 lr0.004682744685275263 	 Loss Classification: 0.399499 Loss T 0.051711 Method MME


Labeled Target set: Average loss: 0.0489, Accuracy: 1064/1080 F1 (98.5185%)


Test set: Average loss: 1.3784, Accuracy: 13488/18312 F1 (73.6566%)


Val set: Average loss: 1.3215, Accuracy: 262/360 F1 (72.7778%)

best acc test 72.045653  acc val 72.777778 acc labeled target 98.518519
saving model...
S real T clipart Train Ep: 17600 lr0.004670014063802979 	 Loss Classification: 0.311470 Loss T 0.054295 Method MME

S real T clipart Train Ep: 17700 lr0.004657363906815676 	 Loss Classification: 0.047423 Loss T 0.053601 Method MME

S real T clipart Train Ep: 17800 lr0.004644793417992855 	 Loss Classification: 0.102772 Loss T 0.034289 Method MME

S real T clipart Train Ep: 17900 lr0.004632301811722062 	 Loss Classification: 0.067969 Loss T 0.041297 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.8888889 1.
 1.        1.        1.        1.        1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.
 1.        0.8888889 0.8888889 1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.6666667 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        0.8333333
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        0.8888889 0.6666667 1.
 1.        1.        1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [47, 63, 103, 69, 20, 12, 102, 108, 29, 30, 87, 82, 85, 89, 84, 83, 90, 88, 86, 0, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 68, 67, 91, 92, 96, 94, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 107, 106, 105, 104, 101, 100, 99, 98, 97, 95, 93, 66, 62, 64, 28, 27, 26, 25, 24, 23, 22, 21, 19, 18, 17, 16, 31, 15, 13, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 14, 32, 33, 34, 124, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 65, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 73 100 104  97  78 140  66 129 110  79 191 263 104 166 205 102 115 136
 132 153  69 149 106 146  93 108  89 133  90 516 100 122  95 206 276 180
 168 151 118 167 175  92 142 112 148 161 260 117 242 221 141 147 263 203
 104 155 213 137 209 106 130 135 142 148 186  95 151  59  89 103  97 168
 117 148 112  69 161 144 107 136 152 112 103 105 158 139 102 106 111 123
 111 118 144  98 289 179 141 156 217 129 121 212 168 177 113 123 245 243
  72 212 273 205 190 185 145 119 107 200  79 107 128 147 151 178 270 270]
CBFL per class weights: tensor([1.3900, 1.1398, 1.1144, 1.1603, 1.3298, 0.9569, 1.4903, 0.9946, 1.0801,
        1.3187, 0.8468, 0.7779, 1.1144, 0.8905, 0.8281, 1.1268, 1.0546, 0.9698,
        0.9836, 0.9203, 1.4447, 0.9308, 1.1025, 0.9391, 1.1898, 1.0911, 1.2223,
        0.9801, 1.2139, 0.7266, 1.1398, 1.0227, 1.1747, 0.8269, 0.7707, 0.8641,
        0.8864, 0.9255, 1.0404, 0.8884, 0.8729, 1.1977, 0.9508, 1.0696, 0.9335,
        0.9013, 0.7797, 1.0450, 0.7922, 0.8105, 0.9538, 0.9363, 0.7779, 0.8306,
        1.1144, 0.9154, 0.8189, 0.9665, 0.8234, 1.1025, 0.9909, 0.9732, 0.9508,
        0.9335, 0.8543, 1.1747, 0.9255, 1.6154, 1.2223, 1.1206, 1.1603, 0.8864,
        1.0450, 0.9335, 1.0696, 1.4447, 0.9013, 0.9448, 1.0968, 0.9698, 0.9229,
        1.0696, 1.1206, 1.1084, 0.9082, 0.9600, 1.1268, 1.1025, 1.0748, 1.0184,
        1.0748, 1.0404, 0.9448, 1.1533, 0.7645, 0.8658, 0.9538, 0.9129, 0.8146,
        0.9946, 1.0270, 0.8200, 0.8864, 0.8693, 1.0645, 1.0184, 0.7899, 0.7914,
        1.4030, 0.8200, 0.7723, 0.8281, 0.8482, 0.8559, 0.9419, 1.0358, 1.0968,
        0.8344, 1.3187, 1.0968, 0.9984, 0.9363, 0.9255, 0.8676, 0.7739, 0.7739],
       device='cuda:0')
S real T clipart Train Ep: 18000 lr0.004619888312917149 	 Loss Classification: 0.192345 Loss T 0.038600 Method MME


Labeled Target set: Average loss: 0.0811, Accuracy: 1059/1080 F1 (98.0556%)


Test set: Average loss: 1.4637, Accuracy: 13305/18312 F1 (72.6573%)


Val set: Average loss: 1.5208, Accuracy: 263/360 F1 (73.0556%)

best acc test 72.045653  acc val 73.055556 acc labeled target 98.055556
saving model...
S real T clipart Train Ep: 18100 lr0.00460755215684026 	 Loss Classification: 0.086456 Loss T 0.054358 Method MME

S real T clipart Train Ep: 18200 lr0.00459529258892745 	 Loss Classification: 0.130241 Loss T 0.058043 Method MME

S real T clipart Train Ep: 18300 lr0.004583108864617844 	 Loss Classification: 0.172920 Loss T 0.054061 Method MME

S real T clipart Train Ep: 18400 lr0.0045710002491862545 	 Loss Classification: 0.121973 Loss T 0.036954 Method MME

S real T clipart Train Ep: 18500 lr0.0045589660175791875 	 Loss Classification: 0.209477 Loss T 0.048433 Method MME


Labeled Target set: Average loss: 0.0552, Accuracy: 1064/1080 F1 (98.5185%)


Test set: Average loss: 1.4358, Accuracy: 13492/18312 F1 (73.6785%)


Val set: Average loss: 1.3444, Accuracy: 266/360 F1 (73.8889%)

best acc test 73.678462  acc val 73.888889 acc labeled target 98.518519
saving model...
S real T clipart Train Ep: 18600 lr0.004547005454254138 	 Loss Classification: 0.176278 Loss T 0.052571 Method MME

S real T clipart Train Ep: 18700 lr0.004535117853022106 	 Loss Classification: 0.129279 Loss T 0.050587 Method MME

S real T clipart Train Ep: 18800 lr0.004523302516893268 	 Loss Classification: 0.131367 Loss T 0.031867 Method MME

S real T clipart Train Ep: 18900 lr0.004511558757925708 	 Loss Classification: 0.242172 Loss T 0.046685 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        0.8888889 0.8888889 1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.6666667 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.8888889 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        0.8333333 0.6666667 1.
 1.        0.8333333 1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [63, 103, 47, 106, 102, 9, 13, 10, 108, 89, 84, 85, 86, 87, 88, 82, 81, 83, 0, 79, 90, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 80, 91, 95, 93, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 107, 105, 104, 101, 100, 99, 98, 97, 96, 94, 92, 65, 62, 124, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 30, 17, 15, 14, 12, 11, 8, 7, 6, 5, 4, 3, 2, 1, 16, 31, 32, 33, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 64, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 81  99 104  97  76 139  63 131 111  87 193 268 103 164 201 100 114 135
 135 152  68 150 106 141  95 111  87 144  88 430 111 100  97 202 277 178
 164 152 124 167 176  93 142 112 150 161 274 120 243 223 143 148 263 202
  96 159 203 137 207 114 127 143 141 151 182  96 150  70  89 111  97 164
 112 144 115  69 165 149 104 139 131 112 111 113 159 140 113 106 110 121
 111 119 147  97 265 180 137 164 214 129 119 239 170 172 113 121 251 242
  75 217 275 209 195 181 146 119 106 205  85 104 131 149 153 178 275 265]
CBFL per class weights: tensor([1.3028, 1.1512, 1.1190, 1.1651, 1.3585, 0.9640, 1.5468, 0.9913, 1.0793,
        1.2448, 0.8474, 0.7782, 1.1252, 0.8984, 0.8365, 1.1445, 1.0639, 0.9772,
        0.9772, 0.9267, 1.4655, 0.9320, 1.1071, 0.9578, 1.1796, 1.0793, 1.2448,
        0.9487, 1.2360, 0.7353, 1.0793, 1.1445, 1.1651, 0.8353, 0.7734, 0.8712,
        0.8984, 0.9267, 1.0185, 0.8921, 0.8747, 1.1948, 0.9547, 1.0741, 0.9320,
        0.9050, 0.7749, 1.0356, 0.7947, 0.8119, 0.9517, 0.9374, 0.7811, 0.8353,
        1.1723, 0.9096, 0.8340, 0.9705, 0.8291, 1.0639, 1.0064, 0.9517, 0.9578,
        0.9293, 0.8643, 1.1723, 0.9320, 1.4363, 1.2273, 1.0793, 1.1651, 0.8984,
        1.0741, 0.9487, 1.0589, 1.4507, 0.8963, 0.9347, 1.1190, 0.9640, 0.9913,
        1.0741, 1.0793, 1.0689, 0.9096, 0.9609, 1.0689, 1.1071, 1.0846, 1.0312,
        1.0793, 1.0401, 0.9402, 1.1651, 0.7800, 0.8677, 0.9705, 0.8984, 0.8212,
        0.9987, 1.0401, 0.7978, 0.8861, 0.8822, 1.0689, 1.0312, 0.7889, 0.7955,
        1.3705, 0.8180, 0.7744, 0.8268, 0.8446, 0.8660, 0.9430, 1.0401, 1.1071,
        0.8315, 1.2632, 1.1190, 0.9913, 0.9347, 0.9242, 0.8712, 0.7744, 0.7800],
       device='cuda:0')
S real T clipart Train Ep: 19000 lr0.004499885897077159 	 Loss Classification: 0.046928 Loss T 0.042682 Method MME


Labeled Target set: Average loss: 0.0838, Accuracy: 1060/1080 F1 (98.1481%)


Test set: Average loss: 1.4654, Accuracy: 13354/18312 F1 (72.9249%)


Val set: Average loss: 1.3692, Accuracy: 261/360 F1 (72.5000%)

best acc test 73.678462  acc val 72.500000 acc labeled target 98.148148
saving model...
S real T clipart Train Ep: 19100 lr0.004488283264059669 	 Loss Classification: 1.171048 Loss T 0.045608 Method MME

S real T clipart Train Ep: 19200 lr0.004476750197197131 	 Loss Classification: 0.291507 Loss T 0.025400 Method MME

S real T clipart Train Ep: 19300 lr0.004465286043285614 	 Loss Classification: 0.399410 Loss T 0.033167 Method MME

S real T clipart Train Ep: 19400 lr0.004453890157456425 	 Loss Classification: 0.222881 Loss T 0.044463 Method MME

S real T clipart Train Ep: 19500 lr0.004442561903041838 	 Loss Classification: 0.295196 Loss T 0.031344 Method MME


Labeled Target set: Average loss: 0.0497, Accuracy: 1065/1080 F1 (98.6111%)


Test set: Average loss: 1.4489, Accuracy: 13479/18312 F1 (73.6075%)


Val set: Average loss: 1.3918, Accuracy: 262/360 F1 (72.7778%)

best acc test 73.678462  acc val 72.777778 acc labeled target 98.611111
saving model...
S real T clipart Train Ep: 19600 lr0.004431300651443432 	 Loss Classification: 0.183886 Loss T 0.045995 Method MME

S real T clipart Train Ep: 19700 lr0.004420105782002992 	 Loss Classification: 0.316808 Loss T 0.045139 Method MME

S real T clipart Train Ep: 19800 lr0.004408976681875879 	 Loss Classification: 0.176964 Loss T 0.057412 Method MME

S real T clipart Train Ep: 19900 lr0.004397912745906863 	 Loss Classification: 0.220697 Loss T 0.040602 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        1.        1.
 0.8888889 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.8333333 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.6666667 1.
 1.        1.        1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        0.8888889 1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [91, 63, 103, 26, 14, 45, 10, 116, 108, 80, 86, 83, 87, 88, 89, 82, 81, 85, 84, 0, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 90, 79, 95, 93, 123, 122, 121, 120, 119, 118, 117, 115, 114, 113, 112, 111, 92, 110, 107, 106, 105, 104, 102, 101, 100, 99, 98, 97, 96, 94, 109, 65, 62, 124, 29, 28, 27, 25, 24, 23, 22, 21, 20, 19, 18, 17, 30, 16, 13, 12, 11, 9, 8, 7, 6, 5, 4, 3, 2, 1, 15, 31, 32, 33, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 64, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 81 100 104  99  81 137  62 130 113  82 189 270 103 162 197 101 114 138
 135 153  71 152 105 143  96 110  88 144  95 361 115  81  96 204 281 178
 163 156 123 165 177  95 144 120 151 166 275 128 244 227 147 158 266 202
  93 160 201 137 208 115 126 145 139 156 183  96 150  74  91 116  97 166
 113 140 113  71 165 148 106 139 122 115 107 118 157 141 109 107 110 124
 105 117 144  93 247 180 138 165 215 127 119 252 170 178 113 125 253 245
  74 217 274 210 196 193 148 120 104 200  85 102 132 149 155 177 276 269]
CBFL per class weights: tensor([1.3045, 1.1460, 1.1205, 1.1527, 1.3045, 0.9718, 1.5667, 0.9963, 1.0703,
        1.2942, 0.8544, 0.7781, 1.1267, 0.9040, 0.8429, 1.1394, 1.0653, 0.9685,
        0.9785, 0.9254, 1.4243, 0.9279, 1.1145, 0.9530, 1.1738, 1.0861, 1.2376,
        0.9500, 1.1812, 0.7464, 1.0604, 1.3045, 1.1738, 0.8339, 0.7724, 0.8723,
        0.9018, 0.9179, 1.0240, 0.8975, 0.8741, 1.1812, 0.9500, 1.0370, 0.9306,
        0.8954, 0.7754, 1.0039, 0.7950, 0.8092, 0.9414, 0.9131, 0.7804, 0.8364,
        1.1964, 0.9085, 0.8376, 0.9718, 0.8290, 1.0604, 1.0117, 0.9471, 0.9653,
        0.9179, 0.8638, 1.1738, 0.9332, 1.3848, 1.2123, 1.0555, 1.1666, 0.8954,
        1.0703, 0.9621, 1.0703, 1.4243, 0.8975, 0.9386, 1.1086, 0.9653, 1.0283,
        1.0604, 1.1028, 1.0461, 0.9155, 0.9590, 1.0915, 1.1028, 1.0861, 1.0198,
        1.1145, 1.0507, 0.9500, 1.1964, 0.7928, 0.8689, 0.9685, 0.8975, 0.8212,
        1.0077, 1.0415, 0.7892, 0.8872, 0.8723, 1.0703, 1.0157, 0.7886, 0.7942,
        1.3848, 0.8190, 0.7760, 0.8267, 0.8443, 0.8485, 0.9386, 1.0370, 1.1205,
        0.8389, 1.2648, 1.1330, 0.9890, 0.9359, 0.9204, 0.8741, 0.7749, 0.7787],
       device='cuda:0')
S real T clipart Train Ep: 20000 lr0.004386913376508308 	 Loss Classification: 0.098602 Loss T 0.044788 Method MME


Labeled Target set: Average loss: 0.0953, Accuracy: 1055/1080 F1 (97.6852%)


Test set: Average loss: 1.5158, Accuracy: 13264/18312 F1 (72.4334%)


Val set: Average loss: 1.5745, Accuracy: 265/360 F1 (73.6111%)

best acc test 73.678462  acc val 73.611111 acc labeled target 97.685185
saving model...
S real T clipart Train Ep: 20100 lr0.004375977983540715 	 Loss Classification: 0.096248 Loss T 0.040390 Method MME

S real T clipart Train Ep: 20200 lr0.004365105984195512 	 Loss Classification: 0.231787 Loss T 0.043865 Method MME

S real T clipart Train Ep: 20300 lr0.004354296802880095 	 Loss Classification: 0.402077 Loss T 0.035694 Method MME

S real T clipart Train Ep: 20400 lr0.004343549871105023 	 Loss Classification: 0.563502 Loss T 0.038179 Method MME

S real T clipart Train Ep: 20500 lr0.0043328646273733526 	 Loss Classification: 0.344614 Loss T 0.033730 Method MME


Labeled Target set: Average loss: 0.0638, Accuracy: 1060/1080 F1 (98.1481%)


Test set: Average loss: 1.4916, Accuracy: 13492/18312 F1 (73.6785%)


Val set: Average loss: 1.5430, Accuracy: 264/360 F1 (73.3333%)

best acc test 73.678462  acc val 73.333333 acc labeled target 98.148148
saving model...
S real T clipart Train Ep: 20600 lr0.00432224051707205 	 Loss Classification: 0.244928 Loss T 0.033099 Method MME

S real T clipart Train Ep: 20700 lr0.0043116769923654385 	 Loss Classification: 0.027672 Loss T 0.039802 Method MME

S real T clipart Train Ep: 20800 lr0.004301173512090631 	 Loss Classification: 0.071392 Loss T 0.034565 Method MME

S real T clipart Train Ep: 20900 lr0.004290729541654919 	 Loss Classification: 0.103714 Loss T 0.034060 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 0.8888889 0.8888889 1.        1.        1.        1.        0.8888889
 1.        0.8333333 1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.5       1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.8888889 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 0.8888889 1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.6666667 1.
 1.        0.7777778 1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        0.8888889]
Top k classes which perform poorly are:  [47, 103, 91, 106, 15, 125, 108, 13, 92, 7, 118, 70, 8, 72, 90, 89, 88, 87, 86, 85, 84, 83, 67, 82, 69, 81, 80, 79, 71, 77, 76, 75, 74, 73, 68, 78, 96, 94, 123, 122, 121, 120, 119, 117, 116, 115, 114, 113, 112, 111, 110, 109, 107, 105, 104, 102, 101, 100, 99, 98, 97, 66, 95, 93, 65, 0, 63, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 31, 18, 16, 14, 12, 11, 10, 9, 6, 5, 4, 3, 2, 1, 17, 32, 33, 34, 124, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 64, 62]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 80 100 105 100  82 138  64 125 114  86 190 264 108 158 196  99 114 139
 143 159  69 153 112 147  96 114  85 141  94 316 115  72  96 202 283 178
 163 153 128 165 173  92 149 122 152 163 272 127 243 227 145 156 268 202
  89 165 198 137 210 117 130 143 142 157 185  96 150  76  94 126  99 165
 114 135 115  74 165 152 104 141 111 117 107 123 160 141 109 111 112 126
 105 119 142  97 244 180 136 164 220 126 121 241 167 178 111 124 252 246
  77 219 277 219 195 186 148 121 106 195  89 105 131 146 153 179 279 272]
CBFL per class weights: tensor([1.3179, 1.1485, 1.1169, 1.1485, 1.2970, 0.9706, 1.5348, 1.0179, 1.0676,
        1.2582, 0.8547, 0.7833, 1.0994, 0.9151, 0.8461, 1.1552, 1.0676, 0.9674,
        0.9550, 0.9128, 1.4557, 0.9274, 1.0778, 0.9434, 1.1763, 1.0676, 1.2676,
        0.9611, 1.1912, 0.7598, 1.0626, 1.4138, 1.1763, 0.8382, 0.7731, 0.8742,
        0.9037, 0.9274, 1.0060, 0.8994, 0.8834, 1.2068, 0.9379, 1.0305, 0.9299,
        0.9037, 0.7787, 1.0099, 0.7975, 0.8109, 0.9491, 0.9199, 0.7809, 0.8382,
        1.2316, 0.8994, 0.8434, 0.9739, 0.8285, 1.0530, 0.9984, 0.9550, 0.9580,
        0.9175, 0.8625, 1.1763, 0.9352, 1.3632, 1.1912, 1.0139, 1.1552, 0.8994,
        1.0676, 0.9806, 1.0626, 1.3878, 0.8994, 0.9299, 1.1229, 0.9611, 1.0830,
        1.0530, 1.1051, 1.0262, 0.9104, 0.9611, 1.0939, 1.0830, 1.0778, 1.0139,
        1.1169, 1.0437, 0.9580, 1.1692, 0.7967, 0.8707, 0.9772, 0.9015, 0.8177,
        1.0139, 1.0348, 0.7990, 0.8952, 0.8742, 1.0830, 1.0220, 0.7909, 0.7952,
        1.3514, 0.8187, 0.7761, 0.8187, 0.8475, 0.8609, 0.9406, 1.0348, 1.1110,
        0.8475, 1.2316, 1.1169, 0.9947, 0.9462, 0.9274, 0.8725, 0.7750, 0.7787],
       device='cuda:0')
S real T clipart Train Ep: 21000 lr0.0042803445529350555 	 Loss Classification: 0.076429 Loss T 0.028751 Method MME


Labeled Target set: Average loss: 0.0768, Accuracy: 1063/1080 F1 (98.4259%)


Test set: Average loss: 1.5548, Accuracy: 13239/18312 F1 (72.2969%)


Val set: Average loss: 1.5971, Accuracy: 260/360 F1 (72.2222%)

best acc test 73.678462  acc val 72.222222 acc labeled target 98.425926
saving model...
S real T clipart Train Ep: 21100 lr0.0042700180241784045 	 Loss Classification: 0.088550 Loss T 0.031909 Method MME

S real T clipart Train Ep: 21200 lr0.004259749439905917 	 Loss Classification: 0.087836 Loss T 0.039375 Method MME

S real T clipart Train Ep: 21300 lr0.004249538290816886 	 Loss Classification: 0.051745 Loss T 0.031475 Method MME

S real T clipart Train Ep: 21400 lr0.004239384073695442 	 Loss Classification: 0.212791 Loss T 0.040120 Method MME

S real T clipart Train Ep: 21500 lr0.004229286291318768 	 Loss Classification: 0.527672 Loss T 0.027299 Method MME


Labeled Target set: Average loss: 0.0483, Accuracy: 1063/1080 F1 (98.4259%)


Test set: Average loss: 1.5413, Accuracy: 13511/18312 F1 (73.7822%)


Val set: Average loss: 1.4822, Accuracy: 268/360 F1 (74.4444%)

best acc test 73.782219  acc val 74.444444 acc labeled target 98.425926
saving model...
S real T clipart Train Ep: 21600 lr0.004219244452366975 	 Loss Classification: 0.441335 Loss T 0.035193 Method MME

S real T clipart Train Ep: 21700 lr0.004209258071334615 	 Loss Classification: 0.348496 Loss T 0.039727 Method MME

S real T clipart Train Ep: 21800 lr0.004199326668443797 	 Loss Classification: 0.075003 Loss T 0.027911 Method MME

S real T clipart Train Ep: 21900 lr0.004189449769558871 	 Loss Classification: 0.211302 Loss T 0.035211 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        0.8888889 1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        0.8888889 1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        0.8888889 1.        1.        1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.5       1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        1.        0.8888889
 1.        1.        1.        1.        1.        0.8888889 1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.6666667 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [63, 91, 103, 1, 89, 29, 83, 118, 48, 73, 43, 82, 85, 86, 87, 88, 90, 84, 81, 0, 78, 77, 76, 75, 74, 72, 71, 70, 69, 68, 67, 66, 80, 79, 94, 93, 123, 122, 121, 120, 119, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 102, 101, 100, 99, 98, 97, 96, 95, 92, 65, 62, 124, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 28, 15, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 14, 30, 31, 32, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 47, 46, 45, 44, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 64, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 79  99 104 104  83 145  67 131 115  88 188 262 105 159 197 101 114 138
 141 155  72 148 114 151  97 109  86 140  96 261 115  67  99 203 288 178
 159 159 130 164 177  97 152 127 152 163 269 129 241 228 145 155 267 201
  87 166 198 138 209 115 137 145 142 157 188 101 150  76  92 125 100 160
 114 135 117  77 166 153 104 143  96 115 106 123 161 141 109 112 115 124
 107 123 140  93 224 180 135 165 223 126 122 249 169 175 112 125 255 245
  80 220 282 223 197 185 150 120 109 193  87 105 130 147 154 185 288 269]
CBFL per class weights: tensor([1.3309, 1.1571, 1.1248, 1.1248, 1.2890, 0.9507, 1.4883, 0.9964, 1.0644,
        1.2423, 0.8592, 0.7858, 1.1187, 0.9143, 0.8461, 1.1438, 1.0693, 0.9722,
        0.9627, 0.9239, 1.4161, 0.9422, 1.0693, 0.9341, 1.1711, 1.0957, 1.2603,
        0.9658, 1.1783, 0.7864, 1.0644, 1.4883, 1.1571, 0.8383, 0.7720, 0.8757,
        0.9143, 0.9143, 1.0001, 0.9030, 0.8774, 1.1711, 0.9315, 1.0116, 0.9315,
        0.9052, 0.7816, 1.0038, 0.8003, 0.8113, 0.9507, 0.9239, 0.7828, 0.8408,
        1.2512, 0.8988, 0.8448, 0.9722, 0.8310, 1.0644, 0.9755, 0.9507, 0.9596,
        0.9190, 0.8592, 1.1438, 0.9367, 1.3654, 1.2088, 1.0196, 1.1504, 0.9119,
        1.0693, 0.9822, 1.0547, 1.3536, 0.8988, 0.9289, 1.1248, 0.9566, 1.1783,
        1.0644, 1.1128, 1.0279, 0.9097, 0.9627, 1.0957, 1.0796, 1.0644, 1.0237,
        1.1070, 1.0279, 0.9658, 1.2009, 0.8151, 0.8722, 0.9822, 0.9009, 0.8161,
        1.0155, 1.0322, 0.7943, 0.8926, 0.8811, 1.0796, 1.0196, 0.7902, 0.7973,
        1.3201, 0.8191, 0.7748, 0.8161, 0.8461, 0.8639, 0.9367, 1.0409, 1.0957,
        0.8517, 1.2512, 1.1187, 1.0001, 0.9450, 0.9264, 0.8639, 0.7720, 0.7816],
       device='cuda:0')
S real T clipart Train Ep: 22000 lr0.004179626906102638 	 Loss Classification: 0.061584 Loss T 0.022522 Method MME


Labeled Target set: Average loss: 0.0891, Accuracy: 1059/1080 F1 (98.0556%)


Test set: Average loss: 1.5342, Accuracy: 13416/18312 F1 (73.2634%)


Val set: Average loss: 1.5834, Accuracy: 263/360 F1 (73.0556%)

best acc test 73.782219  acc val 73.055556 acc labeled target 98.055556
saving model...
S real T clipart Train Ep: 22100 lr0.004169857614974071 	 Loss Classification: 0.091849 Loss T 0.034979 Method MME

S real T clipart Train Ep: 22200 lr0.004160141438467499 	 Loss Classification: 0.022536 Loss T 0.048432 Method MME

S real T clipart Train Ep: 22300 lr0.004150477924193236 	 Loss Classification: 0.115305 Loss T 0.030063 Method MME

S real T clipart Train Ep: 22400 lr0.00414086662499961 	 Loss Classification: 0.076706 Loss T 0.046222 Method MME

S real T clipart Train Ep: 22500 lr0.004131307098896385 	 Loss Classification: 0.260816 Loss T 0.029308 Method MME


Labeled Target set: Average loss: 0.0403, Accuracy: 1063/1080 F1 (98.4259%)


Test set: Average loss: 1.5514, Accuracy: 13579/18312 F1 (74.1536%)


Val set: Average loss: 1.5134, Accuracy: 263/360 F1 (73.0556%)

best acc test 73.782219  acc val 73.055556 acc labeled target 98.425926
saving model...
S real T clipart Train Ep: 22600 lr0.0041217989089795196 	 Loss Classification: 0.088087 Loss T 0.040402 Method MME

S real T clipart Train Ep: 22700 lr0.004112341623357265 	 Loss Classification: 0.223845 Loss T 0.030997 Method MME

S real T clipart Train Ep: 22800 lr0.004102934815077543 	 Loss Classification: 0.064789 Loss T 0.034672 Method MME

S real T clipart Train Ep: 22900 lr0.004093578062056604 	 Loss Classification: 0.357092 Loss T 0.045340 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        0.8888889 0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.6666667 1.
 1.        1.        1.        1.        1.        0.8888889 1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.8888889 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        0.8888889 0.6666667 1.
 1.        1.        1.        0.8888889 1.        1.        1.
 1.        0.8888889 1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [47, 63, 103, 82, 102, 113, 54, 10, 108, 9, 27, 85, 83, 86, 87, 88, 89, 81, 84, 0, 79, 90, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 80, 91, 95, 93, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 112, 111, 110, 109, 107, 106, 105, 104, 101, 100, 99, 98, 97, 96, 94, 92, 66, 62, 64, 29, 28, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 30, 16, 14, 13, 12, 11, 8, 7, 6, 5, 4, 3, 2, 1, 15, 31, 32, 33, 124, 61, 60, 59, 58, 57, 56, 55, 53, 52, 51, 50, 49, 48, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 65, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 79 100 103 102  85 149  68 132 113  90 189 264 104 161 197  99 116 138
 141 159  74 150 113 153  98 107  90 137 102 220 118  61  97 199 292 177
 164 156 128 168 178 101 149 122 156 165 267 126 239 229 145 153 266 201
  88 167 198 139 205 114 137 142 146 156 185 101 154  80  92 121  99 160
 112 132 119  86 164 154 101 144  89 116 107 120 156 138 104 113 116 129
 108 123 145  95 221 185 136 165 225 128 124 246 165 178 113 126 257 242
  81 223 287 219 197 190 148 123 110 196  87 101 132 147 155 184 299 268]
CBFL per class weights: tensor([1.3322, 1.1514, 1.1320, 1.1383, 1.2708, 0.9403, 1.4743, 0.9936, 1.0754,
        1.2263, 0.8584, 0.7853, 1.1258, 0.9105, 0.8469, 1.1582, 1.0605, 0.9731,
        0.9635, 0.9151, 1.3913, 0.9376, 1.0754, 0.9297, 1.1651, 1.1080, 1.2263,
        0.9764, 1.1383, 0.8198, 1.0510, 1.5927, 1.1721, 0.8442, 0.7709, 0.8782,
        0.9039, 0.9222, 1.0086, 0.8955, 0.8765, 1.1448, 0.9403, 1.0331, 0.9222,
        0.9017, 0.7835, 1.0165, 0.8026, 0.8112, 0.9515, 0.9297, 0.7841, 0.8416,
        1.2434, 0.8975, 0.8456, 0.9698, 0.8366, 1.0703, 0.9764, 0.9605, 0.9487,
        0.9222, 0.8647, 1.1448, 0.9272, 1.3213, 1.2099, 1.0375, 1.1582, 0.9128,
        1.0805, 0.9936, 1.0464, 1.2615, 0.9039, 0.9272, 1.1448, 0.9545, 1.2348,
        1.0605, 1.1080, 1.0419, 0.9222, 0.9731, 1.1258, 1.0754, 1.0605, 1.0048,
        1.1023, 1.0288, 0.9515, 1.1867, 0.8188, 0.8647, 0.9797, 0.9017, 0.8149,
        1.0086, 1.0246, 0.7972, 0.9017, 0.8765, 1.0754, 1.0165, 0.7896, 0.8003,
        1.3106, 0.8168, 0.7732, 0.8208, 0.8469, 0.8569, 0.9430, 1.0288, 1.0912,
        0.8483, 1.2523, 1.1448, 0.9936, 0.9458, 0.9247, 0.8663, 0.7680, 0.7829],
       device='cuda:0')
S real T clipart Train Ep: 23000 lr0.00408427094700893 	 Loss Classification: 0.091257 Loss T 0.031378 Method MME


Labeled Target set: Average loss: 0.0731, Accuracy: 1062/1080 F1 (98.3333%)


Test set: Average loss: 1.5479, Accuracy: 13418/18312 F1 (73.2744%)


Val set: Average loss: 1.5602, Accuracy: 265/360 F1 (73.6111%)

best acc test 73.782219  acc val 73.611111 acc labeled target 98.333333
saving model...
S real T clipart Train Ep: 23100 lr0.004075013057378346 	 Loss Classification: 0.209708 Loss T 0.040391 Method MME

S real T clipart Train Ep: 23200 lr0.004065803985270331 	 Loss Classification: 0.034036 Loss T 0.033753 Method MME

S real T clipart Train Ep: 23300 lr0.004056643327385506 	 Loss Classification: 0.252590 Loss T 0.042197 Method MME

S real T clipart Train Ep: 23400 lr0.004047530684954247 	 Loss Classification: 0.049872 Loss T 0.030946 Method MME

S real T clipart Train Ep: 23500 lr0.0040384656636724406 	 Loss Classification: 0.169475 Loss T 0.032303 Method MME


Labeled Target set: Average loss: 0.0694, Accuracy: 1059/1080 F1 (98.0556%)


Test set: Average loss: 1.5559, Accuracy: 13578/18312 F1 (74.1481%)


Val set: Average loss: 1.5500, Accuracy: 264/360 F1 (73.3333%)

best acc test 73.782219  acc val 73.333333 acc labeled target 98.055556
saving model...
S real T clipart Train Ep: 23600 lr0.004029447873638333 	 Loss Classification: 0.211954 Loss T 0.023938 Method MME

S real T clipart Train Ep: 23700 lr0.00402047692929045 	 Loss Classification: 0.063825 Loss T 0.028621 Method MME

S real T clipart Train Ep: 23800 lr0.004011552449346588 	 Loss Classification: 0.207756 Loss T 0.036027 Method MME

S real T clipart Train Ep: 23900 lr0.004002674056743821 	 Loss Classification: 0.009633 Loss T 0.033006 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        0.8888889 1.        1.        0.8888889 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.8888889 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        0.8333333 0.6666667 1.
 1.        0.8888889 1.        1.        1.        1.        1.
 1.        0.8333333 1.        1.        1.        1.        0.8888889
 0.5       0.8888889 1.        0.7777778 1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.6666667 1.
 1.        0.8333333 1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [63, 47, 103, 66, 57, 46, 106, 64, 50, 26, 108, 12, 62, 9, 113, 85, 84, 83, 82, 118, 119, 81, 80, 120, 79, 78, 77, 76, 75, 74, 121, 73, 72, 71, 70, 69, 122, 123, 86, 87, 88, 89, 112, 111, 110, 109, 115, 107, 105, 104, 116, 102, 101, 117, 100, 99, 98, 97, 96, 95, 68, 93, 92, 91, 90, 114, 94, 0, 65, 28, 27, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 11, 10, 8, 7, 6, 5, 4, 3, 2, 1, 29, 30, 31, 32, 124, 61, 60, 59, 58, 56, 55, 54, 53, 52, 51, 49, 67, 48, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 45, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 79 101 110 104  86 144  68 134 112  93 192 257 111 154 195  99 118 139
 139 160  72 153 113 153  99 109  90 139  95 192 120  53  99 198 290 178
 168 152 129 165 175  99 152 124 157 167 264 123 238 231 144 154 269 202
  92 167 198 138 205 111 135 144 143 158 189 100 153  78  91 124 100 159
 111 131 120  87 169 153 104 146  82 116 104 118 158 142 103 121 115 131
 107 125 142  95 224 185 138 166 227 127 126 239 169 177 113 127 258 242
  83 224 286 224 200 188 148 123 108 197  86 105 132 147 154 183 301 270]
CBFL per class weights: tensor([1.3312, 1.1440, 1.0904, 1.1250, 1.2606, 0.9538, 1.4733, 0.9859, 1.0798,
        1.2012, 0.8534, 0.7891, 1.0850, 0.9266, 0.8491, 1.1574, 1.0503, 0.9692,
        0.9692, 0.9121, 1.4164, 0.9291, 1.0746, 0.9291, 1.1574, 1.0959, 1.2254,
        0.9692, 1.1859, 0.8534, 1.0412, 1.7664, 1.1574, 0.8450, 0.7713, 0.8758,
        0.8948, 0.9317, 1.0041, 0.9011, 0.8812, 1.1574, 0.9317, 1.0239, 0.9192,
        0.8969, 0.7847, 1.0281, 0.8029, 0.8088, 0.9538, 0.9266, 0.7818, 0.8397,
        1.2091, 0.8969, 0.8450, 0.9724, 0.8360, 1.0850, 0.9824, 0.9538, 0.9568,
        0.9168, 0.8578, 1.1506, 0.9291, 1.3424, 1.2171, 1.0239, 1.1506, 0.9144,
        1.0850, 0.9966, 1.0412, 1.2515, 0.8928, 0.9291, 1.1250, 0.9480, 1.2994,
        1.0597, 1.1250, 1.0503, 0.9168, 0.9598, 1.1312, 1.0367, 1.0646, 0.9966,
        1.1072, 1.0198, 0.9598, 1.1859, 0.8153, 0.8641, 0.9724, 0.8990, 0.8124,
        1.0118, 1.0158, 0.8021, 0.8928, 0.8776, 1.0746, 1.0118, 0.7884, 0.7997,
        1.2893, 0.8153, 0.7731, 0.8153, 0.8423, 0.8593, 0.9424, 1.0281, 1.1015,
        0.8463, 1.2606, 1.1190, 0.9929, 0.9452, 0.9266, 0.8673, 0.7667, 0.7812],
       device='cuda:0')
S real T clipart Train Ep: 24000 lr0.0039938413785795416 	 Loss Classification: 0.031599 Loss T 0.027069 Method MME


Labeled Target set: Average loss: 0.0813, Accuracy: 1059/1080 F1 (98.0556%)


Test set: Average loss: 1.5090, Accuracy: 13508/18312 F1 (73.7658%)


Val set: Average loss: 1.5212, Accuracy: 261/360 F1 (72.5000%)

best acc test 73.782219  acc val 72.500000 acc labeled target 98.055556
saving model...
S real T clipart Train Ep: 24100 lr0.003985054046053481 	 Loss Classification: 0.216537 Loss T 0.029315 Method MME

S real T clipart Train Ep: 24200 lr0.003976311694410721 	 Loss Classification: 0.036561 Loss T 0.036621 Method MME

S real T clipart Train Ep: 24300 lr0.00396761396288564 	 Loss Classification: 0.095672 Loss T 0.035812 Method MME

S real T clipart Train Ep: 24400 lr0.003958960494646819 	 Loss Classification: 0.051942 Loss T 0.039319 Method MME

S real T clipart Train Ep: 24500 lr0.0039503509367428465 	 Loss Classification: 0.113117 Loss T 0.032003 Method MME


Labeled Target set: Average loss: 0.0404, Accuracy: 1067/1080 F1 (98.7963%)


Test set: Average loss: 1.5617, Accuracy: 13590/18312 F1 (74.2136%)


Val set: Average loss: 1.4658, Accuracy: 267/360 F1 (74.1667%)

best acc test 73.782219  acc val 74.166667 acc labeled target 98.796296
saving model...
S real T clipart Train Ep: 24600 lr0.00394178494004904 	 Loss Classification: 0.302677 Loss T 0.025973 Method MME

S real T clipart Train Ep: 24700 lr0.003933262159215038 	 Loss Classification: 0.065878 Loss T 0.028581 Method MME

S real T clipart Train Ep: 24800 lr0.00392478225261327 	 Loss Classification: 0.078657 Loss T 0.027686 Method MME

S real T clipart Train Ep: 24900 lr0.003916344882288264 	 Loss Classification: 0.206487 Loss T 0.037529 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.
 0.8333333 1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 0.8888889 1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.6666667 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [63, 103, 91, 56, 48, 70, 80, 87, 88, 84, 83, 89, 82, 81, 90, 86, 85, 77, 78, 76, 75, 74, 73, 72, 71, 69, 68, 67, 66, 65, 79, 92, 0, 94, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 64, 110, 108, 107, 106, 105, 104, 102, 101, 100, 99, 98, 97, 96, 95, 109, 93, 62, 61, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 28, 124, 29, 31, 60, 59, 58, 57, 55, 54, 53, 52, 51, 50, 49, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 30, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 84 103 110 104  86 142  66 134 110 104 193 259 106 154 196  98 119 143
 141 164  74 151 111 153 100 111  89 133  95 168 122  52  97 198 288 177
 176 159 130 163 176  99 153 121 155 167 260 125 235 230 143 151 272 202
  89 170 200 138 208 114 136 144 145 159 195 102 156  73  92 115 101 160
 112 130 118  90 170 152 103 141  78 113 108 119 157 141 103 115 118 130
 106 123 145  94 228 185 144 167 227 126 130 236 167 181 114 130 257 242
  86 223 289 225 201 174 150 122 109 201  88 104 134 145 150 180 304 269]
CBFL per class weights: tensor([1.2794, 1.1311, 1.0904, 1.1250, 1.2605, 0.9597, 1.5044, 0.9858, 1.0904,
        1.1250, 0.8519, 0.7877, 1.1129, 0.9265, 0.8476, 1.1642, 1.0456, 0.9567,
        0.9628, 0.9032, 1.3903, 0.9342, 1.0850, 0.9290, 1.1505, 1.0850, 1.2338,
        0.9893, 1.1858, 0.8948, 1.0323, 1.7920, 1.1712, 0.8449, 0.7721, 0.8776,
        0.8794, 0.9144, 1.0002, 0.9053, 0.8794, 1.1573, 0.9290, 1.0367, 0.9240,
        0.8968, 0.7871, 1.0197, 0.8053, 0.8096, 0.9567, 0.9342, 0.7801, 0.8397,
        1.2338, 0.8907, 0.8423, 0.9723, 0.8323, 1.0695, 0.9790, 0.9537, 0.9508,
        0.9144, 0.8490, 1.1375, 0.9215, 1.4031, 1.2090, 1.0645, 1.1439, 0.9121,
        1.0797, 1.0002, 1.0502, 1.2253, 0.8907, 0.9316, 1.1311, 0.9628, 1.3423,
        1.0746, 1.1014, 1.0456, 0.9191, 0.9628, 1.1311, 1.0645, 1.0502, 1.0002,
        1.1129, 1.0280, 0.9508, 1.1934, 0.8115, 0.8640, 0.9537, 0.8968, 0.8124,
        1.0157, 1.0002, 0.8045, 0.8968, 0.8706, 1.0695, 1.0002, 0.7890, 0.7997,
        1.2605, 0.8162, 0.7717, 0.8143, 0.8410, 0.8831, 0.9369, 1.0323, 1.0958,
        0.8410, 1.2425, 1.1250, 0.9858, 0.9508, 0.9369, 0.8723, 0.7655, 0.7818],
       device='cuda:0')
S real T clipart Train Ep: 25000 lr0.003907949713906802 	 Loss Classification: 0.017796 Loss T 0.032129 Method MME


Labeled Target set: Average loss: 0.0750, Accuracy: 1064/1080 F1 (98.5185%)


Test set: Average loss: 1.5971, Accuracy: 13387/18312 F1 (73.1051%)


Val set: Average loss: 1.5232, Accuracy: 268/360 F1 (74.4444%)

best acc test 73.105068  acc val 74.444444 acc labeled target 98.518519
saving model...
S real T clipart Train Ep: 25100 lr0.003899596416708869 	 Loss Classification: 0.028770 Loss T 0.028790 Method MME

S real T clipart Train Ep: 25200 lr0.0038912846634594346 	 Loss Classification: 0.395041 Loss T 0.038313 Method MME

S real T clipart Train Ep: 25300 lr0.0038830141304009892 	 Loss Classification: 0.076891 Loss T 0.036036 Method MME

S real T clipart Train Ep: 25400 lr0.003874784497206876 	 Loss Classification: 0.212347 Loss T 0.031666 Method MME

S real T clipart Train Ep: 25500 lr0.003866595446935362 	 Loss Classification: 0.304187 Loss T 0.029125 Method MME


Labeled Target set: Average loss: 0.0340, Accuracy: 1070/1080 F1 (99.0741%)


Test set: Average loss: 1.5947, Accuracy: 13562/18312 F1 (74.0607%)


Val set: Average loss: 1.5088, Accuracy: 265/360 F1 (73.6111%)

best acc test 73.105068  acc val 73.611111 acc labeled target 99.074074
saving model...
S real T clipart Train Ep: 25600 lr0.003858446665984465 	 Loss Classification: 0.219814 Loss T 0.032373 Method MME

S real T clipart Train Ep: 25700 lr0.0038503378440474917 	 Loss Classification: 0.059683 Loss T 0.032533 Method MME

S real T clipart Train Ep: 25800 lr0.003842268674069313 	 Loss Classification: 0.076385 Loss T 0.036159 Method MME

S real T clipart Train Ep: 25900 lr0.0038342388522033147 	 Loss Classification: 0.077199 Loss T 0.023018 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.8333333 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        0.8888889 1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [63, 91, 12, 41, 106, 48, 71, 90, 89, 88, 87, 86, 85, 66, 84, 83, 82, 67, 81, 80, 79, 68, 78, 77, 69, 76, 75, 74, 73, 72, 70, 92, 0, 65, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 93, 110, 108, 107, 105, 104, 103, 102, 101, 100, 99, 98, 97, 96, 95, 109, 94, 62, 124, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 29, 64, 30, 32, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 47, 46, 45, 44, 43, 42, 40, 39, 38, 37, 36, 35, 34, 33, 31, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 84 101 109 102  86 141  69 133 113  92 194 255 102 151 199  99 117 144
 136 165  78 150 112 161 100 108  90 131  92 142 123  53  98 199 295 180
 176 157 127 165 176 104 153 123 154 168 256 127 234 230 145 151 273 201
  89 169 199 138 214 115 135 145 145 160 193 103 156  73  94 116 101 162
 112 126 117  93 170 148 104 141  74 118 107 117 158 140  98 114 121 132
 103 124 144  92 244 182 149 168 228 127 128 228 166 186 115 131 261 243
  84 220 289 224 197 185 148 123 109 205  88 105 134 145 156 181 306 264]
CBFL per class weights: tensor([1.2789, 1.1435, 1.0954, 1.1370, 1.2600, 0.9624, 1.4577, 0.9889, 1.0741,
        1.2085, 0.8501, 0.7900, 1.1370, 0.9338, 0.8432, 1.1568, 1.0544, 0.9533,
        0.9785, 0.9006, 1.3418, 0.9365, 1.0793, 0.9094, 1.1501, 1.1010, 1.2248,
        0.9961, 1.2085, 0.9593, 1.0276, 1.7655, 1.1637, 0.8432, 0.7687, 0.8719,
        0.8790, 0.9187, 1.0113, 0.9006, 0.8790, 1.1245, 0.9286, 1.0276, 0.9261,
        0.8944, 0.7893, 1.0113, 0.8058, 0.8093, 0.9504, 0.9338, 0.7792, 0.8406,
        1.2333, 0.8924, 0.8432, 0.9719, 0.8251, 1.0641, 0.9819, 0.9504, 0.9504,
        0.9117, 0.8515, 1.1307, 0.9212, 1.4025, 1.1929, 1.0592, 1.1435, 0.9072,
        1.0793, 1.0153, 1.0544, 1.2006, 0.8904, 0.9419, 1.1245, 0.9624, 1.3897,
        1.0498, 1.1067, 1.0544, 0.9164, 0.9655, 1.1637, 1.0691, 1.0362, 0.9925,
        1.1307, 1.0234, 0.9533, 1.2085, 0.7978, 0.8685, 0.9392, 0.8944, 0.8111,
        1.0113, 1.0074, 0.8111, 0.8985, 0.8621, 1.0641, 0.9961, 0.7862, 0.7986,
        1.2789, 0.8188, 0.7714, 0.8149, 0.8459, 0.8636, 0.9419, 1.0276, 1.0954,
        0.8356, 1.2420, 1.1184, 0.9854, 0.9504, 0.9212, 0.8702, 0.7644, 0.7843],
       device='cuda:0')
S real T clipart Train Ep: 26000 lr0.0038262480777690546 	 Loss Classification: 0.015234 Loss T 0.038382 Method MME


Labeled Target set: Average loss: 0.0730, Accuracy: 1059/1080 F1 (98.0556%)


Test set: Average loss: 1.6186, Accuracy: 13403/18312 F1 (73.1924%)


Val set: Average loss: 1.6499, Accuracy: 261/360 F1 (72.5000%)

best acc test 73.105068  acc val 72.500000 acc labeled target 98.055556
saving model...
S real T clipart Train Ep: 26100 lr0.0038182960532105875 	 Loss Classification: 0.170058 Loss T 0.023491 Method MME

S real T clipart Train Ep: 26200 lr0.0038103824840554513 	 Loss Classification: 0.451297 Loss T 0.013910 Method MME

S real T clipart Train Ep: 26300 lr0.0038025070788743048 	 Loss Classification: 0.187939 Loss T 0.019834 Method MME

S real T clipart Train Ep: 26400 lr0.003794669549241204 	 Loss Classification: 0.093599 Loss T 0.036371 Method MME

S real T clipart Train Ep: 26500 lr0.0037868696096944997 	 Loss Classification: 0.033202 Loss T 0.024231 Method MME


Labeled Target set: Average loss: 0.0599, Accuracy: 1061/1080 F1 (98.2407%)


Test set: Average loss: 1.6488, Accuracy: 13575/18312 F1 (74.1317%)


Val set: Average loss: 1.5669, Accuracy: 265/360 F1 (73.6111%)

best acc test 73.105068  acc val 73.611111 acc labeled target 98.240741
saving model...
S real T clipart Train Ep: 26600 lr0.00377910697769836 	 Loss Classification: 0.124308 Loss T 0.032175 Method MME

S real T clipart Train Ep: 26700 lr0.0037713813736048834 	 Loss Classification: 0.150887 Loss T 0.034368 Method MME

S real T clipart Train Ep: 26800 lr0.0037636925206168117 	 Loss Classification: 0.009411 Loss T 0.029409 Method MME

S real T clipart Train Ep: 26900 lr0.0037560401447508216 	 Loss Classification: 0.180319 Loss T 0.018009 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.8888889 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        0.8888889 1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        1.        0.8333333
 1.        1.        1.        1.        1.        1.        1.
 0.8888889 0.8888889 1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.8888889 1.
 0.8888889 1.        1.        1.        1.        1.        1.
 0.5       1.        1.        1.        1.        0.8333333 1.
 1.        1.        1.        1.        1.        0.6666667 1.
 1.        0.8888889 1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [91, 63, 103, 96, 48, 106, 57, 56, 38, 29, 12, 84, 82, 89, 68, 88, 87, 86, 85, 83, 69, 71, 70, 80, 90, 79, 78, 77, 76, 75, 74, 73, 72, 81, 0, 94, 93, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 105, 104, 102, 101, 100, 99, 98, 97, 95, 67, 92, 66, 62, 64, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 28, 15, 13, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 14, 65, 30, 32, 124, 61, 60, 59, 58, 55, 54, 53, 52, 51, 50, 49, 31, 47, 45, 44, 43, 42, 41, 40, 39, 37, 36, 35, 34, 33, 46, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 82 103 114 104  86 137  69 130 112  94 192 254 104 152 203  99 119 147
 141 166  76 152 111 160  99 111  90 126  84 120 126  51 102 199 293 180
 180 157 128 168 177 109 153 120 153 171 259 129 233 231 146 150 275 201
  91 169 200 138 213 114 136 143 146 158 192 102 154  72  94 117 102 163
 113 128 115  87 170 153 105 140  73 119 108 120 159 141 100 112 118 130
 102 130 142  90 231 183 147 170 227 125 128 236 167 185 116 131 261 244
  81 223 288 223 198 192 148 124 110 202  87 105 132 146 154 183 303 266]
CBFL per class weights: tensor([1.2972, 1.1293, 1.0678, 1.1231, 1.2584, 0.9740, 1.4560, 0.9986, 1.0780,
        1.1914, 0.8519, 0.7897, 1.1231, 0.9301, 0.8370, 1.1554, 1.0439, 0.9436,
        0.9612, 0.8974, 1.3634, 0.9301, 1.0832, 0.9106, 1.1554, 1.0832, 1.2233,
        1.0140, 1.2773, 1.0394, 1.0140, 1.8158, 1.1356, 0.8422, 0.7687, 0.8709,
        0.8709, 0.9176, 1.0062, 0.8933, 0.8761, 1.0940, 0.9275, 1.0394, 0.9275,
        0.8873, 0.7865, 1.0024, 0.8057, 0.8074, 0.9464, 0.9354, 0.7772, 0.8396,
        1.2151, 0.8913, 0.8409, 0.9707, 0.8252, 1.0678, 0.9774, 0.9552, 0.9464,
        0.9152, 0.8519, 1.1356, 0.9250, 1.4140, 1.1914, 1.0532, 1.1356, 0.9039,
        1.0728, 1.0062, 1.0628, 1.2494, 0.8893, 0.9275, 1.1171, 0.9644, 1.4008,
        1.0439, 1.0996, 1.0394, 0.9129, 0.9612, 1.1487, 1.0780, 1.0485, 0.9986,
        1.1356, 0.9986, 0.9582, 1.2233, 0.8074, 0.8658, 0.9436, 0.8893, 0.8111,
        1.0181, 1.0062, 0.8032, 0.8954, 0.8626, 1.0579, 0.9949, 0.7852, 0.7968,
        1.3075, 0.8149, 0.7709, 0.8149, 0.8435, 0.8519, 0.9408, 1.0222, 1.0886,
        0.8383, 1.2494, 1.1171, 0.9913, 0.9464, 0.9250, 0.8658, 0.7646, 0.7822],
       device='cuda:0')
S real T clipart Train Ep: 27000 lr0.003748423974801389 	 Loss Classification: 0.141431 Loss T 0.025427 Method MME


Labeled Target set: Average loss: 0.0542, Accuracy: 1071/1080 F1 (99.1667%)


Test set: Average loss: 1.6168, Accuracy: 13465/18312 F1 (73.5310%)


Val set: Average loss: 1.5465, Accuracy: 265/360 F1 (73.6111%)

best acc test 73.105068  acc val 73.611111 acc labeled target 99.166667
saving model...
S real T clipart Train Ep: 27100 lr0.003740843742305213 	 Loss Classification: 0.247909 Loss T 0.026836 Method MME

S real T clipart Train Ep: 27200 lr0.0037332991815061845 	 Loss Classification: 0.036594 Loss T 0.023368 Method MME

S real T clipart Train Ep: 27300 lr0.003725790029320905 	 Loss Classification: 0.062145 Loss T 0.023764 Method MME

S real T clipart Train Ep: 27400 lr0.0037183160253047272 	 Loss Classification: 0.093322 Loss T 0.018914 Method MME

S real T clipart Train Ep: 27500 lr0.003710876911618321 	 Loss Classification: 0.026730 Loss T 0.030004 Method MME


Labeled Target set: Average loss: 0.0388, Accuracy: 1065/1080 F1 (98.6111%)


Test set: Average loss: 1.6560, Accuracy: 13529/18312 F1 (73.8805%)


Val set: Average loss: 1.4495, Accuracy: 265/360 F1 (73.6111%)

best acc test 73.105068  acc val 73.611111 acc labeled target 98.611111
saving model...
S real T clipart Train Ep: 27600 lr0.0037034724329947483 	 Loss Classification: 0.316779 Loss T 0.015975 Method MME

S real T clipart Train Ep: 27700 lr0.0036961023367070435 	 Loss Classification: 0.074989 Loss T 0.029867 Method MME

S real T clipart Train Ep: 27800 lr0.003688766372536283 	 Loss Classification: 0.231096 Loss T 0.022677 Method MME

S real T clipart Train Ep: 27900 lr0.0036814642927401444 	 Loss Classification: 0.172316 Loss T 0.022332 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        0.8888889 1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        0.8888889 1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        0.8888889 1.        1.        1.        0.6666667 1.
 1.        1.        1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [63, 91, 103, 11, 80, 99, 108, 29, 3, 87, 86, 89, 84, 83, 82, 81, 90, 88, 85, 79, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 78, 92, 0, 94, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 93, 111, 109, 107, 106, 105, 104, 102, 101, 100, 98, 97, 65, 95, 110, 96, 62, 124, 30, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 10, 9, 8, 7, 6, 5, 4, 2, 1, 31, 64, 32, 34, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 33, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 84 104 114 105  87 139  71 130 114  97 191 257 104 152 204  99 119 145
 138 164  77 153 113 158  98 115  97 121  87 106 127  51 101 200 293 179
 179 157 128 167 175 111 152 123 152 173 262 127 236 233 149 152 278 200
  89 166 202 138 212 113 136 142 147 155 189 102 153  74  94 124 102 163
 113 125 117  90 169 153 104 139  70 121 109 117 161 140 107 112 115 127
 101 129 141  93 220 184 147 168 231 125 129 241 166 180 115 131 265 241
  82 223 289 226 198 190 148 124 110 198  86 105 132 144 153 183 300 267]
CBFL per class weights: tensor([1.2786, 1.1243, 1.0688, 1.1182, 1.2506, 0.9685, 1.4290, 0.9996, 1.0688,
        1.1705, 0.8542, 0.7885, 1.1243, 0.9310, 0.8366, 1.1566, 1.0450, 0.9502,
        0.9717, 0.9026, 1.3530, 0.9285, 1.0739, 0.9162, 1.1635, 1.0639, 1.1705,
        1.0360, 1.2506, 1.1123, 1.0111, 1.8177, 1.1432, 0.8417, 0.7694, 0.8735,
        0.8735, 0.9186, 1.0072, 0.8963, 0.8807, 1.0843, 0.9310, 1.0274, 0.9310,
        0.8844, 0.7854, 1.0111, 0.8040, 0.8065, 0.9390, 0.9310, 0.7765, 0.8417,
        1.2331, 0.8984, 0.8392, 0.9717, 0.8272, 1.0739, 0.9784, 0.9591, 0.9445,
        0.9234, 0.8572, 1.1368, 0.9285, 1.3894, 1.1926, 1.0232, 1.1368, 0.9048,
        1.0739, 1.0191, 1.0542, 1.2246, 0.8922, 0.9285, 1.1243, 0.9685, 1.4430,
        1.0360, 1.0952, 1.0542, 0.9092, 0.9653, 1.1064, 1.0791, 1.0639, 1.0111,
        1.1432, 1.0034, 0.9622, 1.2004, 0.8187, 0.8651, 0.9445, 0.8942, 0.8083,
        1.0191, 1.0034, 0.7999, 0.8984, 0.8718, 1.0639, 0.9959, 0.7836, 0.7999,
        1.2985, 0.8157, 0.7712, 0.8128, 0.8444, 0.8557, 0.9417, 1.0232, 1.0897,
        0.8444, 1.2597, 1.1182, 0.9923, 0.9532, 0.9285, 0.8667, 0.7666, 0.7824],
       device='cuda:0')
S real T clipart Train Ep: 28000 lr0.003674195852021934 	 Loss Classification: 0.192015 Loss T 0.021391 Method MME


Labeled Target set: Average loss: 0.0657, Accuracy: 1064/1080 F1 (98.5185%)


Test set: Average loss: 1.6388, Accuracy: 13504/18312 F1 (73.7440%)


Val set: Average loss: 1.5794, Accuracy: 267/360 F1 (74.1667%)

best acc test 73.105068  acc val 74.166667 acc labeled target 98.518519
saving model...
S real T clipart Train Ep: 28100 lr0.0036669608075000928 	 Loss Classification: 0.137363 Loss T 0.024628 Method MME

S real T clipart Train Ep: 28200 lr0.00365975891867815 	 Loss Classification: 0.092487 Loss T 0.022431 Method MME

S real T clipart Train Ep: 28300 lr0.003652589947415138 	 Loss Classification: 0.081657 Loss T 0.032286 Method MME

S real T clipart Train Ep: 28400 lr0.0036454536578964408 	 Loss Classification: 0.118507 Loss T 0.019488 Method MME

S real T clipart Train Ep: 28500 lr0.0036383498166050877 	 Loss Classification: 0.143845 Loss T 0.020825 Method MME


Labeled Target set: Average loss: 0.0394, Accuracy: 1065/1080 F1 (98.6111%)


Test set: Average loss: 1.7338, Accuracy: 13516/18312 F1 (73.8095%)


Val set: Average loss: 1.6984, Accuracy: 263/360 F1 (73.0556%)

best acc test 73.105068  acc val 73.055556 acc labeled target 98.611111
saving model...
S real T clipart Train Ep: 28600 lr0.0036312781922934662 	 Loss Classification: 0.220908 Loss T 0.021946 Method MME

S real T clipart Train Ep: 28700 lr0.003624238555955462 	 Loss Classification: 0.134951 Loss T 0.023124 Method MME

S real T clipart Train Ep: 28800 lr0.003617230680799007 	 Loss Classification: 0.146390 Loss T 0.022008 Method MME

S real T clipart Train Ep: 28900 lr0.0036102543422190363 	 Loss Classification: 0.263606 Loss T 0.030079 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        0.8888889 1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        0.7777778 1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        0.6666667 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        0.8888889 1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.6666667 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        0.8888889]
Top k classes which perform poorly are:  [47, 103, 63, 23, 45, 11, 125, 86, 85, 87, 88, 89, 90, 83, 82, 91, 81, 84, 80, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 79, 92, 94, 95, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 102, 101, 100, 99, 98, 97, 96, 93, 65, 0, 124, 28, 27, 26, 25, 24, 22, 21, 20, 19, 18, 17, 16, 29, 15, 13, 12, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 14, 30, 31, 32, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 46, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 64, 62]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 82 103 113 104  88 139  70 128 114 102 193 261 100 150 200 100 120 146
 138 166  75 155 112 157 100 112 100 120  86 100 125  55 101 201 294 179
 173 157 128 167 176 111 150 124 150 174 259 124 236 232 149 148 278 198
  82 167 200 139 208 117 137 144 146 158 188 102 152  79  95 127 103 164
 115 123 117  93 173 152 104 143  68 120 107 113 159 141 109 113 113 129
 109 128 143  94 220 183 145 166 231 123 126 244 168 180 117 133 264 241
  83 225 285 222 198 187 149 124 110 200  88 105 130 143 158 183 307 268]
CBFL per class weights: tensor([1.2994, 1.1312, 1.0746, 1.1250, 1.2426, 0.9692, 1.4440, 1.0079, 1.0696,
        1.1376, 0.8519, 0.7865, 1.1506, 0.9370, 0.8423, 1.1506, 1.0412, 0.9480,
        0.9724, 0.8990, 1.3779, 0.9241, 1.0798, 0.9192, 1.1506, 1.0798, 1.1506,
        1.0412, 1.2606, 1.1506, 1.0198, 1.7178, 1.1440, 0.8410, 0.7695, 0.8741,
        0.8850, 0.9192, 1.0079, 0.8969, 0.8794, 1.0851, 0.9370, 1.0239, 0.9370,
        0.8831, 0.7878, 1.0239, 0.8045, 0.8079, 0.9397, 0.9424, 0.7770, 0.8450,
        1.2994, 0.8969, 0.8423, 0.9692, 0.8324, 1.0550, 0.9757, 0.9538, 0.9480,
        0.9168, 0.8594, 1.1376, 0.9317, 1.3312, 1.1859, 1.0118, 1.1312, 0.9032,
        1.0646, 1.0281, 1.0550, 1.2012, 0.8850, 0.9317, 1.1250, 0.9568, 1.4733,
        1.0412, 1.1072, 1.0746, 0.9145, 0.9629, 1.0959, 1.0746, 1.0746, 1.0041,
        1.0959, 1.0079, 0.9568, 1.1935, 0.8192, 0.8673, 0.9509, 0.8990, 0.8088,
        1.0281, 1.0158, 0.7982, 0.8948, 0.8724, 1.0550, 0.9894, 0.7847, 0.8005,
        1.2893, 0.8143, 0.7736, 0.8172, 0.8450, 0.8609, 0.9397, 1.0239, 1.0904,
        0.8423, 1.2426, 1.1190, 1.0003, 0.9568, 0.9168, 0.8673, 0.7644, 0.7824],
       device='cuda:0')
S real T clipart Train Ep: 29000 lr0.003603309317770844 	 Loss Classification: 0.130486 Loss T 0.019745 Method MME


Labeled Target set: Average loss: 0.0669, Accuracy: 1060/1080 F1 (98.1481%)


Test set: Average loss: 1.6165, Accuracy: 13510/18312 F1 (73.7768%)


Val set: Average loss: 1.6260, Accuracy: 266/360 F1 (73.8889%)

best acc test 73.105068  acc val 73.888889 acc labeled target 98.148148
saving model...
S real T clipart Train Ep: 29100 lr0.0035963953871438275 	 Loss Classification: 0.329745 Loss T 0.037492 Method MME

S real T clipart Train Ep: 29200 lr0.0035895123321356215 	 Loss Classification: 0.145049 Loss T 0.027728 Method MME

S real T clipart Train Ep: 29300 lr0.003582659936626608 	 Loss Classification: 0.026114 Loss T 0.023023 Method MME

S real T clipart Train Ep: 29400 lr0.0035758379865547998 	 Loss Classification: 0.327990 Loss T 0.027008 Method MME

S real T clipart Train Ep: 29500 lr0.0035690462698910875 	 Loss Classification: 0.116369 Loss T 0.022890 Method MME


Labeled Target set: Average loss: 0.0339, Accuracy: 1069/1080 F1 (98.9815%)


Test set: Average loss: 1.7327, Accuracy: 13534/18312 F1 (73.9078%)


Val set: Average loss: 1.6074, Accuracy: 265/360 F1 (73.6111%)

best acc test 73.105068  acc val 73.611111 acc labeled target 98.981481
saving model...
S real T clipart Train Ep: 29600 lr0.0035622845766148485 	 Loss Classification: 0.026175 Loss T 0.028756 Method MME

S real T clipart Train Ep: 29700 lr0.0035555526986899093 	 Loss Classification: 0.027294 Loss T 0.030589 Method MME

S real T clipart Train Ep: 29800 lr0.0035488504300408524 	 Loss Classification: 0.108393 Loss T 0.023413 Method MME

S real T clipart Train Ep: 29900 lr0.0035421775665296674 	 Loss Classification: 0.181271 Loss T 0.021344 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.8333333 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.5       1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [103, 63, 91, 12, 118, 80, 81, 82, 83, 84, 86, 87, 88, 89, 90, 85, 79, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 78, 92, 0, 64, 123, 122, 121, 120, 119, 117, 116, 115, 114, 113, 112, 111, 110, 109, 108, 107, 106, 105, 104, 102, 101, 100, 99, 98, 97, 96, 95, 93, 94, 62, 61, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 29, 30, 31, 32, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 124, 47, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 46, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 78 103 111 103  89 142  71 130 113 107 191 258 100 150 196 100 122 144
 133 165  77 153 111 157 102 112  96 121  89  94 124  54 101 201 293 179
 170 157 131 171 179 110 150 125 149 175 260 121 237 231 151 149 280 197
  85 164 193 138 209 113 138 143 146 159 191 100 152  80  95 126 104 160
 116 124 119  91 172 150 104 146  67 121 107 115 157 143 108 111 114 131
 104 129 142  97 225 183 149 169 231 122 126 237 168 184 116 133 272 241
  87 225 284 223 196 184 150 124 114 199  88 107 128 143 161 183 305 269]
CBFL per class weights: tensor([1.3426, 1.1314, 1.0852, 1.1314, 1.2341, 0.9599, 1.4302, 1.0004, 1.0748,
        1.1074, 0.8550, 0.7885, 1.1508, 0.9371, 0.8478, 1.1508, 1.0325, 0.9540,
        0.9895, 0.9012, 1.3541, 0.9292, 1.0852, 0.9193, 1.1377, 1.0800, 1.1787,
        1.0369, 1.2341, 1.1936, 1.0241, 1.7419, 1.1442, 0.8411, 0.7701, 0.8742,
        0.8909, 0.9193, 0.9967, 0.8890, 0.8742, 1.0906, 0.9371, 1.0200, 0.9398,
        0.8814, 0.7873, 1.0369, 0.8038, 0.8089, 0.9344, 0.9398, 0.7761, 0.8464,
        1.2701, 0.9034, 0.8520, 0.9725, 0.8313, 1.0748, 0.9725, 0.9569, 0.9481,
        0.9146, 0.8550, 1.1508, 0.9318, 1.3205, 1.1861, 1.0159, 1.1252, 0.9123,
        1.0599, 1.0241, 1.0458, 1.2173, 0.8870, 0.9371, 1.1252, 0.9481, 1.4889,
        1.0369, 1.1074, 1.0648, 0.9193, 0.9569, 1.1017, 1.0852, 1.0697, 0.9967,
        1.1252, 1.0042, 0.9599, 1.1715, 0.8144, 0.8674, 0.9398, 0.8929, 0.8089,
        1.0325, 1.0159, 0.8038, 0.8950, 0.8658, 1.0599, 0.9895, 0.7803, 0.8006,
        1.2517, 0.8144, 0.7742, 0.8164, 0.8478, 0.8658, 0.9371, 1.0241, 1.0697,
        0.8438, 1.2428, 1.1074, 1.0080, 0.9569, 0.9100, 0.8674, 0.7653, 0.7819],
       device='cuda:0')
S real T clipart Train Ep: 30000 lr0.003535533905932738 	 Loss Classification: 0.034935 Loss T 0.016956 Method MME


Labeled Target set: Average loss: 0.0706, Accuracy: 1061/1080 F1 (98.2407%)


Test set: Average loss: 1.6828, Accuracy: 13503/18312 F1 (73.7385%)


Val set: Average loss: 1.6808, Accuracy: 259/360 F1 (71.9444%)

best acc test 73.105068  acc val 71.944444 acc labeled target 98.240741
saving model...
S real T clipart Train Ep: 30100 lr0.0035289192479181558 	 Loss Classification: 0.017136 Loss T 0.026713 Method MME

S real T clipart Train Ep: 30200 lr0.003522333394023364 	 Loss Classification: 0.057839 Loss T 0.022644 Method MME

S real T clipart Train Ep: 30300 lr0.0035157761476331158 	 Loss Classification: 0.088617 Loss T 0.017899 Method MME

S real T clipart Train Ep: 30400 lr0.003509247313957748 	 Loss Classification: 0.010318 Loss T 0.020955 Method MME

S real T clipart Train Ep: 30500 lr0.003502746700011762 	 Loss Classification: 0.291415 Loss T 0.023325 Method MME


Labeled Target set: Average loss: 0.0462, Accuracy: 1061/1080 F1 (98.2407%)


Test set: Average loss: 1.7614, Accuracy: 13533/18312 F1 (73.9024%)


Val set: Average loss: 1.5570, Accuracy: 270/360 F1 (75.0000%)

best acc test 73.902359  acc val 75.000000 acc labeled target 98.240741
saving model...
S real T clipart Train Ep: 30600 lr0.003496274114592713 	 Loss Classification: 0.017496 Loss T 0.032021 Method MME

S real T clipart Train Ep: 30700 lr0.0034898293682603908 	 Loss Classification: 0.030945 Loss T 0.014374 Method MME

S real T clipart Train Ep: 30800 lr0.0034834122733162975 	 Loss Classification: 0.268024 Loss T 0.022671 Method MME

S real T clipart Train Ep: 30900 lr0.0034770226437834152 	 Loss Classification: 0.057439 Loss T 0.012806 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        0.8888889 1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        0.8888889 1.        1.
 0.8888889 1.        0.8888889 0.8888889 1.        0.6666667 0.8888889
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        0.8888889 1.        1.        1.        1.
 0.6666667 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        0.8888889 1.        1.
 1.        1.        1.        1.        0.8888889 0.6666667 0.8888889
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [47, 63, 103, 104, 102, 45, 44, 25, 48, 58, 95, 39, 42, 84, 85, 86, 87, 83, 0, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 88, 89, 92, 91, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 90, 110, 108, 107, 106, 105, 101, 100, 99, 98, 97, 96, 94, 93, 68, 109, 67, 62, 65, 26, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 27, 28, 29, 30, 64, 124, 61, 60, 59, 57, 56, 55, 54, 53, 52, 51, 66, 50, 46, 43, 41, 40, 38, 37, 36, 35, 34, 33, 32, 31, 49, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 78 103 111 102  90 137  70 130 114 105 190 259 103 153 195 101 120 145
 134 168  77 152 109 158 102 113  95 129  90  81 119  53 102 202 294 178
 173 158 131 173 178 111 147 126 149 175 264 121 236 231 151 150 277 198
  91 166 189 138 210 112 138 143 146 159 190 100 153  80  93 128 103 162
 115 125 121  93 170 154 104 148  67 121 108 117 157 140 106 112 114 130
 104 130 142  96 224 183 147 172 233 122 125 239 167 184 116 132 264 239
  86 223 286 223 196 184 151 124 117 199  87 106 130 144 159 183 304 268]
CBFL per class weights: tensor([1.3420, 1.1309, 1.0847, 1.1372, 1.2250, 0.9754, 1.4435, 1.0000, 1.0692,
        1.1186, 0.8560, 0.7875, 1.1309, 0.9288, 0.8488, 1.1437, 1.0408, 0.9506,
        0.9856, 0.8945, 1.3535, 0.9314, 1.0955, 0.9165, 1.1372, 1.0743, 1.1855,
        1.0037, 1.2250, 1.3093, 1.0453, 1.7658, 1.1372, 0.8395, 0.7693, 0.8756,
        0.8847, 0.9165, 0.9963, 0.8847, 0.8756, 1.0847, 0.9449, 1.0154, 0.9393,
        0.8810, 0.7845, 1.0364, 0.8043, 0.8086, 0.9340, 0.9366, 0.7773, 0.8447,
        1.2168, 0.8987, 0.8575, 0.9721, 0.8298, 1.0794, 0.9721, 0.9565, 0.9477,
        0.9142, 0.8560, 1.1503, 0.9288, 1.3199, 1.2008, 1.0076, 1.1309, 0.9073,
        1.0643, 1.0195, 1.0364, 1.2008, 0.8905, 0.9263, 1.1247, 0.9421, 1.4882,
        1.0364, 1.1011, 1.0546, 0.9189, 0.9657, 1.1127, 1.0794, 1.0692, 1.0000,
        1.1247, 1.0000, 0.9595, 1.1782, 0.8150, 0.8670, 0.9449, 0.8866, 0.8068,
        1.0321, 1.0195, 0.8018, 0.8966, 0.8654, 1.0594, 0.9926, 0.7845, 0.8018,
        1.2602, 0.8160, 0.7729, 0.8160, 0.8474, 0.8654, 0.9340, 1.0236, 1.0546,
        0.8434, 1.2511, 1.1127, 1.0000, 0.9535, 0.9142, 0.8670, 0.7653, 0.7821],
       device='cuda:0')
S real T clipart Train Ep: 31000 lr0.003470660295386255 	 Loss Classification: 0.157396 Loss T 0.015605 Method MME


Labeled Target set: Average loss: 0.0631, Accuracy: 1062/1080 F1 (98.3333%)


Test set: Average loss: 1.6571, Accuracy: 13481/18312 F1 (73.6184%)


Val set: Average loss: 1.6639, Accuracy: 262/360 F1 (72.7778%)

best acc test 73.902359  acc val 72.777778 acc labeled target 98.333333
saving model...
S real T clipart Train Ep: 31100 lr0.0034643250455311855 	 Loss Classification: 0.061579 Loss T 0.020159 Method MME

S real T clipart Train Ep: 31200 lr0.0034580167132870383 	 Loss Classification: 0.169790 Loss T 0.019703 Method MME

S real T clipart Train Ep: 31300 lr0.00345173511936598 	 Loss Classification: 0.045623 Loss T 0.024776 Method MME

S real T clipart Train Ep: 31400 lr0.0034454800861046533 	 Loss Classification: 0.066936 Loss T 0.020294 Method MME

S real T clipart Train Ep: 31500 lr0.003439251437445577 	 Loss Classification: 0.043976 Loss T 0.025291 Method MME


Labeled Target set: Average loss: 0.0353, Accuracy: 1066/1080 F1 (98.7037%)


Test set: Average loss: 1.7785, Accuracy: 13540/18312 F1 (73.9406%)


Val set: Average loss: 1.7024, Accuracy: 269/360 F1 (74.7222%)

best acc test 73.902359  acc val 74.722222 acc labeled target 98.703704
saving model...
S real T clipart Train Ep: 31600 lr0.0034330489989188046 	 Loss Classification: 0.450341 Loss T 0.028674 Method MME

S real T clipart Train Ep: 31700 lr0.0034268725976238346 	 Loss Classification: 0.060368 Loss T 0.024798 Method MME

S real T clipart Train Ep: 31800 lr0.003420722062211772 	 Loss Classification: 0.088504 Loss T 0.016920 Method MME

S real T clipart Train Ep: 31900 lr0.0034145972228677326 	 Loss Classification: 0.246827 Loss T 0.016237 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.8888889 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.6666667 1.        1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.7777778 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        0.6666667 1.
 0.8888889 1.        1.        1.        1.        1.        1.
 1.        1.        0.8888889 1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [63, 103, 91, 47, 66, 34, 76, 114, 105, 85, 87, 81, 83, 88, 82, 65, 89, 90, 86, 84, 67, 78, 77, 75, 74, 73, 72, 71, 70, 69, 68, 80, 79, 0, 64, 123, 122, 121, 120, 119, 118, 117, 116, 115, 113, 112, 111, 110, 109, 108, 107, 106, 104, 102, 101, 100, 99, 98, 97, 96, 95, 94, 92, 93, 62, 61, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 28, 124, 29, 31, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 33, 32, 30, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
