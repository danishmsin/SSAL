Dataset multi Source painting Target real Labeled num perclass 3 Network resnet34
126 classes in this dataset
Unlabelled Target Dataset Size:  69980
Labelled Target Dataset Size:  378
Misc. Labelled Target Dataset Size:  378
Bank keys - Target:  dict_keys(['feat_vec', 'labels', 'names', 'domain_identifier']) Source:  dict_keys(['feat_vec', 'labels', 'names', 'domain_identifier'])
Num  - Target:  69980 Source:  31502
Unlabeled Target Data Size: 1457
S painting T real Train Ep: 0 lr0.01 	 Loss Classification: 4.981334 Loss T 0.470722 Method MME

S painting T real Train Ep: 100 lr0.009925650290240803 	 Loss Classification: 2.053492 Loss T 0.269516 Method MME

S painting T real Train Ep: 200 lr0.009852577760521605 	 Loss Classification: 2.079221 Loss T 0.195687 Method MME

S painting T real Train Ep: 300 lr0.009780748269686728 	 Loss Classification: 2.607080 Loss T 0.193292 Method MME

S painting T real Train Ep: 400 lr0.009710128909124701 	 Loss Classification: 1.449074 Loss T 0.173228 Method MME

S painting T real Train Ep: 500 lr0.00964068794694323 	 Loss Classification: 1.406410 Loss T 0.163866 Method MME


Labeled Target set: Average loss: 1.9815, Accuracy: 606/1080 F1 (56.1111%)


Test set: Average loss: 1.6431, Accuracy: 43095/69960 F1 (61.5995%)


Val set: Average loss: 1.7960, Accuracy: 214/360 F1 (59.4444%)

best acc test 61.599485  acc val 59.444444 acc labeled target 56.111111
saving model...
S painting T real Train Ep: 600 lr0.00957239477517603 	 Loss Classification: 1.070155 Loss T 0.143299 Method MME

S painting T real Train Ep: 700 lr0.009505219859830012 	 Loss Classification: 1.269010 Loss T 0.140091 Method MME

S painting T real Train Ep: 800 lr0.009439134693595126 	 Loss Classification: 1.234066 Loss T 0.102950 Method MME

S painting T real Train Ep: 900 lr0.009374111751051751 	 Loss Classification: 1.432914 Loss T 0.100749 Method MME

S painting T real Train Ep: 1000 lr0.009310124446222227 	 Loss Classification: 1.616262 Loss T 0.131703 Method MME


Labeled Target set: Average loss: 1.7924, Accuracy: 616/1080 F1 (57.0370%)


Test set: Average loss: 1.4684, Accuracy: 45886/69960 F1 (65.5889%)


Val set: Average loss: 1.5730, Accuracy: 231/360 F1 (64.1667%)

best acc test 65.588908  acc val 64.166667 acc labeled target 57.037037
saving model...
S painting T real Train Ep: 1100 lr0.00924714709232377 	 Loss Classification: 1.570498 Loss T 0.126390 Method MME

S painting T real Train Ep: 1200 lr0.009185154863590003 	 Loss Classification: 0.841509 Loss T 0.105968 Method MME

S painting T real Train Ep: 1300 lr0.00912412375903735 	 Loss Classification: 1.703897 Loss T 0.109441 Method MME

S painting T real Train Ep: 1400 lr0.009064030568061049 	 Loss Classification: 0.591026 Loss T 0.120930 Method MME

S painting T real Train Ep: 1500 lr0.009004852837753237 	 Loss Classification: 0.607985 Loss T 0.129130 Method MME


Labeled Target set: Average loss: 1.8997, Accuracy: 626/1080 F1 (57.9630%)


Test set: Average loss: 1.4139, Accuracy: 47505/69960 F1 (67.9031%)


Val set: Average loss: 1.5248, Accuracy: 229/360 F1 (63.6111%)

best acc test 65.588908  acc val 63.611111 acc labeled target 57.962963
saving model...
S painting T real Train Ep: 1600 lr0.008946568841842816 	 Loss Classification: 1.426234 Loss T 0.082676 Method MME

S painting T real Train Ep: 1700 lr0.008889157551163433 	 Loss Classification: 1.360543 Loss T 0.111162 Method MME

S painting T real Train Ep: 1800 lr0.008832598605562044 	 Loss Classification: 1.058289 Loss T 0.094556 Method MME

S painting T real Train Ep: 1900 lr0.008776872287166303 	 Loss Classification: 0.973121 Loss T 0.083608 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.         0.6666667  1.         0.11111111 0.6666667  0.
 0.6666667  0.         0.6666667  0.5        0.8888889  0.6666667
 0.         0.         0.6666667  0.6666667  0.22222222 1.
 0.11111111 0.6666667  0.11111111 0.6666667  1.         0.33333334
 0.         0.7777778  0.5555556  0.6666667  0.8888889  0.8888889
 1.         0.         0.44444445 0.22222222 0.44444445 1.
 0.         0.5555556  0.33333334 1.         0.6666667  1.
 0.         0.5555556  1.         0.7777778  0.8888889  0.7777778
 0.         0.8888889  0.         0.33333334 0.6666667  0.8888889
 0.         0.44444445 0.8888889  1.         0.7777778  0.5
 0.7777778  0.33333334 1.         0.         1.         1.
 0.         1.         0.8333333  1.         1.         0.44444445
 0.7777778  0.6666667  0.11111111 0.5555556  0.6666667  0.33333334
 0.         1.         0.         0.7777778  0.11111111 0.
 0.         1.         1.         0.33333334 0.         0.44444445
 0.         1.         0.6666667  0.8333333  0.8888889  0.7777778
 0.11111111 1.         0.11111111 0.6666667  0.8888889  0.6666667
 0.7777778  0.44444445 0.         0.6666667  0.7777778  0.8888889
 0.         0.5        0.6666667  0.33333334 1.         0.8888889
 0.33333334 1.         0.44444445 1.         0.         1.
 0.5555556  0.8888889  0.5555556  0.7777778  0.5555556  1.        ]
Top k classes which perform poorly are:  [104, 63, 66, 54, 50, 48, 42, 118, 36, 78, 80, 83, 24, 84, 108, 31, 7, 5, 13, 12, 90, 88, 3, 18, 98, 20, 74, 82, 96, 33, 16, 61, 114, 111, 51, 87, 77, 38, 23, 32, 55, 116, 103, 34, 89, 71, 9, 59, 109, 122, 26, 120, 75, 43, 37, 124, 73, 110, 105, 99, 76, 92, 101, 52, 40, 15, 19, 21, 27, 14, 8, 11, 4, 1, 6, 72, 102, 106, 60, 25, 81, 123, 95, 45, 47, 58, 93, 68, 94, 100, 10, 121, 28, 29, 46, 49, 53, 56, 113, 107, 119, 117, 112, 115, 0, 62, 91, 2, 17, 22, 30, 35, 39, 41, 44, 57, 64, 65, 67, 69, 70, 79, 85, 86, 97, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [124 175 128   8 262  10 191 112  24 369 179 200   3 127 128 174 405 459
  48 154   3 150 270 237  18 396 233 409 513  24 203  20  28  22  97  82
   3 144 175 109 448 547 235  48 489 243 326 547   8 220   8 174 258 131
  11   9 459 229  44 210 423   5 407   4 388 216  25 495 184 341 284 248
 135 347  14  29 914 261 203 323   4 182  13   6 382 227 204 261  21 154
   9 304 414 362 285 387  13 442  41 419 327   9 227 192 267 464 309 168
  15 338 220   4 437 298  14 373 169 496   3 443 147 204  40 312 127 534]
CBFL per class weights: tensor([0.3273, 0.2817, 0.3222, 3.0182, 0.2512, 2.4386, 0.2732, 0.3452, 1.0880,
        0.2390, 0.2794, 0.2692, 7.8507, 0.3234, 0.3222, 0.2823, 0.2372, 0.2355,
        0.6093, 0.2962, 7.8507, 0.2995, 0.2497, 0.2569, 1.4090, 0.2376, 0.2580,
        0.2371, 0.2345, 1.0880, 0.2680, 1.2805, 0.9506, 1.1754, 0.3744, 0.4154,
        7.8507, 0.3049, 0.2817, 0.3503, 0.2358, 0.2341, 0.2574, 0.6093, 0.2349,
        0.2554, 0.2423, 0.2341, 3.0182, 0.2619, 3.0182, 0.2823, 0.2520, 0.3186,
        2.2279, 2.6962, 0.2355, 0.2591, 0.6524, 0.2653, 0.2365, 4.7577, 0.2371,
        5.9175, 0.2380, 0.2632, 1.0495, 0.2348, 0.2767, 0.2410, 0.2474, 0.2542,
        0.3140, 0.2405, 1.7765, 0.9223, 0.2332, 0.2514, 0.2680, 0.2426, 5.9175,
        0.2778, 1.9038, 3.9845, 0.2383, 0.2597, 0.2676, 0.2514, 1.2255, 0.2962,
        2.6962, 0.2447, 0.2369, 0.2395, 0.2473, 0.2380, 1.9038, 0.2359, 0.6904,
        0.2367, 0.2422, 2.6962, 0.2597, 0.2728, 0.2503, 0.2354, 0.2441, 0.2860,
        1.6662, 0.2412, 0.2619, 5.9175, 0.2361, 0.2455, 1.7765, 0.2388, 0.2854,
        0.2348, 7.8507, 0.2359, 0.3021, 0.2676, 0.7044, 0.2438, 0.3234, 0.2343],
       device='cuda:0')
S painting T real Train Ep: 2000 lr0.008721959494934213 	 Loss Classification: 0.276379 Loss T 0.066281 Method MME


Labeled Target set: Average loss: 2.0368, Accuracy: 630/1080 F1 (58.3333%)


Test set: Average loss: 1.7180, Accuracy: 44038/69960 F1 (62.9474%)


Val set: Average loss: 1.9465, Accuracy: 221/360 F1 (61.3889%)

best acc test 65.588908  acc val 61.388889 acc labeled target 58.333333
saving model...
S painting T real Train Ep: 2100 lr0.008667841720414475 	 Loss Classification: 1.809254 Loss T 0.077996 Method MME

S painting T real Train Ep: 2200 lr0.008614501024650454 	 Loss Classification: 1.134741 Loss T 0.134130 Method MME

S painting T real Train Ep: 2300 lr0.008561920016164943 	 Loss Classification: 1.179729 Loss T 0.104001 Method MME

S painting T real Train Ep: 2400 lr0.008510081829966844 	 Loss Classification: 1.271719 Loss T 0.120240 Method MME

S painting T real Train Ep: 2500 lr0.008458970107524513 	 Loss Classification: 1.414818 Loss T 0.168082 Method MME


Labeled Target set: Average loss: 2.4523, Accuracy: 478/1080 F1 (44.2593%)


Test set: Average loss: 2.1131, Accuracy: 36535/69960 F1 (52.2227%)


Val set: Average loss: 2.2137, Accuracy: 176/360 F1 (48.8889%)

best acc test 65.588908  acc val 48.888889 acc labeled target 44.259259
saving model...
S painting T real Train Ep: 2600 lr0.008408568977653933 	 Loss Classification: 3.458275 Loss T 0.154110 Method MME

S painting T real Train Ep: 2700 lr0.00835886303827305 	 Loss Classification: 1.666345 Loss T 0.018377 Method MME

S painting T real Train Ep: 2800 lr0.008309837338976545 	 Loss Classification: 3.063505 Loss T 0.021016 Method MME

S painting T real Train Ep: 2900 lr0.008261477364388068 	 Loss Classification: 2.080420 Loss T 0.041208 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.         0.         0.5555556  1.         0.         0.44444445
 0.         0.         0.         0.6666667  0.11111111 0.6666667
 0.         0.5        0.33333334 0.         0.22222222 1.
 0.16666667 0.22222222 0.         0.6666667  1.         0.33333334
 0.         0.33333334 0.6666667  0.5        0.33333334 0.7777778
 0.22222222 0.5555556  0.         0.33333334 0.33333334 0.8888889
 0.         0.5        0.33333334 1.         0.6666667  1.
 0.44444445 0.5555556  0.6666667  1.         0.         0.6666667
 0.44444445 1.         0.33333334 0.22222222 0.33333334 0.44444445
 0.22222222 1.         0.33333334 0.7777778  0.         1.
 0.7777778  0.         0.         0.         0.8888889  0.22222222
 0.         0.22222222 0.6666667  1.         0.44444445 0.22222222
 0.         0.33333334 0.6666667  1.         0.5555556  0.33333334
 0.         1.         0.11111111 0.6666667  0.5555556  0.
 0.         0.8888889  0.11111111 0.33333334 0.         0.6666667
 0.22222222 0.44444445 0.6666667  0.6666667  0.33333334 0.5
 0.11111111 0.6666667  0.33333334 0.7777778  0.33333334 0.6666667
 0.7777778  0.33333334 0.11111111 0.6666667  0.6666667  0.5
 0.44444445 0.44444445 0.7777778  0.16666667 1.         0.8888889
 0.         0.8888889  0.11111111 0.6666667  0.22222222 0.7777778
 0.         0.11111111 0.33333334 0.22222222 0.7777778  0.6666667 ]
Top k classes which perform poorly are:  [62, 32, 36, 114, 63, 72, 24, 61, 78, 58, 20, 46, 15, 120, 6, 1, 88, 4, 84, 7, 8, 66, 12, 83, 10, 86, 80, 96, 116, 121, 104, 18, 111, 54, 65, 51, 30, 71, 118, 19, 16, 90, 123, 67, 94, 52, 77, 98, 100, 56, 73, 87, 103, 14, 25, 28, 50, 33, 34, 38, 23, 122, 108, 109, 91, 42, 5, 70, 48, 53, 107, 37, 13, 27, 95, 2, 82, 43, 31, 76, 117, 92, 89, 97, 106, 105, 101, 93, 125, 81, 9, 74, 11, 68, 26, 40, 44, 47, 21, 110, 102, 119, 57, 99, 60, 124, 29, 85, 64, 35, 115, 113, 17, 22, 3, 41, 39, 45, 49, 55, 59, 69, 75, 79, 112, 0]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 123  150  121   58  252   56  185  102   24  321  177  183 4314  116
  133  161  337  396   48  139 1646  149  231  195   19  344  205  345
  433   33  189   29   26   25   89   78 3993  133  162  103  381  469
  213   55  430  227  277  501  204  201   84  155  219  124   39   29
  402  189   43  183  375  587  360   31  327  184   34  416  173  298
  243  217  122  316   57   43  790  247  187  279  272  168   39  109
  347  210  180  227   26  138   14  263  355  307  241  347   36  381
   41  368  291   27  219  178  242  423  275  152   32  295  207 1976
  352  271   19  319  160  429  111  396  132  188   56  283  125  445]
CBFL per class weights: tensor([0.8365, 0.7623, 0.8435, 1.3435, 0.6447, 1.3789, 0.7030, 0.9255, 2.7691,
        0.6180, 0.7140, 0.7056, 0.5935, 0.8622, 0.8050, 0.7403, 0.6143, 0.6048,
        1.5507, 0.7885, 0.5935, 0.7645, 0.6580, 0.6908, 3.4141, 0.6128, 0.6801,
        0.6126, 0.6012, 2.1025, 0.6979, 2.3474, 2.5809, 2.6712, 1.0039, 1.0922,
        0.5935, 0.8050, 0.7384, 0.9204, 0.6067, 0.5989, 0.6726, 1.3976, 0.6015,
        0.6610, 0.6326, 0.5974, 0.6811, 0.6842, 1.0410, 0.7518, 0.6674, 0.8331,
        1.8302, 2.3474, 0.6041, 0.6979, 1.6913, 0.7056, 0.6075, 0.5951, 0.6098,
        2.2170, 0.6165, 0.7043, 2.0504, 0.6027, 0.7200, 0.6247, 0.6500, 0.6690,
        0.8399, 0.6193, 1.3609, 1.6913, 0.5937, 0.6476, 0.7004, 0.6317, 0.6347,
        0.7280, 1.8302, 0.8916, 0.6122, 0.6753, 0.7097, 0.6610, 2.5809, 0.7911,
        4.5217, 0.6389, 0.6107, 0.6219, 0.6513, 0.6122, 1.9549, 0.6067, 1.7573,
        0.6086, 0.6272, 2.4972, 0.6674, 0.7126, 0.6506, 0.6021, 0.6334, 0.7580,
        2.1580, 0.6258, 0.6782, 0.5935, 0.6113, 0.6352, 3.4141, 0.6185, 0.7421,
        0.6016, 0.8828, 0.6048, 0.8079, 0.6992, 1.3789, 0.6301, 0.8297, 0.6003],
       device='cuda:0')
S painting T real Train Ep: 3000 lr0.008213769018249545 	 Loss Classification: 1.212461 Loss T 0.121623 Method MME


Labeled Target set: Average loss: 3.5275, Accuracy: 302/1080 F1 (27.9630%)


Test set: Average loss: 3.2613, Accuracy: 21762/69960 F1 (31.1063%)


Val set: Average loss: 3.4315, Accuracy: 106/360 F1 (29.4444%)

best acc test 65.588908  acc val 29.444444 acc labeled target 27.962963
saving model...
S painting T real Train Ep: 3100 lr0.008166698608209509 	 Loss Classification: 0.885642 Loss T 0.102637 Method MME

S painting T real Train Ep: 3200 lr0.008120252831274708 	 Loss Classification: 0.741490 Loss T 0.113706 Method MME

S painting T real Train Ep: 3300 lr0.008074418759891278 	 Loss Classification: 0.565161 Loss T 0.101654 Method MME

S painting T real Train Ep: 3400 lr0.008029183828623731 	 Loss Classification: 1.323844 Loss T 0.114161 Method MME

S painting T real Train Ep: 3500 lr0.007984535821401871 	 Loss Classification: 0.604472 Loss T 0.092772 Method MME


Labeled Target set: Average loss: 2.0304, Accuracy: 624/1080 F1 (57.7778%)


Test set: Average loss: 1.6501, Accuracy: 45503/69960 F1 (65.0415%)


Val set: Average loss: 1.7810, Accuracy: 233/360 F1 (64.7222%)

best acc test 65.041452  acc val 64.722222 acc labeled target 57.777778
saving model...
S painting T real Train Ep: 3600 lr0.007940462859307384 	 Loss Classification: 1.845880 Loss T 0.106764 Method MME

S painting T real Train Ep: 3700 lr0.007896953388873518 	 Loss Classification: 1.331108 Loss T 0.099762 Method MME

S painting T real Train Ep: 3800 lr0.007853996170872714 	 Loss Classification: 0.341376 Loss T 0.096570 Method MME

S painting T real Train Ep: 3900 lr0.00781158026956848 	 Loss Classification: 0.484756 Loss T 0.090173 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.         0.33333334 0.6666667  1.         0.         0.7777778
 0.22222222 0.33333334 0.         0.6666667  0.6666667  0.6666667
 0.         0.33333334 0.6666667  0.11111111 0.5555556  1.
 0.         1.         0.         0.33333334 0.         0.5
 0.6666667  1.         0.33333334 0.6666667  0.6666667  1.
 0.5555556  0.5555556  1.         0.11111111 0.5555556  0.6666667
 0.         1.         0.33333334 1.         0.5        1.
 0.16666667 1.         0.8888889  1.         0.6666667  0.8333333
 0.         1.         0.6666667  0.6666667  0.44444445 0.33333334
 0.         0.7777778  0.6666667  0.7777778  0.6666667  0.6666667
 0.8888889  0.         1.         1.         1.         1.
 0.         0.5        1.         0.8888889  1.         0.
 0.44444445 0.44444445 0.6666667  0.8333333  0.11111111 0.33333334
 0.         1.         0.         1.         0.8888889  0.
 0.         1.         0.33333334 0.33333334 0.16666667 0.33333334
 0.11111111 1.         0.6666667  0.7777778  0.6666667  0.6666667
 0.44444445 0.6666667  0.6666667  0.7777778  0.6666667  0.6666667
 0.44444445 0.44444445 0.11111111 1.         0.6666667  0.44444445
 0.44444445 0.5555556  0.6666667  0.         1.         1.
 0.5        0.8888889  0.7777778  0.8888889  0.         1.
 0.22222222 0.7777778  0.6666667  0.22222222 0.7777778  1.        ]
Top k classes which perform poorly are:  [111, 83, 78, 84, 36, 118, 48, 22, 20, 18, 54, 12, 80, 4, 66, 61, 71, 8, 15, 33, 104, 90, 76, 88, 42, 120, 123, 6, 77, 38, 1, 86, 13, 89, 26, 7, 53, 87, 21, 73, 52, 108, 107, 72, 102, 96, 103, 40, 67, 114, 23, 16, 31, 30, 109, 34, 92, 94, 95, 97, 98, 100, 74, 106, 51, 46, 11, 10, 9, 24, 35, 14, 101, 50, 56, 28, 27, 58, 59, 2, 122, 110, 93, 116, 124, 5, 55, 57, 99, 121, 75, 47, 115, 117, 82, 44, 69, 60, 105, 113, 112, 119, 0, 62, 85, 3, 17, 19, 25, 29, 32, 37, 39, 41, 43, 45, 49, 63, 64, 65, 68, 70, 79, 81, 91, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 174  175  145   99  209   68  192  109  123  386  200  239 3304  172
  184  175  307  431  100  190 1293  186  212  194  451  369  219  363
  505   82  206  346  219   49  134  118 3030  176  193  155  411  533
  250  115  456  251  274  552  189  226  114  202  257  139  190   84
  469  248  167  210  398  584  405  151  352  250   79  329  208  336
  249  242  131  368  117  126  695  253  206  319  227  182   59  101
  401  259  178  265  292  153 2067  300  409  398  240  370  291  435
   78  442  279  110  273  200  280  481  288  160  703  316  249 1751
  404  310 1265  368  198  446   93  396  135  200   72  308  172  450]
CBFL per class weights: tensor([1.0135, 1.0114, 1.0913, 1.3282, 0.9539, 1.6908, 0.9793, 1.2577, 1.1799,
        0.8548, 0.9667, 0.9205, 0.8371, 1.0178, 0.9935, 1.0114, 0.8772, 0.8483,
        1.3205, 0.9827, 0.8372, 0.9898, 0.9500, 0.9760, 0.8462, 0.8582, 0.9413,
        0.8595, 0.8424, 1.4912, 0.9580, 0.8638, 0.9413, 2.1527, 1.1314, 1.2053,
        0.8371, 1.0093, 0.9777, 1.0605, 0.8508, 0.8411, 0.9110, 1.2218, 0.8458,
        0.9102, 0.8941, 0.8404, 0.9845, 0.9335, 1.2275, 0.9637, 0.9056, 1.1123,
        0.9827, 1.4684, 0.8447, 0.9126, 1.0293, 0.9526, 0.8528, 0.8395, 0.8517,
        1.0722, 0.8622, 0.9110, 1.5278, 0.8690, 0.9552, 0.8668, 0.9118, 0.9178,
        1.1437, 0.8584, 1.2107, 1.1657, 0.8379, 0.9086, 0.9580, 0.8725, 0.9324,
        0.9973, 1.8715, 1.3129, 0.8523, 0.9041, 1.0051, 0.8999, 0.8841, 1.0663,
        0.8371, 0.8803, 0.8511, 0.8528, 0.9196, 0.8580, 0.8846, 0.8479, 1.5406,
        0.8471, 0.8911, 1.2514, 0.8947, 0.9667, 0.8905, 0.8439, 0.8862, 1.0468,
        0.8379, 0.8736, 0.9118, 0.8371, 0.8518, 0.8760, 0.8372, 0.8584, 0.9697,
        0.8467, 1.3785, 0.8531, 1.1274, 0.9667, 1.6255, 0.8768, 1.0178, 0.8463],
       device='cuda:0')
S painting T real Train Ep: 4000 lr0.007769695042409123 	 Loss Classification: 0.644747 Loss T 0.092748 Method MME


Labeled Target set: Average loss: 2.1758, Accuracy: 613/1080 F1 (56.7593%)


Test set: Average loss: 1.9724, Accuracy: 41990/69960 F1 (60.0200%)


Val set: Average loss: 2.2343, Accuracy: 201/360 F1 (55.8333%)

best acc test 65.041452  acc val 55.833333 acc labeled target 56.759259
saving model...
S painting T real Train Ep: 4100 lr0.007728330130142108 	 Loss Classification: 1.086872 Loss T 0.085646 Method MME

S painting T real Train Ep: 4200 lr0.007687475447329114 	 Loss Classification: 1.814531 Loss T 0.086644 Method MME

S painting T real Train Ep: 4300 lr0.0076471211732427845 	 Loss Classification: 0.893719 Loss T 0.081065 Method MME

S painting T real Train Ep: 4400 lr0.007607257743127307 	 Loss Classification: 0.931581 Loss T 0.069177 Method MME

S painting T real Train Ep: 4500 lr0.007567875839805858 	 Loss Classification: 0.753271 Loss T 0.079776 Method MME


Labeled Target set: Average loss: 1.6359, Accuracy: 686/1080 F1 (63.5185%)


Test set: Average loss: 1.3146, Accuracy: 49734/69960 F1 (71.0892%)


Val set: Average loss: 1.4433, Accuracy: 241/360 F1 (66.9444%)

best acc test 71.089194  acc val 66.944444 acc labeled target 63.518519
saving model...
S painting T real Train Ep: 4600 lr0.007528966385618854 	 Loss Classification: 0.902556 Loss T 0.070110 Method MME

S painting T real Train Ep: 4700 lr0.007490520534677821 	 Loss Classification: 1.185403 Loss T 0.068760 Method MME

S painting T real Train Ep: 4800 lr0.007452529665420465 	 Loss Classification: 1.236733 Loss T 0.078655 Method MME

S painting T real Train Ep: 4900 lr0.007414985373453289 	 Loss Classification: 1.046633 Loss T 0.063965 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.         0.22222222 0.7777778  0.8888889  0.         1.
 0.8888889  0.8888889  0.6666667  0.5        0.6666667  0.6666667
 0.         0.5        0.6666667  0.         0.6666667  1.
 0.22222222 1.         0.         0.5555556  1.         0.33333334
 0.         1.         0.6666667  0.6666667  0.8888889  1.
 0.7777778  0.44444445 1.         0.6666667  0.33333334 0.6666667
 0.         1.         0.33333334 1.         0.6666667  1.
 0.44444445 0.6666667  1.         1.         0.7777778  0.8888889
 0.6666667  1.         0.6666667  0.6666667  0.33333334 0.7777778
 0.16666667 1.         0.6666667  1.         0.7777778  0.33333334
 1.         0.         1.         0.44444445 1.         0.5555556
 0.         1.         0.7777778  1.         0.8888889  0.5
 0.5555556  0.33333334 0.44444445 1.         0.6666667  0.44444445
 0.         1.         0.         1.         0.8888889  0.
 0.         1.         0.8888889  0.33333334 0.33333334 0.33333334
 0.22222222 1.         0.6666667  0.6666667  0.7777778  1.
 0.16666667 1.         0.6666667  0.7777778  0.5        0.5
 1.         0.22222222 0.33333334 0.7777778  0.5        0.6666667
 0.         0.6666667  0.6666667  0.         1.         1.
 0.33333334 1.         0.7777778  1.         0.         1.
 0.6666667  1.         0.6666667  1.         0.5555556  1.        ]
Top k classes which perform poorly are:  [66, 24, 78, 36, 20, 80, 83, 15, 84, 12, 118, 61, 108, 4, 111, 96, 54, 90, 18, 103, 1, 38, 73, 59, 52, 34, 114, 104, 87, 89, 88, 23, 63, 42, 74, 77, 31, 106, 100, 13, 101, 9, 71, 124, 65, 21, 72, 110, 107, 76, 92, 109, 56, 93, 48, 98, 51, 8, 10, 11, 14, 16, 122, 120, 27, 33, 35, 26, 50, 43, 40, 68, 116, 58, 53, 30, 2, 94, 99, 105, 46, 86, 6, 7, 3, 82, 28, 70, 47, 112, 102, 113, 121, 119, 123, 117, 115, 0, 62, 95, 5, 17, 19, 22, 25, 29, 32, 37, 39, 41, 44, 97, 45, 55, 57, 60, 64, 67, 69, 75, 79, 81, 85, 91, 49, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 240  214  202  171  206  293  230  214  254  431  264  296 2374  227
  237  209  431  484  165  274  964  247  283  280  387  440  284  460
  584  113  271  643  361  138  196  146 2229  267  231  240  475  588
  335  161  566  301  355  618  326  297  326  240  300  168  239  158
  530  346  262  293  473  475  524  257  413  341  100  428  241  375
  311  307  201  429  192  150  760  293  249  360  188  221  746   89
  482  305  290  306  307  196 1689  364  471  502  302  410  263  489
  115  520  357  179  330  282  343  546  323  216  549  363  284 1379
  473  368 1100  396  280  492  112  462  241  230  170  366  239  523]
CBFL per class weights: tensor([1.0109, 1.0416, 1.0595, 1.1214, 1.0532, 0.9714, 1.0216, 1.0416, 0.9980,
        0.9326, 0.9900, 0.9698, 0.9203, 1.0250, 1.0140, 1.0487, 0.9326, 0.9275,
        1.1369, 0.9829, 0.9204, 1.0042, 0.9772, 0.9790, 0.9396, 0.9315, 0.9766,
        0.9295, 0.9229, 1.3558, 0.9850, 0.9218, 0.9454, 1.2268, 1.0695, 1.1961,
        0.9203, 0.9878, 1.0205, 1.0109, 0.9282, 0.9228, 0.9532, 1.1479, 0.9235,
        0.9673, 0.9471, 0.9222, 0.9564, 0.9693, 0.9564, 1.0109, 0.9678, 1.1290,
        1.0119, 1.1567, 0.9248, 0.9497, 0.9916, 0.9714, 0.9283, 0.9282, 0.9251,
        0.9955, 0.9351, 0.9512, 1.4517, 0.9330, 1.0099, 0.9421, 0.9626, 0.9644,
        1.0611, 0.9328, 1.0767, 1.1821, 0.9208, 0.9714, 1.0024, 0.9457, 1.0842,
        1.0323, 0.9208, 1.5568, 0.9276, 0.9654, 0.9731, 0.9649, 0.9644, 1.0695,
        0.9203, 0.9447, 0.9285, 0.9263, 0.9668, 0.9355, 0.9908, 0.9271, 1.3432,
        0.9253, 0.9465, 1.1028, 0.9550, 0.9778, 0.9506, 0.9242, 0.9576, 1.0388,
        0.9240, 0.9449, 0.9766, 0.9203, 0.9283, 0.9437, 0.9203, 0.9379, 0.9790,
        0.9269, 1.3623, 0.9293, 1.0099, 1.0216, 1.1239, 0.9442, 1.0119, 0.9252],
       device='cuda:0')
S painting T real Train Ep: 5000 lr0.007377879464668811 	 Loss Classification: 0.821373 Loss T 0.095396 Method MME


Labeled Target set: Average loss: 2.1155, Accuracy: 643/1080 F1 (59.5370%)


Test set: Average loss: 1.7950, Accuracy: 44671/69960 F1 (63.8522%)


Val set: Average loss: 1.9998, Accuracy: 221/360 F1 (61.3889%)

best acc test 71.089194  acc val 61.388889 acc labeled target 59.537037
saving model...
S painting T real Train Ep: 5100 lr0.007341203948625087 	 Loss Classification: 0.636551 Loss T 0.078788 Method MME

S painting T real Train Ep: 5200 lr0.007304951032175895 	 Loss Classification: 0.367929 Loss T 0.068439 Method MME

S painting T real Train Ep: 5300 lr0.007269113113340497 	 Loss Classification: 0.491888 Loss T 0.083052 Method MME

S painting T real Train Ep: 5400 lr0.007233682775402483 	 Loss Classification: 0.767435 Loss T 0.091364 Method MME

S painting T real Train Ep: 5500 lr0.007198652781227704 	 Loss Classification: 1.035604 Loss T 0.086390 Method MME


Labeled Target set: Average loss: 1.6129, Accuracy: 704/1080 F1 (65.1852%)


Test set: Average loss: 1.2621, Accuracy: 50489/69960 F1 (72.1684%)


Val set: Average loss: 1.4036, Accuracy: 254/360 F1 (70.5556%)

best acc test 72.168382  acc val 70.555556 acc labeled target 65.185185
saving model...
S painting T real Train Ep: 5600 lr0.007164016067791809 	 Loss Classification: 0.374392 Loss T 0.062625 Method MME

S painting T real Train Ep: 5700 lr0.0071297657409083726 	 Loss Classification: 0.469441 Loss T 0.065688 Method MME

S painting T real Train Ep: 5800 lr0.0070958950701490225 	 Loss Classification: 0.938920 Loss T 0.077142 Method MME

S painting T real Train Ep: 5900 lr0.007062397483947426 	 Loss Classification: 0.409070 Loss T 0.062699 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.         0.11111111 0.8888889  1.         0.11111111 1.
 0.44444445 0.5555556  0.6666667  0.7777778  0.7777778  0.6666667
 0.         0.22222222 0.6666667  0.33333334 0.5555556  1.
 0.11111111 0.7777778  0.11111111 0.44444445 1.         0.22222222
 0.22222222 0.7777778  0.6666667  0.6666667  0.6666667  1.
 0.5555556  0.5555556  1.         0.6666667  0.33333334 0.6666667
 0.         1.         0.33333334 1.         0.6666667  0.8888889
 0.44444445 0.8333333  0.8888889  1.         0.8888889  1.
 0.8888889  1.         0.6666667  0.5555556  0.33333334 0.7777778
 0.11111111 1.         0.6666667  1.         1.         0.6666667
 1.         0.         1.         0.7777778  1.         0.8888889
 0.         1.         0.7777778  0.7777778  1.         0.33333334
 0.5555556  0.33333334 0.8888889  1.         0.6666667  0.33333334
 0.         0.8888889  0.33333334 0.6666667  1.         0.
 0.         1.         1.         0.33333334 0.11111111 0.5555556
 1.         0.7777778  0.5        0.7777778  0.6666667  1.
 0.33333334 1.         1.         0.8888889  1.         0.6666667
 1.         0.16666667 0.22222222 0.8888889  0.6666667  0.7777778
 0.         0.6666667  0.8888889  0.         0.8888889  0.5
 0.6666667  1.         1.         1.         0.         1.
 0.5555556  0.7777778  0.5        0.22222222 0.6666667  1.        ]
Top k classes which perform poorly are:  [118, 84, 83, 66, 36, 78, 108, 111, 12, 61, 20, 18, 54, 88, 4, 1, 103, 13, 104, 23, 24, 123, 87, 38, 52, 96, 34, 71, 80, 73, 77, 15, 6, 21, 42, 122, 113, 92, 30, 7, 89, 51, 72, 16, 120, 31, 124, 59, 56, 106, 109, 33, 114, 101, 40, 35, 28, 27, 26, 81, 14, 11, 76, 8, 94, 50, 69, 68, 91, 63, 53, 93, 25, 19, 121, 10, 9, 107, 43, 48, 99, 105, 2, 41, 79, 46, 44, 112, 110, 65, 74, 115, 116, 117, 119, 100, 102, 0, 62, 97, 3, 5, 17, 22, 29, 32, 37, 39, 45, 47, 49, 98, 55, 58, 60, 64, 67, 70, 75, 82, 85, 86, 90, 95, 57, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 292  238  256  223  259  380  258  279  342  466  312  339 1783  262
  303  260  519  539  216  348  688  303  339  316  277  509  327  534
  640  147  330  785  457  191  257  173 1622  313  288  328  530  646
  411  200  665  335  442  669  535  331  496  268  341  214  214  205
  579  419  330  393  530  299  637  326  479  412  115  540  285  411
  372  356  300  490  263  177  844  357  279  390  159  252 1011   79
  524  349  416  340  305  255 1202  429  520  570  372  448  271  537
  171  589  461  202  390  364  391  591  363  286  459  422  316  983
  528  400  831  439  371  530  242  514  361  263  248  401  305  591]
CBFL per class weights: tensor([1.0014, 1.0436, 1.0265, 1.0610, 1.0240, 0.9694, 1.0248, 1.0093, 0.9796,
        0.9570, 0.9912, 0.9806, 0.9481, 1.0215, 0.9955, 1.0231, 0.9533, 0.9524,
        1.0702, 0.9777, 0.9491, 0.9955, 0.9806, 0.9895, 1.0106, 0.9539, 0.9850,
        0.9526, 0.9497, 1.2285, 0.9838, 0.9485, 0.9578, 1.1111, 1.0256, 1.1503,
        0.9481, 0.9908, 1.0037, 0.9846, 0.9528, 0.9496, 0.9636, 1.0948, 0.9493,
        0.9820, 0.9594, 0.9493, 0.9525, 0.9835, 0.9547, 1.0169, 0.9800, 1.0730,
        1.0730, 1.0866, 0.9510, 0.9624, 0.9838, 0.9668, 0.9528, 0.9976, 0.9497,
        0.9854, 0.9559, 0.9635, 1.3838, 0.9523, 1.0055, 0.9636, 0.9712, 0.9754,
        0.9970, 0.9551, 1.0207, 1.1407, 0.9483, 0.9751, 1.0093, 0.9673, 1.1886,
        1.0300, 0.9482, 1.7303, 0.9531, 0.9774, 0.9629, 0.9803, 0.9945, 1.0273,
        0.9481, 0.9610, 0.9533, 0.9512, 0.9712, 0.9588, 1.0147, 0.9525, 1.1553,
        0.9507, 0.9575, 1.0915, 0.9673, 0.9732, 0.9671, 0.9506, 0.9735, 1.0049,
        0.9576, 0.9620, 0.9895, 0.9482, 0.9529, 0.9655, 0.9484, 0.9598, 0.9715,
        0.9528, 1.0395, 0.9536, 0.9740, 1.0207, 1.0336, 0.9653, 0.9945, 0.9506],
       device='cuda:0')
S painting T real Train Ep: 6000 lr0.007029266564879363 	 Loss Classification: 0.534151 Loss T 0.067839 Method MME


Labeled Target set: Average loss: 1.8079, Accuracy: 682/1080 F1 (63.1481%)


Test set: Average loss: 1.4950, Accuracy: 47666/69960 F1 (68.1332%)


Val set: Average loss: 1.6495, Accuracy: 233/360 F1 (64.7222%)

best acc test 72.168382  acc val 64.722222 acc labeled target 63.148148
saving model...
S painting T real Train Ep: 6100 lr0.006996496045111504 	 Loss Classification: 0.127776 Loss T 0.083772 Method MME

S painting T real Train Ep: 6200 lr0.00696407980201184 	 Loss Classification: 0.371585 Loss T 0.082062 Method MME

S painting T real Train Ep: 6300 lr0.006932011853915101 	 Loss Classification: 0.388202 Loss T 0.091684 Method MME

S painting T real Train Ep: 6400 lr0.006900286356036734 	 Loss Classification: 0.590432 Loss T 0.063291 Method MME

S painting T real Train Ep: 6500 lr0.006868897596529406 	 Loss Classification: 0.875474 Loss T 0.070572 Method MME


Labeled Target set: Average loss: 1.5880, Accuracy: 722/1080 F1 (66.8519%)


Test set: Average loss: 1.2550, Accuracy: 51155/69960 F1 (73.1204%)


Val set: Average loss: 1.4804, Accuracy: 245/360 F1 (68.0556%)

best acc test 72.168382  acc val 68.055556 acc labeled target 66.851852
saving model...
S painting T real Train Ep: 6600 lr0.006837839992676177 	 Loss Classification: 0.212553 Loss T 0.069712 Method MME

S painting T real Train Ep: 6700 lr0.006807108087214876 	 Loss Classification: 0.433131 Loss T 0.057238 Method MME

S painting T real Train Ep: 6800 lr0.006776696544788352 	 Loss Classification: 0.369540 Loss T 0.084501 Method MME

S painting T real Train Ep: 6900 lr0.006746600148515609 	 Loss Classification: 0.555433 Loss T 0.050972 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.         0.6666667  0.6666667  1.         0.         1.
 0.8888889  1.         0.8888889  0.7777778  0.8888889  0.6666667
 0.         0.33333334 0.6666667  0.33333334 0.33333334 0.8888889
 0.6666667  1.         0.11111111 0.33333334 1.         0.33333334
 0.11111111 0.6666667  0.6666667  0.6666667  0.7777778  1.
 0.6666667  0.44444445 1.         0.33333334 0.33333334 0.6666667
 0.         0.7777778  0.         1.         0.6666667  0.8888889
 0.33333334 0.6666667  1.         0.8888889  0.7777778  1.
 0.7777778  1.         0.7777778  0.5        0.33333334 0.44444445
 0.11111111 1.         0.7777778  1.         0.5        0.8888889
 1.         0.         1.         0.8888889  1.         1.
 0.         1.         0.8888889  1.         1.         0.44444445
 0.5555556  0.44444445 0.6666667  0.8888889  0.6666667  0.44444445
 0.         1.         0.11111111 0.6666667  0.5555556  0.
 0.22222222 1.         1.         0.33333334 0.         0.5555556
 0.11111111 0.7777778  0.6666667  0.8888889  0.8888889  1.
 0.6666667  0.8888889  0.5555556  0.6666667  1.         0.6666667
 1.         0.33333334 0.33333334 1.         0.8888889  1.
 0.6666667  0.6666667  0.7777778  0.         1.         1.
 0.33333334 1.         0.6666667  1.         0.22222222 1.
 0.6666667  1.         0.5555556  0.6666667  0.8888889  1.        ]
Top k classes which perform poorly are:  [88, 66, 61, 4, 78, 36, 83, 12, 111, 38, 80, 24, 54, 90, 20, 84, 118, 42, 52, 34, 33, 23, 114, 15, 21, 87, 103, 16, 13, 104, 71, 53, 31, 77, 73, 51, 58, 122, 82, 98, 72, 89, 109, 108, 76, 74, 92, 81, 99, 14, 30, 101, 11, 35, 26, 25, 116, 96, 123, 40, 2, 43, 1, 18, 120, 27, 48, 91, 46, 28, 110, 37, 56, 50, 9, 97, 6, 10, 8, 106, 75, 68, 63, 124, 59, 93, 41, 45, 17, 94, 0, 105, 107, 112, 115, 117, 119, 121, 102, 113, 62, 95, 3, 5, 7, 19, 22, 29, 32, 39, 44, 47, 100, 49, 57, 60, 64, 65, 67, 69, 70, 79, 85, 86, 55, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 331  249  287  250  314  396  273  319  377  497  336  369 1478  287
  336  310  587  563  238  455  544  330  370  326  262  554  351  556
  666  155  357  902  507  203  284  186 1345  353  312  405  562  696
  467  226  691  361  487  701  585  338  553  284  347  250  287  218
  600  458  375  482  552  249  690  379  508  439  134  600  307  437
  406  391  363  527  320  184  908  399  297  403  141  270 1140  101
  538  381  461  349  309  272 1043  471  541  587  416  463  335  556
  190  620  508  248  422  404  421  614  378  350  400  448  339  807
  541  442  752  456  412  540  224  561  390  286  284  424  385  603]
CBFL per class weights: tensor([0.9948, 1.0447, 1.0159, 1.0437, 1.0018, 0.9774, 1.0251, 0.9996, 0.9813,
        0.9657, 0.9930, 0.9832, 0.9591, 1.0159, 0.9930, 1.0036, 0.9618, 0.9625,
        1.0557, 0.9691, 0.9632, 0.9952, 0.9830, 0.9968, 1.0334, 0.9628, 0.9881,
        0.9627, 0.9603, 1.2150, 0.9864, 0.9592, 0.9650, 1.1024, 1.0177, 1.1340,
        0.9591, 0.9876, 1.0027, 0.9758, 0.9625, 0.9600, 0.9680, 1.0695, 0.9600,
        0.9853, 0.9664, 0.9600, 0.9618, 0.9923, 0.9628, 1.0177, 0.9894, 1.0437,
        1.0159, 1.0799, 0.9614, 0.9688, 0.9818, 0.9667, 0.9629, 1.0447, 0.9601,
        0.9809, 0.9650, 0.9709, 1.2963, 0.9614, 1.0051, 0.9711, 0.9756, 0.9783,
        0.9848, 0.9639, 0.9992, 1.1382, 0.9592, 0.9768, 1.0102, 0.9761, 1.2660,
        1.0272, 0.9591, 1.5042, 0.9634, 0.9804, 0.9685, 0.9888, 1.0041, 1.0258,
        0.9591, 0.9676, 0.9633, 0.9618, 0.9740, 0.9683, 0.9934, 0.9627, 1.1259,
        0.9610, 0.9650, 1.0456, 0.9731, 0.9759, 0.9733, 0.9611, 0.9811, 0.9884,
        0.9767, 0.9699, 0.9920, 0.9594, 0.9633, 0.9705, 0.9596, 0.9690, 0.9746,
        0.9634, 1.0720, 0.9625, 0.9785, 1.0165, 1.0177, 0.9728, 0.9796, 0.9614],
       device='cuda:0')
S painting T real Train Ep: 7000 lr0.006716813796678979 	 Loss Classification: 0.344742 Loss T 0.086207 Method MME


Labeled Target set: Average loss: 1.6723, Accuracy: 689/1080 F1 (63.7963%)


Test set: Average loss: 1.4428, Accuracy: 48794/69960 F1 (69.7456%)


Val set: Average loss: 1.6409, Accuracy: 239/360 F1 (66.3889%)

best acc test 72.168382  acc val 66.388889 acc labeled target 63.796296
saving model...
S painting T real Train Ep: 7100 lr0.006687332499522798 	 Loss Classification: 0.655327 Loss T 0.046221 Method MME

S painting T real Train Ep: 7200 lr0.006658151376159165 	 Loss Classification: 0.353682 Loss T 0.075053 Method MME

S painting T real Train Ep: 7300 lr0.00662926565157663 	 Loss Classification: 0.291690 Loss T 0.077867 Method MME

S painting T real Train Ep: 7400 lr0.006600670653747793 	 Loss Classification: 0.564240 Loss T 0.078460 Method MME

S painting T real Train Ep: 7500 lr0.0065723618108320175 	 Loss Classification: 0.328443 Loss T 0.054224 Method MME


Labeled Target set: Average loss: 0.0565, Accuracy: 1067/1080 F1 (98.7963%)


Test set: Average loss: 1.0035, Accuracy: 53869/69960 F1 (76.9997%)


Val set: Average loss: 1.1636, Accuracy: 272/360 F1 (75.5556%)

best acc test 76.999714  acc val 75.555556 acc labeled target 98.796296
saving model...
S painting T real Train Ep: 7600 lr0.006544334648469591 	 Loss Classification: 1.108830 Loss T 0.068006 Method MME

S painting T real Train Ep: 7700 lr0.006516584787163857 	 Loss Classification: 0.229018 Loss T 0.076666 Method MME

S painting T real Train Ep: 7800 lr0.006489107939747966 	 Loss Classification: 0.997956 Loss T 0.039372 Method MME

S painting T real Train Ep: 7900 lr0.0064618999089330565 	 Loss Classification: 0.377834 Loss T 0.047686 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        0.7777778 1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        1.        1.
 1.        1.        1.        0.8888889 0.8888889 1.        1.
 1.        1.        1.        1.        1.        0.8888889 0.8333333
 1.        1.        1.        1.        0.8888889 1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        1.        1.
 0.8888889 1.        1.        0.8888889 0.8888889 1.        1.
 1.        1.        1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [23, 62, 94, 84, 52, 53, 80, 45, 88, 61, 67, 87, 86, 89, 90, 91, 83, 82, 85, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 81, 92, 93, 95, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 66, 110, 108, 107, 106, 105, 104, 103, 102, 101, 100, 99, 98, 97, 96, 109, 65, 0, 63, 27, 26, 25, 24, 22, 21, 20, 19, 18, 17, 16, 15, 28, 14, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 13, 29, 30, 31, 124, 60, 59, 58, 57, 56, 55, 54, 51, 50, 49, 48, 47, 46, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 64, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 344  306  311  268  410  386  274  346  404  518  348  394 1231  326
  358  406  619  579  295  524  612  351  391  344  267  590  363  580
  683  166  375  801  522  224  291  201 1146  378  327  453  573  719
  495  245  704  377  521  711  602  355  610  307  364  274  249  226
  635  484  436  548  572  218  709  431  530  471  159  594  328  441
  440  457  407  557  357  181  932  423  314  427  202  283 1006  168
  568  394  504  381  354  292  963  491  559  621  442  471  351  576
  199  641  534  288  439  433  450  638  396  401  465  467  351  682
  566  465  662  462  444  556  308  596  452  309  310  469  425  612]
CBFL per class weights: tensor([1.0011, 1.0164, 1.0140, 1.0398, 0.9855, 0.9900, 1.0354, 1.0004, 0.9865,
        0.9748, 0.9998, 0.9883, 0.9695, 1.0076, 0.9968, 0.9862, 0.9714, 0.9724,
        1.0222, 0.9745, 0.9716, 0.9988, 0.9889, 1.0011, 1.0406, 0.9721, 0.9954,
        0.9724, 0.9705, 1.1948, 0.9924, 0.9698, 0.9746, 1.0836, 1.0245, 1.1178,
        0.9695, 0.9917, 1.0072, 0.9798, 0.9726, 0.9702, 0.9762, 1.0598, 0.9703,
        0.9919, 0.9747, 0.9703, 0.9718, 0.9977, 0.9716, 1.0159, 0.9952, 1.0354,
        1.0560, 1.0810, 0.9711, 0.9770, 0.9818, 0.9735, 0.9726, 1.0915, 0.9703,
        0.9824, 0.9742, 0.9781, 1.2154, 0.9720, 1.0068, 0.9812, 0.9813, 0.9794,
        0.9860, 0.9731, 0.9971, 1.1572, 0.9696, 0.9835, 1.0126, 0.9830, 1.1161,
        1.0294, 0.9695, 1.1893, 0.9727, 0.9883, 0.9757, 0.9910, 0.9979, 1.0239,
        0.9696, 0.9765, 0.9730, 0.9714, 0.9810, 0.9781, 0.9988, 0.9725, 1.1212,
        0.9711, 0.9741, 1.0263, 0.9814, 0.9822, 0.9801, 0.9711, 0.9880, 0.9870,
        0.9786, 0.9785, 0.9988, 0.9705, 0.9728, 0.9786, 0.9708, 0.9789, 0.9808,
        0.9731, 1.0155, 0.9719, 0.9799, 1.0150, 1.0145, 0.9783, 0.9832, 0.9716],
       device='cuda:0')
S painting T real Train Ep: 8000 lr0.006434956584934828 	 Loss Classification: 0.251760 Loss T 0.070190 Method MME


Labeled Target set: Average loss: 0.1174, Accuracy: 1054/1080 F1 (97.5926%)


Test set: Average loss: 1.2606, Accuracy: 50937/69960 F1 (72.8087%)


Val set: Average loss: 1.4082, Accuracy: 253/360 F1 (70.2778%)

best acc test 76.999714  acc val 70.277778 acc labeled target 97.592593
saving model...
S painting T real Train Ep: 8100 lr0.006408273943175546 	 Loss Classification: 0.508418 Loss T 0.069259 Method MME

S painting T real Train Ep: 8200 lr0.006381848042058713 	 Loss Classification: 0.174429 Loss T 0.045457 Method MME

S painting T real Train Ep: 8300 lr0.0063556750208137005 	 Loss Classification: 0.704669 Loss T 0.047722 Method MME

S painting T real Train Ep: 8400 lr0.006329751097407787 	 Loss Classification: 0.363168 Loss T 0.053346 Method MME

S painting T real Train Ep: 8500 lr0.0063040725665231365 	 Loss Classification: 0.328699 Loss T 0.042148 Method MME


Labeled Target set: Average loss: 0.0417, Accuracy: 1073/1080 F1 (99.3519%)


Test set: Average loss: 1.0238, Accuracy: 54157/69960 F1 (77.4114%)


Val set: Average loss: 1.1211, Accuracy: 269/360 F1 (74.7222%)

best acc test 76.999714  acc val 74.722222 acc labeled target 99.351852
saving model...
S painting T real Train Ep: 8600 lr0.006278635797596355 	 Loss Classification: 0.693714 Loss T 0.048843 Method MME

S painting T real Train Ep: 8700 lr0.006253437232918371 	 Loss Classification: 0.739859 Loss T 0.047649 Method MME

S painting T real Train Ep: 8800 lr0.0062284733857924735 	 Loss Classification: 0.222051 Loss T 0.076672 Method MME

S painting T real Train Ep: 8900 lr0.006203740838748417 	 Loss Classification: 0.417587 Loss T 0.062005 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        0.8888889 1.        1.
 1.        1.        0.8888889 1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        0.8888889 1.        1.        1.
 1.        1.        1.        0.8888889 1.        1.        1.
 1.        1.        1.        1.        1.        0.8888889 1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 0.8888889 1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        0.8888889
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.
 1.        1.        1.        1.        1.        1.        1.       ]
Top k classes which perform poorly are:  [23, 45, 90, 38, 77, 18, 54, 83, 84, 85, 0, 87, 88, 89, 91, 86, 82, 81, 80, 79, 78, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 92, 93, 94, 95, 123, 122, 121, 120, 119, 118, 117, 116, 115, 114, 113, 112, 111, 66, 110, 108, 107, 106, 105, 104, 103, 102, 101, 100, 99, 98, 97, 96, 109, 65, 62, 63, 28, 27, 26, 25, 24, 22, 21, 20, 19, 17, 16, 15, 29, 14, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 13, 30, 31, 32, 124, 61, 60, 59, 58, 57, 56, 55, 53, 52, 51, 50, 49, 48, 47, 46, 44, 43, 42, 41, 40, 39, 37, 36, 35, 34, 33, 64, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
Predicted Number of Examples per Class is (According to the pseudo labels + labelled target examples):  [ 362  380  326  284  460  359  281  378  424  541  371  412 1015  346
  372  490  655  587  319  588  648  372  410  359  274  607  380  595
  694  179  385  747  528  245  304  206 1005  392  335  495  593  741
  525  279  720  388  549  717  626  364  643  321  371  294  209  234
  654  492  481  624  581  230  735  478  540  491  169  590  356  452
  465  522  451  582  389  187  975  429  329  447  301  292  860  215
  597  411  524  408  411  311  901  505  572  638  466  477  375  592
  228  671  554  319  467  459  478  656  400  450  541  486  371  612
  583  490  559  473  467  562  400  611  494  321  352  515  461  625]
CBFL per class weights: tensor([1.0016, 0.9972, 1.0135, 1.0349, 0.9849, 1.0024, 1.0368, 0.9976, 0.9892,
        0.9795, 0.9993, 0.9910, 0.9753, 1.0064, 0.9990, 0.9824, 0.9766, 0.9779,
        1.0165, 0.9779, 0.9767, 0.9990, 0.9914, 1.0024, 1.0416, 0.9775, 0.9972,
        0.9777, 0.9762, 1.1686, 0.9961, 0.9758, 0.9801, 1.0661, 1.0235, 1.1160,
        0.9753, 0.9946, 1.0101, 0.9821, 0.9778, 0.9758, 0.9803, 1.0381, 0.9760,
        0.9954, 0.9792, 0.9760, 0.9771, 1.0011, 0.9768, 1.0156, 0.9993, 1.0289,
        1.1113, 1.0779, 0.9766, 0.9823, 0.9831, 0.9771, 0.9781, 1.0826, 0.9759,
        0.9833, 0.9796, 0.9823, 1.1937, 0.9779, 1.0033, 0.9858, 0.9845, 0.9804,
        0.9859, 0.9781, 0.9952, 1.1510, 0.9753, 0.9885, 1.0124, 0.9863, 1.0250,
        1.0300, 0.9754, 1.1023, 0.9777, 0.9912, 0.9803, 0.9917, 0.9912, 1.0201,
        0.9754, 0.9814, 0.9784, 0.9769, 0.9844, 0.9834, 0.9983, 0.9778, 1.0850,
        0.9764, 0.9790, 1.0165, 0.9843, 0.9850, 0.9833, 0.9766, 0.9931, 0.9860,
        0.9795, 0.9827, 0.9993, 0.9774, 0.9781, 0.9824, 0.9788, 0.9837, 0.9843,
        0.9787, 0.9931, 0.9774, 0.9821, 1.0156, 1.0045, 0.9808, 0.9848, 0.9771],
       device='cuda:0')
S painting T real Train Ep: 9000 lr0.006179236241810624 	 Loss Classification: 0.403859 Loss T 0.051935 Method MME


Labeled Target set: Average loss: 0.0740, Accuracy: 1064/1080 F1 (98.5185%)

