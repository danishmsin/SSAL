Dataset multi Source painting Target real Labeled num perclass 3 Network resnet34
126 classes in this dataset
Unlabelled Target Dataset Size:  69980
Labelled Target Dataset Size:  378
Misc. Labelled Target Dataset Size:  378
Bank keys - Target:  dict_keys(['feat_vec', 'labels', 'names', 'domain_identifier']) Source:  dict_keys(['feat_vec', 'labels', 'names', 'domain_identifier'])
Num  - Target:  69980 Source:  31502
Unlabeled Target Data Size: 1457
S painting T real Train Ep: 0 lr0.01 	 Loss Classification: 4.946222 Loss T 0.471782 Method MME

S painting T real Train Ep: 100 lr0.009925650290240803 	 Loss Classification: 2.405523 Loss T 0.226135 Method MME

S painting T real Train Ep: 200 lr0.009852577760521605 	 Loss Classification: 2.167987 Loss T 0.184319 Method MME

S painting T real Train Ep: 300 lr0.009780748269686728 	 Loss Classification: 1.358100 Loss T 0.142505 Method MME

S painting T real Train Ep: 400 lr0.009710128909124701 	 Loss Classification: 1.319227 Loss T 0.146656 Method MME

S painting T real Train Ep: 500 lr0.00964068794694323 	 Loss Classification: 1.751258 Loss T 0.147768 Method MME


Labeled Target set: Average loss: 1.9369, Accuracy: 604/1080 F1 (55.9259%)


Test set: Average loss: 1.5506, Accuracy: 44462/69960 F1 (63.5535%)


Val set: Average loss: 1.7350, Accuracy: 212/360 F1 (58.8889%)

best acc test 63.553459  acc val 58.888889 acc labeled target 55.925926
saving model...
S painting T real Train Ep: 600 lr0.00957239477517603 	 Loss Classification: 1.402908 Loss T 0.138540 Method MME

S painting T real Train Ep: 700 lr0.009505219859830012 	 Loss Classification: 1.072659 Loss T 0.117166 Method MME

S painting T real Train Ep: 800 lr0.009439134693595126 	 Loss Classification: 1.284497 Loss T 0.122470 Method MME

S painting T real Train Ep: 900 lr0.009374111751051751 	 Loss Classification: 1.970505 Loss T 0.107967 Method MME

S painting T real Train Ep: 1000 lr0.009310124446222227 	 Loss Classification: 1.465624 Loss T 0.110978 Method MME


Labeled Target set: Average loss: 1.8619, Accuracy: 604/1080 F1 (55.9259%)


Test set: Average loss: 1.4647, Accuracy: 45887/69960 F1 (65.5903%)


Val set: Average loss: 1.6947, Accuracy: 221/360 F1 (61.3889%)

best acc test 65.590337  acc val 61.388889 acc labeled target 55.925926
saving model...
S painting T real Train Ep: 1100 lr0.00924714709232377 	 Loss Classification: 1.353881 Loss T 0.096009 Method MME

S painting T real Train Ep: 1200 lr0.009185154863590003 	 Loss Classification: 1.480851 Loss T 0.126076 Method MME

S painting T real Train Ep: 1300 lr0.00912412375903735 	 Loss Classification: 1.287159 Loss T 0.093821 Method MME

S painting T real Train Ep: 1400 lr0.009064030568061049 	 Loss Classification: 0.391063 Loss T 0.103665 Method MME

S painting T real Train Ep: 1500 lr0.009004852837753237 	 Loss Classification: 1.076867 Loss T 0.093028 Method MME


Labeled Target set: Average loss: 1.7634, Accuracy: 661/1080 F1 (61.2037%)


Test set: Average loss: 1.3900, Accuracy: 47795/69960 F1 (68.3176%)


Val set: Average loss: 1.5626, Accuracy: 235/360 F1 (65.2778%)

best acc test 68.317610  acc val 65.277778 acc labeled target 61.203704
saving model...
S painting T real Train Ep: 1600 lr0.008946568841842816 	 Loss Classification: 0.444686 Loss T 0.064509 Method MME

S painting T real Train Ep: 1700 lr0.008889157551163433 	 Loss Classification: 1.687192 Loss T 0.098208 Method MME

S painting T real Train Ep: 1800 lr0.008832598605562044 	 Loss Classification: 0.930169 Loss T 0.100664 Method MME

S painting T real Train Ep: 1900 lr0.008776872287166303 	 Loss Classification: 1.199819 Loss T 0.084017 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.         0.6666667  0.7777778  0.44444445 0.5        0.5555556
 0.7777778  0.33333334 1.         0.6666667  0.6666667  0.6666667
 0.         0.33333334 0.6666667  0.5555556  0.33333334 1.
 0.33333334 0.7777778  0.16666667 0.6666667  1.         0.33333334
 0.         0.6666667  0.6666667  0.5555556  0.8888889  0.33333334
 1.         0.22222222 0.8888889  0.         0.33333334 0.6666667
 0.         0.6666667  0.33333334 1.         0.6666667  1.
 0.5555556  0.33333334 1.         1.         0.7777778  0.8888889
 0.         1.         0.33333334 0.6666667  0.6666667  0.6666667
 0.         0.6666667  0.8888889  1.         0.5555556  0.44444445
 1.         0.8888889  1.         0.         1.         0.7777778
 0.         1.         0.6666667  1.         1.         0.7777778
 0.11111111 0.6666667  0.6666667  0.5555556  0.6666667  0.33333334
 0.         1.         0.         0.6666667  0.22222222 0.
 0.11111111 1.         1.         0.5        0.         0.6666667
 0.22222222 1.         0.8888889  0.8888889  1.         0.7777778
 0.         0.6666667  0.7777778  0.6666667  1.         0.5
 1.         0.33333334 0.11111111 1.         0.6666667  0.8888889
 0.22222222 0.5        1.         0.         0.8888889  1.
 0.16666667 1.         0.5555556  1.         0.22222222 1.
 0.5        0.8888889  0.22222222 0.8333333  0.33333334 1.        ]
Top k classes which perform poorly are:  [83, 96, 111, 48, 36, 88, 54, 33, 66, 24, 78, 80, 12, 63, 72, 84, 104, 114, 20, 31, 118, 90, 122, 108, 82, 77, 38, 103, 34, 7, 43, 13, 29, 124, 16, 18, 23, 50, 3, 59, 87, 109, 101, 4, 120, 116, 27, 75, 5, 58, 42, 15, 40, 73, 74, 76, 81, 11, 10, 9, 106, 89, 1, 97, 99, 14, 68, 21, 26, 37, 51, 25, 55, 35, 53, 52, 98, 46, 71, 65, 95, 2, 6, 19, 123, 107, 121, 47, 112, 28, 56, 92, 93, 61, 32, 119, 115, 113, 110, 117, 105, 0, 62, 100, 8, 17, 22, 30, 39, 41, 44, 45, 49, 102, 57, 64, 67, 69, 70, 79, 85, 86, 91, 94, 60, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
S painting T real Train Ep: 2000 lr0.008721959494934213 	 Loss Classification: 0.474319 Loss T 0.076886 Method MME


Labeled Target set: Average loss: 2.0057, Accuracy: 633/1080 F1 (58.6111%)


Test set: Average loss: 1.6809, Accuracy: 44766/69960 F1 (63.9880%)


Val set: Average loss: 1.8651, Accuracy: 211/360 F1 (58.6111%)

best acc test 68.317610  acc val 58.611111 acc labeled target 58.611111
saving model...
S painting T real Train Ep: 2100 lr0.008667841720414475 	 Loss Classification: 0.967754 Loss T 0.065860 Method MME

S painting T real Train Ep: 2200 lr0.008614501024650454 	 Loss Classification: 0.981199 Loss T 0.067367 Method MME

S painting T real Train Ep: 2300 lr0.008561920016164943 	 Loss Classification: 0.820653 Loss T 0.088099 Method MME

S painting T real Train Ep: 2400 lr0.008510081829966844 	 Loss Classification: 0.581361 Loss T 0.078552 Method MME

S painting T real Train Ep: 2500 lr0.008458970107524513 	 Loss Classification: 0.957715 Loss T 0.079935 Method MME


Labeled Target set: Average loss: 1.7231, Accuracy: 672/1080 F1 (62.2222%)


Test set: Average loss: 1.3560, Accuracy: 49122/69960 F1 (70.2144%)


Val set: Average loss: 1.5721, Accuracy: 237/360 F1 (65.8333%)

best acc test 70.214408  acc val 65.833333 acc labeled target 62.222222
saving model...
S painting T real Train Ep: 2600 lr0.008408568977653933 	 Loss Classification: 1.227090 Loss T 0.071422 Method MME

S painting T real Train Ep: 2700 lr0.00835886303827305 	 Loss Classification: 1.006146 Loss T 0.072758 Method MME

S painting T real Train Ep: 2800 lr0.008309837338976545 	 Loss Classification: 0.466712 Loss T 0.057532 Method MME

S painting T real Train Ep: 2900 lr0.008261477364388068 	 Loss Classification: 0.678609 Loss T 0.064857 Method MME

Per Class Accuracy Calculated According to the Labelled Target examples is:  [1.         0.33333334 0.6666667  0.6666667  0.33333334 0.6666667
 0.5        0.22222222 0.8888889  0.8888889  0.7777778  0.6666667
 0.         0.22222222 0.6666667  0.6666667  0.44444445 0.8888889
 0.         0.8888889  0.         0.7777778  1.         0.22222222
 0.16666667 0.8888889  0.6666667  0.6666667  0.8888889  0.6666667
 0.5555556  1.         1.         0.33333334 0.33333334 0.7777778
 0.         1.         0.33333334 1.         0.6666667  1.
 0.5        0.7777778  1.         0.8888889  0.8888889  1.
 0.6666667  1.         0.6666667  0.22222222 0.5        0.33333334
 0.         1.         0.7777778  0.8888889  0.6666667  0.8888889
 0.8888889  0.22222222 1.         0.         1.         0.8888889
 0.         1.         1.         1.         1.         0.5555556
 0.33333334 0.5        0.11111111 0.22222222 0.6666667  0.33333334
 0.         1.         0.         0.8888889  0.33333334 0.
 0.         1.         0.8888889  0.33333334 0.22222222 1.
 0.22222222 0.6666667  0.6666667  1.         0.8888889  1.
 0.22222222 0.6666667  0.7777778  1.         0.8888889  0.6666667
 0.7777778  0.6666667  0.33333334 1.         0.6666667  0.8888889
 0.11111111 0.6666667  1.         0.11111111 1.         0.8888889
 0.5555556  1.         0.5555556  1.         0.22222222 0.8888889
 0.33333334 0.8888889  0.22222222 0.33333334 0.44444445 1.        ]
Top k classes which perform poorly are:  [80, 66, 63, 18, 78, 54, 20, 83, 12, 84, 36, 111, 74, 108, 24, 90, 51, 118, 75, 61, 122, 23, 88, 13, 96, 7, 38, 123, 87, 53, 120, 4, 33, 34, 72, 1, 104, 77, 82, 16, 124, 6, 73, 52, 42, 116, 114, 71, 30, 76, 58, 3, 2, 5, 92, 97, 101, 50, 48, 11, 14, 40, 15, 106, 29, 27, 26, 91, 103, 109, 98, 102, 43, 10, 21, 56, 35, 94, 25, 28, 121, 9, 86, 107, 119, 8, 45, 81, 46, 100, 57, 59, 60, 113, 65, 19, 17, 112, 110, 117, 115, 105, 0, 62, 95, 22, 31, 32, 37, 39, 41, 44, 47, 49, 55, 64, 67, 68, 69, 70, 79, 85, 89, 93, 99, 125]
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
Assigned Classwise weights to source
S painting T real Train Ep: 3000 lr0.008213769018249545 	 Loss Classification: 0.421919 Loss T 0.064843 Method MME


Labeled Target set: Average loss: 2.0171, Accuracy: 640/1080 F1 (59.2593%)


Test set: Average loss: 1.7161, Accuracy: 45805/69960 F1 (65.4731%)


Val set: Average loss: 1.9267, Accuracy: 223/360 F1 (61.9444%)

best acc test 70.214408  acc val 61.944444 acc labeled target 59.259259
saving model...
S painting T real Train Ep: 3100 lr0.008166698608209509 	 Loss Classification: 0.759761 Loss T 0.101679 Method MME

S painting T real Train Ep: 3200 lr0.008120252831274708 	 Loss Classification: 0.719206 Loss T 0.069266 Method MME

S painting T real Train Ep: 3300 lr0.008074418759891278 	 Loss Classification: 1.066458 Loss T 0.076677 Method MME

S painting T real Train Ep: 3400 lr0.008029183828623731 	 Loss Classification: 0.537841 Loss T 0.068624 Method MME

[W python_anomaly_mode.cpp:104] Warning: Error detected in PowBackward0. Traceback of forward call that caused the error:
  File "main_classwise.py", line 410, in <module>
    train()
  File "main_classwise.py", line 268, in train
    loss = torch.mean(weights_source * criterion(out1, target))
  File "/home/megh/anaconda3/envs/ssal/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/megh/projects/domain-adaptation/SSAL/utils/loss.py", line 68, in forward
    return focal_loss(F.cross_entropy(input, target, reduction='none', weight=self.weight), self.gamma)
  File "/home/megh/projects/domain-adaptation/SSAL/utils/loss.py", line 57, in focal_loss
    loss = (1 - p) ** gamma * input_values
 (function _print_stack)
/home/megh/projects/domain-adaptation/SSAL/utils/source_classwise_weighting.py:148: RuntimeWarning: divide by zero encountered in true_divide
  effective_num = 1.0 - np.power(beta, class_num_list)
/home/megh/projects/domain-adaptation/SSAL/utils/source_classwise_weighting.py:149: RuntimeWarning: invalid value encountered in true_divide
  per_cls_weights = (1.0 - beta) / np.array(effective_num)
Predicted Number of Examples per Class is (According to the pseudo labels):  [283 210 279  95 573  29 252 216 329 471 306 356   0 210 315 540 705 558
 109 290   6 280 372 337  55 559 378 562 627  66 325 134 195  46 163 141
   0 230 292 267 553 702 429 131 690 328 447 692 470 328 237 291 367 203
  68  78 598 419 168 452 534   3 633   7 498 433  51 692 320 420 440 400
 260 508  38  64 926 392 266 403   7 246  29  22 509 383 451 343 114 300
  45 438 522 574 421 456 106 580  78 625 526  29 442 389 465 614 365 320
  57 447 299  20 574 432  67 472 365 564   0 522 316 293 118 449 271 600]
CBFL per class weights: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., nan, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., nan, 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., nan, 0.,
        0., 0., 0., 0., 0., 0.], device='cuda:0')
Traceback (most recent call last):
  File "main_classwise.py", line 410, in <module>
    train()
  File "main_classwise.py", line 292, in train
    loss.backward()
  File "/home/megh/anaconda3/envs/ssal/lib/python3.7/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/megh/anaconda3/envs/ssal/lib/python3.7/site-packages/torch/autograd/__init__.py", line 132, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: Function 'PowBackward0' returned nan values in its 0th output.
